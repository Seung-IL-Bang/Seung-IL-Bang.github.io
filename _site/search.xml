<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[Spring Batch 데이터 집계(Aggregation) 최적화]]></title>
      <url>/spring%20batch/2025/03/23/Spring-Batch-Optimized-aggregation/</url>
      <content type="text"><![CDATA[  데이터 집계를 위한 배치 처리 시 GROUP BY와 SUM 대신 Redis를 활용하여 성능을 대폭 개선할 수 있는 방법에 대해 알아봅시다!🚀 들어가기대용량 데이터의 집계 처리는 배치 애플리케이션에서 자주 발생하는 작업이지만, 처리해야 할 데이터 양이 증가할수록 성능 이슈가 발생합니다. 본 포스트에서는 기존 GROUP BY, SUM 등 SQL 기반 집계 대신 Redis를 활용한 고성능 집계 방법과 그 최적화 과정에 대해 단계별로 알아보겠습니다.기존 GROUP BY, SUM 쿼리의 한계통계 처리 시 일반적으로 GROUP BY, SUM 쿼리를 사용하는 것은 데이터가 적을 때는 효율적입니다. 그러나 데이터가 대용량으로 증가하고 쿼리가 복잡해질수록 여러 성능 문제가 발생합니다:  여러 테이블을 JOIN하고 GROUP BY하는 복잡한 쿼리는 데이터베이스 옵티마이저가 비효율적인 실행 계획을 생성할 수 있습니다.  대량의 데이터 처리 과정에서 임시 테이블 생성으로 인한 디스크 I/O 부하가 증가합니다.  데이터 양이 증가할수록 쿼리 실행 시간이 기하급수적으로 증가합니다.  쿼리 튜닝이 복잡해지고 데이터베이스 리소스 사용량이 급증합니다.아래 코드는 배치 과정에서 정산 데이터를 집계하기 위해 GROUP BY, SUM을 사용한 SQL문 입니다:String sql = """            SELECT                    ds.settlement_id,                    ds.seller_id,                    ds.settlement_date,                    SUM(IF(dsd.settlement_status = 'COMPLETED', 1, 0)) AS total_order_count,                    SUM(IF(dsd.settlement_status = 'REFUNDED', 1, 0)) AS total_claim_count,                    SUM(CASE                            WHEN dsd.settlement_status = 'COMPLETED' THEN dsd.quantity                            WHEN dsd.settlement_status = 'REFUNDED' THEN -dsd.quantity                            ELSE 0                        END) AS total_quantity,                    SUM(CASE                            WHEN dsd.settlement_status = 'COMPLETED' THEN dsd.sales_amount                            WHEN dsd.settlement_status = 'REFUNDED' THEN -dsd.sales_amount                            ELSE 0                        END) AS total_sales_amount,                    SUM(CASE                            WHEN dsd.settlement_status = 'COMPLETED' THEN dsd.tax_amount                            WHEN dsd.settlement_status = 'REFUNDED' THEN -dsd.tax_amount                            ELSE 0                        END) AS total_tax_amount,                    SUM(CASE                            WHEN dsd.settlement_status = 'COMPLETED' THEN dsd.promotion_discount_amount                            WHEN dsd.settlement_status = 'REFUNDED' THEN -dsd.promotion_discount_amount                            ELSE 0                        END) AS total_promotion_discount_amount,                    SUM(CASE                            WHEN dsd.settlement_status = 'COMPLETED' THEN dsd.coupon_discount_amount                            WHEN dsd.settlement_status = 'REFUNDED' THEN -dsd.coupon_discount_amount                            ELSE 0                        END) AS total_coupon_discount_amount,                    SUM(CASE                            WHEN dsd.settlement_status = 'COMPLETED' THEN dsd.point_used_amount                            WHEN dsd.settlement_status = 'REFUNDED' THEN -dsd.point_used_amount                            ELSE 0                        END) AS total_point_used_amount,                    SUM(CASE                            WHEN dsd.settlement_status = 'COMPLETED' THEN dsd.shipping_fee                            WHEN dsd.settlement_status = 'REFUNDED' THEN -dsd.shipping_fee                            ELSE 0                        END) AS total_shipping_fee,                    SUM(IF(dsd.settlement_status = 'REFUNDED', dsd.claim_shipping_fee, 0)) AS total_claim_shipping_fee,                    SUM(CASE                            WHEN dsd.settlement_status = 'COMPLETED' THEN dsd.commission_amount                            WHEN dsd.settlement_status = 'REFUNDED' THEN -dsd.commission_amount                            ELSE 0                        END) AS total_commission_amount,                    SUM(CASE                            WHEN dsd.settlement_status = 'COMPLETED' THEN dsd.settlement_amount                            WHEN dsd.settlement_status = 'REFUNDED' THEN -dsd.settlement_amount                            ELSE 0                        END) AS total_settlement_amount            FROM daily_settlement_detail dsd            JOIN daily_settlement ds on dsd.daily_settlement_id = ds.settlement_id            WHERE ds.settlement_date = ?            GROUP BY ds.settlement_id, ds.seller_id, ds.settlement_date        """;Redis를 활용한 효율적인 집계 처리대용량 데이터 집계 시 발생하는 GROUP BY 쿼리의 문제를 해결하기 위해 집계 연산을 디스크 기반 RDB 데이터베이스가 아닌 인메모리 데이터베이스인 Redis로 이관할 수 있습니다.Redis를 집계 처리에 활용하면 다음과 같은 장점이 있습니다:  집계에 최적화된 연산 API 지원 (hincrby, hincrbyfloat 등)  메모리 기반 처리로 디스크 I/O 병목 현상 제거  인메모리 데이터베이스의 초고속 연산 처리 능력🚨 주의사항:Redis를 사용하면 집계 연산을 빠르게 처리할 순 있지만, 병목 지점이 될 수 있는 지점이 네트워크 레이턴시입니다. 1,000만 개의 데이터를 합산하기 위해 1,000만 번의 개별 네트워크 요청이 발생한다면, 이로 인한 성능 저하는 피할 수 없습니다. ☠️Redis Pipeline을 통한 네트워크 레이턴시 최소화Redis Pipeline을 활용하면 수많은 Redis 명령을 묶어서 한 번의 네트워크 왕복으로 처리할 수 있습니다. 이를 통해 1,000만 번의 개별 네트워크 요청을 청크 단위로 묶어 1만 번 정도로 줄이면 집계 처리 시간을 획기적으로 단축할 수 있습니다.개별 요청 방식Pipeline 방식💡 중요한 구현 포인트:Spring Batch의 청크 프로세싱 아키텍처를 고려할 때, Redis 파이프라인은 반드시 ItemWriter에서 생성해야 합니다. Spring Batch는 ItemReader → ItemProcessor 단계에서는 항목을 하나씩 처리하다가, 지정된 Chunk 크기만큼 항목이 처리되면 ItemWriter에게 묶음으로 전달합니다.만약 ItemProcessor에서 Redis Pipeline을 생성한다면, 항목마다 Redis 연결을 맺고 끊는 불필요한 오버헤드가 발생합니다. 따라서 ItemWriter에서 청크 단위로 파이프라인을 처리하는 것이 최적의 구현 방식입니다.(처음에 청크 프로세싱 아키텍처를 고려하지 않고, 단순히 ItemProcessor에서 처리하는게 자연스럽단 착각 덕분에 성능이 나빠진 제 경험으로부터 알려드립니다. 😂)다음은 정산 데이터(settlement_detail) 청크를 정산 항목별로 Redis에서 파이프라인을 사용해 집계 처리하는 코드입니다:@Beanpublic ItemWriter&lt;SettlementAggregate&gt; optimizedRedisAggregateWriter() {    return items -&gt; {        // 청크의        // 파이프라인 시작        redisAsyncCommands.multi();        log.info("Processing {} items in a single Redis pipeline transaction", items.size());        for (SettlementAggregate detail : items) {            String key = "daily_settlement:" + detail.getSettlementId();            // 상태별 처리            if (COMPLETED.name().equals(detail.getSettlementStatus())) {                redisAsyncCommands.hincrby(key, "totalOrderCount", 1);                redisAsyncCommands.hincrby(key, "totalQuantity", detail.getQuantity());            } else if (REFUNDED.name().equals(detail.getSettlementStatus())) {                redisAsyncCommands.hincrby(key, "totalClaimCount", 1);                redisAsyncCommands.hincrby(key, "totalQuantity", -detail.getQuantity());            }            // 금액 관련 필드 처리            String statusMultiplier = REFUNDED.name().equals(detail.getSettlementStatus()) ? "-" : "";            redisAsyncCommands.hincrbyfloat(key, "totalSalesAmount",                    Double.parseDouble(statusMultiplier + detail.getSalesAmount().toString()));            redisAsyncCommands.hincrbyfloat(key, "totalTaxAmount",                    Double.parseDouble(statusMultiplier + detail.getTaxAmount().toString()));            redisAsyncCommands.hincrbyfloat(key, "totalPromotionDiscountAmount",                    Double.parseDouble(statusMultiplier + detail.getPromotionDiscountAmount().toString()));            redisAsyncCommands.hincrbyfloat(key, "totalCouponDiscountAmount",                    Double.parseDouble(statusMultiplier + detail.getCouponDiscountAmount().toString()));            redisAsyncCommands.hincrbyfloat(key, "totalPointUsedAmount",                    Double.parseDouble(statusMultiplier + detail.getPointUsedAmount().toString()));            redisAsyncCommands.hincrbyfloat(key, "totalShippingFee",                    Double.parseDouble(statusMultiplier + detail.getShippingFee().toString()));            redisAsyncCommands.hincrbyfloat(key, "totalCommissionAmount",                    Double.parseDouble(statusMultiplier + detail.getCommissionAmount().toString()));            redisAsyncCommands.hincrbyfloat(key, "totalSettlementAmount",                    Double.parseDouble(statusMultiplier + detail.getSettlementAmount().toString()));            // REFUNDED인 경우 클레임 배송비 추가            if (REFUNDED.name().equals(detail.getSettlementStatus())) {                redisAsyncCommands.hincrbyfloat(key, "totalClaimShippingFee",                        Double.parseDouble(detail.getClaimShippingFee().toString()));            }            // seller_id와 settlement_date 저장 (덮어쓰기 - 모든 레코드가 동일한 값을 가짐)            redisAsyncCommands.hset(key, "sellerId", detail.getSellerId().toString());            redisAsyncCommands.hset(key, "settlementDate", detail.getSettlementDate().toString());            // 해당 settlement에 대한 키 목록에 추가            redisAsyncCommands.sadd("settlement_keys:" + detail.getSettlementDate().toString(), key);        }        // 모든 명령을 한 번에 실행        redisAsyncCommands.exec();        log.info("Successfully processed batch of {} settlement details to Redis", items.size());    };}이 방식을 적용한 후에도 redisAsyncCommands 클라이언트를 통해 반복되는 연산 요청을 매번 파이프라인에 적재해야 하므로, 기존 GROUP BY 대비 성능 개선이 기대보다 크지 않았습니다.더 높은 성능 개선을 위해 구글링과 AI에게 조언을 구하여, 저수준 API 활용 방법 중 Lua 스크립트 사용을 고려해 보게 되었습니다.Lua 스크립트를 활용한 집계 처리파이프라인에 연산 명령어를 적재하는 과정에서도 여전히 애플리케이션 측에서 많은 연산 로직이 필요하다는 사실을 발견했습니다.(🤔 StepListener를 활용하여 명령어를 적재하는 과정이 얼마나 시간이 걸리는지 측정했습니다.)이를 해결하기 위해 Lua 스크립트를 활용하여 데이터만 Redis 서버로 전송하고, 모든 집계 연산은 Redis 서버에서 직접 수행하도록 개선했습니다.이 접근 방식의 핵심 장점은 다음과 같습니다:  애플리케이션 측의 연산 오버헤드 제거  데이터 전송량 최소화  Redis 서버에서의 원자적 실행 보장  복잡한 로직을 단일 스크립트로 캡슐화다음은 정산 집계를 위한 Lua 스크립트입니다:// 집계를 위한 Lua 스크립트private static final String AGGREGATE_SCRIPT = """        local key = KEYS[1]        local settlement_key = KEYS[2]        local status = ARGV[1]        local seller_id = ARGV[2]        local settlement_date = ARGV[3]        local quantity = tonumber(ARGV[4])        local tax_amount = tonumber(ARGV[5])        local sales_amount = tonumber(ARGV[6])        local promotion_discount_amount = tonumber(ARGV[7])        local coupon_discount_amount = tonumber(ARGV[8])        local point_used_amount = tonumber(ARGV[9])        local shipping_fee = tonumber(ARGV[10])        local claim_shipping_fee = tonumber(ARGV[11])        local commission_amount = tonumber(ARGV[12])        local settlement_amount = tonumber(ARGV[13])                if status == 'COMPLETED' then          redis.call('hincrby', key, 'totalOrderCount', 1)          redis.call('hincrby', key, 'totalQuantity', quantity)        else          redis.call('hincrby', key, 'totalClaimCount', 1)          redis.call('hincrby', key, 'totalQuantity', -quantity)        end                local multiplier = status == 'REFUNDED' and -1 or 1        redis.call('hincrbyfloat', key, 'totalSalesAmount', multiplier * sales_amount)        redis.call('hincrbyfloat', key, 'totalTaxAmount', multiplier * tax_amount)        redis.call('hincrbyfloat', key, 'totalPromotionDiscountAmount', multiplier * promotion_discount_amount)        redis.call('hincrbyfloat', key, 'totalCouponDiscountAmount', multiplier * coupon_discount_amount)        redis.call('hincrbyfloat', key, 'totalPointUsedAmount', multiplier * point_used_amount)        redis.call('hincrbyfloat', key, 'totalShippingFee', multiplier * shipping_fee)        redis.call('hincrbyfloat', key, 'totalCommissionAmount', multiplier * commission_amount)        redis.call('hincrbyfloat', key, 'totalSettlementAmount', multiplier * settlement_amount)                if status == 'REFUNDED' then          redis.call('hincrbyfloat', key, 'totalClaimShippingFee', claim_shipping_fee)        end                redis.call('hset', key, 'sellerId', seller_id)        redis.call('hset', key, 'settlementDate', settlement_date)        redis.call('sadd', settlement_key, key)                return 1        """;다음은 Lua 스크립트를 활용해 Redis에서 집계 연산을 수행하는 ItemWriter 구현입니다:@Beanpublic ItemWriter&lt;SettlementAggregate&gt; luaScriptDirectWriter() {    return items -&gt; {        log.info("Processing {} items with Redis Lua Script direct execution", items.size());        // Lua 스크립트를 활용한 처리        try (StatefulRedisConnection&lt;String, String&gt; connection = redisClient.connect()) {            RedisAsyncCommands&lt;String, String&gt; async = connection.async();            async.setAutoFlushCommands(false);            List&lt;RedisFuture&lt;?&gt;&gt; futures = new ArrayList&lt;&gt;();            for (SettlementAggregate detail : items) {                String key = "daily_settlement:" + detail.getSettlementId();                String settlementKeysKey = "settlement_keys:" + detail.getSettlementDate().toString();                // Lua 스크립트 실행 - 모든 연산을 레디스 서버에서 처리                futures.add(async.eval(                        AGGREGATE_SCRIPT,                        ScriptOutputType.INTEGER,                        new String[]{key, settlementKeysKey},                        detail.getSettlementStatus(),                        detail.getSellerId().toString(),                        detail.getSettlementDate().toString(),                        String.valueOf(detail.getQuantity()),                        detail.getTaxAmount().toString(),                        detail.getSalesAmount().toString(),                        detail.getPromotionDiscountAmount().toString(),                        detail.getCouponDiscountAmount().toString(),                        detail.getPointUsedAmount().toString(),                        detail.getShippingFee().toString(),                        detail.getClaimShippingFee().toString(),                        detail.getCommissionAmount().toString(),                        detail.getSettlementAmount().toString()                ));            }            // 모든 명령어 한번에 전송            async.flushCommands();            // 모든 작업 완료 대기            CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))                    .get(30, TimeUnit.SECONDS);        } catch (Exception e) {            log.error("Error processing settlements with Lua script: {}", e.getMessage(), e);            throw new RuntimeException("Error executing Redis Lua script", e);        }        log.info("Successfully processed batch of {} settlement details to Redis using Lua script", items.size());    };}Lua 스크립트 사용 시 주의사항과 트레이드 오프Lua 스크립트를 통한 최적화는 큰 성능 향상을 가져올 수 있지만, 몇 가지 중요한 고려사항과 트레이드 오프가 존재합니다:📌 Redis 서버 부하 증가:  집계 연산의 부하가 애플리케이션에서 Redis 서버로 이동합니다.  복잡한 스크립트는 Redis의 싱글 스레드 특성으로 인해 다른 작업을 차단할 수 있습니다.  특히 고부하 상황에서 Redis 서버의 CPU 사용률을 모니터링해야 합니다.📌 디버깅 복잡성:  Lua 스크립트 내부에서 발생하는 오류는 디버깅이 어렵습니다.  로그 출력 등 디버깅 정보를 얻기 어려우므로 철저한 테스트가 필요합니다.  스크립트 오류 시 전체 트랜잭션이 실패할 수 있어 롤백 전략이 필요합니다.📌 유지보수 복잡성:  Lua 스크립트에 대한 버전 관리와 배포 전략이 필요합니다.📌 Lua 언어 학습 곡선:  개발팀이 Lua 언어에 익숙하지 않을 경우 추가적인 학습 비용이 발생합니다.📌 원자성 제한:  Lua 스크립트는 단일 Redis 인스턴스에서는 원자적이지만, 클러스터 환경에서는 원자성이 보장되지 않습니다.  클러스터에서 여러 키에 걸친 작업을 수행할 때는 모든 키가 동일한 슬롯에 있어야 합니다.📌 메모리 사용량 증가:  Lua 스크립트 실행 중 생성되는 임시 데이터는 Redis 메모리를 사용합니다.  대규모 데이터 처리 시 maxmemory 설정과 메모리 모니터링이 중요합니다.이러한 트레이드 오프를 고려하여, 다음과 같은 상황에서 Lua 스크립트 사용을 권장합니다:  집계, 카운팅 등 단순하고 반복적인 연산이 필요한 경우  네트워크 왕복을 최소화해야 하는 성능 매우 중요한 상황  Redis 서버의 리소스가 충분한 경우반면, 다음과 같은 경우는 파이프라인이나 다른 방식을 고려해 볼 수 있습니다:  로직이 매우 복잡하고 자주 변경되는 경우  Redis 클러스터 환경에서 여러 키를 사용해야 하는 경우  Redis 서버가 이미 높은 부하를 겪고 있는 경우성능 비교 및 결론세 가지 구현 방식의 성능을 비교한 결과는 다음과 같습니다:  처리 속도 향상: GROUP BY 쿼리 대비 최대 9배 성능 개선  데이터베이스 부하 감소: 데이터베이스의 집계 연산 부담 제거  확장성 향상: 데이터 증가에도 선형적인 성능 유지  실시간 집계 가능: 배치 작업과 실시간 집계를 병행 가능결론대용량 데이터 집계가 필요한 Spring Batch 애플리케이션에서는 단순히 SQL의 GROUP BY에 의존하기보다 Redis와 같은 인메모리 데이터베이스를 활용하고, Lua 스크립트를 통한 서버 사이드 처리를 구현하여 최적의 성능을 얻을 수도 있습니다. 본 포스트가 배치 처리의 성능을 높이는 데 도움이 되었으면 좋겠습니다.감사합니다.참고 자료  Batch Performance 극한으로 끌어올리기: 1억 건 데이터 처리를 위한 노력 / if(kakao)dev2022  Spring Batch 애플리케이션 성능 향상을 위한 주요 팁 (kakao tech)  Redis Commands Docs - hincrby  Redis Commands Docs - hincrbyfloat  Redis Docs - Scripting with Lua]]></content>
      <categories>
        
          <category> Spring Batch </category>
        
      </categories>
      <tags>
        
          <tag> Spring Batch </tag>
        
          <tag> Redis </tag>
        
          <tag> Aggregation </tag>
        
          <tag> Performance Optimization </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Batch 효율적인 대량 데이터 Write 전략]]></title>
      <url>/spring%20batch/2025/03/23/Spring-Batch-Optimized-Write/</url>
      <content type="text"><![CDATA[  스프링 배치 환경에서 JPA 기반 ItemWriter의 성능 저하 요인을 분석하고, JDBC 배치 인서트 전략을 통한 성능 개선 방안을 알아봅니다.🚀 들어가기대용량 데이터 처리는 엔터프라이즈 애플리케이션에서 흔히 마주하는 도전 과제입니다. 특히 Spring Batch 환경에서 대량의 데이터를 효율적으로 저장하는 것은 전체 배치 프로세스의 성능에 직접적인 영향을 미칩니다. 본 포스트에서는 JPA 기반 ItemWriter 사용 시 발생하는 성능 제약 요인을 상세히 분석하고, JDBC 기반 배치 인서트를 활용한 성능 최적화 전략에 대해 살펴보겠습니다.JPA 기반 ItemWriter의 성능 제약 요인먼저 배치 처리 환경에서 JPA의 적합성에 대해 고민해볼 필요가 있습니다. JPA는 영속성 컨텍스트를 통해 개발자에게 많은 편의성을 제공하지만, 이러한 추상화 계층은 내부적으로 복잡한 로직을 수행하며 대량 데이터 처리 시 성능 오버헤드로 작용할 수 있습니다.영속성 컨텍스트와 더티 체킹 오버헤드JPA는 정교한 ORM 프레임워크로서 엔티티의 상태 변화를 감지하고 관리하기 위해 영속성 컨텍스트와 더티 체킹(Dirty Checking) 메커니즘을 활용합니다. 이 과정에서 다음과 같은 오버헤드가 발생합니다:  엔티티 상태 추적: 모든 엔티티의 원본 상태를 메모리에 유지하고 변경사항을 감지  스냅샷 비교: 트랜잭션 커밋 시점에 원본 스냅샷과 현재 상태를 비교하는 연산 수행  캐시 관리: 1차 캐시와 2차 캐시의 동기화 및 관리 비용배치 처리에서는 일반적으로 데이터의 읽기와 쓰기 단계가 명확히 구분되어 있어, 이러한 정교한 상태 관리 메커니즘이 불필요한 오버헤드로 작용할 수 있습니다.불필요한 컬럼 업데이트 문제JPA는 기본적으로 변경 감지 시 엔티티의 모든 필드를 포함한 UPDATE 쿼리를 생성합니다. 이는 다음과 같은 문제를 야기합니다:  비효율적인 SQL 생성: 실제로 변경된 필드만 업데이트하는 것이 아니라 모든 필드를 포함한 SQL 생성  데이터베이스 부하 증가: 불필요한 컬럼까지 업데이트함으로써 데이터베이스 리소스 낭비  최적화 제한: 특정 컬럼만 선택적으로 업데이트하는 최적화 전략 적용 어려움이러한 JPA의 특성은 대규모 배치 처리 환경에서 심각한 성능 병목 현상을 초래할 수 있습니다.아래는 스프링 배치에서 JPA 기반의 ItemWriter를 사용하는 일반적인 코드입니다:@Beanpublic JpaItemWriter&lt;DailySettlement&gt; aggregateDailySettlementWriter() {    JpaItemWriter&lt;DailySettlement&gt; writer = new JpaItemWriter&lt;&gt;();    writer.setEntityManagerFactory(entityManagerFactory);    return writer;}이 코드는 간결하고 직관적이지만, 내부적으로는 영속성 컨텍스트 관리, 변경 감지, 그리고 각 엔티티별 개별 업데이트라는 오버헤드를 수반합니다.JDBC 배치 인서트를 통한 성능 최적화JDBC 배치 인서트는 여러 SQL 명령을 그룹화하여 단일 네트워크 요청으로 데이터베이스에 전송하는 기법입니다. 이 방식은 다음과 같은 이점을 제공합니다:  네트워크 오버헤드 감소: 여러 쿼리를 하나의 요청으로 묶어 네트워크 왕복 시간 최소화  데이터베이스 최적화: 데이터베이스가 여러 연산을 일괄 처리하여 내부 최적화 적용 가능  리소스 효율성: 커넥션 관리 및 트랜잭션 오버헤드 감소다음은 사이드 프로젝트에서 일일 정산 집계 데이터를 업데이트 하기 위해 JDBC 배치 인서트를 활용한 최적화된 ItemWriter 구현 예시입니다:@Beanpublic ItemWriter&lt;SettlementAggregationResult&gt; optimizedAggregateDailySettlementJdbcWriter() {    return new ItemWriter&lt;SettlementAggregationResult&gt;() {        @Autowired        private JdbcTemplate jdbcTemplate;        @Override        public void write(Chunk&lt;? extends SettlementAggregationResult&gt; items) throws Exception {            if (items.isEmpty()) {                return;            }            log.info("Processing {} settlement aggregation results with JDBC batch update", items.size());            String updateSql = """                        UPDATE daily_settlement SET                            total_order_count = ?,                            total_claim_count = ?,                            total_quantity = ?,                            sales_amount = ?,                            tax_amount = ?,                            promotion_discount_amount = ?,                            coupon_discount_amount = ?,                            point_used_amount = ?,                            shipping_fee = ?,                            claim_shipping_fee = ?,                            commission_amount = ?,                            total_settlement_amount = ?,                            updated_at = ?                        WHERE settlement_id = ?                    """;            jdbcTemplate.batchUpdate(updateSql, new BatchPreparedStatementSetter() {                @Override                public void setValues(PreparedStatement ps, int i) throws SQLException {                    SettlementAggregationResult result = items.getItems().get(i);                    LocalDateTime now = LocalDateTime.now();                    int idx = 1;                    // SET clause parameters                    ps.setInt(idx++, result.getTotalOrderCount());                    ps.setInt(idx++, result.getTotalClaimCount());                    ps.setInt(idx++, result.getTotalQuantity());                    ps.setBigDecimal(idx++, result.getTotalSalesAmount());                    ps.setBigDecimal(idx++, result.getTotalTaxAmount());                    ps.setBigDecimal(idx++, result.getTotalPromotionDiscountAmount());                    ps.setBigDecimal(idx++, result.getTotalCouponDiscountAmount());                    ps.setBigDecimal(idx++, result.getTotalPointUsedAmount());                    ps.setBigDecimal(idx++, result.getTotalShippingFee());                    ps.setBigDecimal(idx++, result.getTotalClaimShippingFee());                    ps.setBigDecimal(idx++, result.getTotalCommissionAmount());                    ps.setBigDecimal(idx++, result.getTotalSettlementAmount());                    ps.setTimestamp(idx++, Timestamp.valueOf(now));                    // WHERE clause parameter                    ps.setLong(idx++, result.getSettlementId());                }                @Override                public int getBatchSize() {                    return items.size();                }            });            log.info("Successfully updated {} daily settlements with batch update", items.size());        }    };}이 구현에서는 JPA의 불필요한 오버헤드를 제거하고, 주어진 청크 내의 모든 항목을 단일 배치 연산으로 처리함으로써 네트워크 왕복 시간을 크게 감소시킵니다.JPA ID 생성 전략과 배치 인서트 제약JPA에서도 hibernate.jdbc.batch_size 속성을 통해 배치 인서트를 지원하지만, ID 생성 전략에 따라 중요한 제약이 존재합니다:  IDENTITY 전략 제약:          IDENTITY 전략을 사용할 경우 Hibernate는 배치 인서트를 비활성화합니다.      이는 데이터베이스가 자동 증가 ID 값을 생성하고, Hibernate는 이 값을 즉시 조회해야 하기 때문입니다.        SEQUENCE 전략 활용:          SEQUENCE 전략을 사용하면 Hibernate가 사전에 ID 값을 할당하여 배치 인서트를 수행할 수 있습니다.      SEQUENCE 전략은 Hibernate가 미리 여러 개의 ID 값을 가져올 수 있습니다(allocationSize 활용).      즉, Hibernate는 ID 값을 미리 할당한 후, 여러 개의 INSERT 문을 한 번의 JDBC batch로 실행할 수 있습니다.        TABLE 전략 오버헤드:          TABLE 전략은 별도의 테이블을 사용하여 ID를 생성하는 방식이며, 배치 인서트 자체는 가능하지만 시퀀스 테이블에 대한 동시 접근으로 인해 잠금 경합(lock contention)이 발생할 가능성이 높습니다.      따라서 대량 삽입 작업 시 성능 저하가 발생할 수 있습니다.      실무 환경에서는 종종 데이터베이스의 자동 증가 기능에 의존하는 IDENTITY 전략을 사용하기 때문에, JPA의 배치 인서트 기능을 활용하기에 현실적으로 어려운 경우가 많습니다. 이런 상황에서는 순수 JDBC를 직접 사용하는 배치 인서트가 효과적인 대안이 될 것입니다.추가 개선 방안JDBC 방식을 활용하면서 발생할 수 있는 SQL문의 타입 안전성 부재와 유지보수성 저하 문제를 해결하기 위해 다음과 같은 개선 방안을 고려할 수 있습니다:QueryDSL 도입QueryDSL을 활용하면 타입 안전한 방식으로 SQL 쿼리를 작성하면서도 배치 인서트의 성능 이점을 유지할 수 있습니다.아래 코드는 QueryDSL을 적용한 예시 코드입니다. 기존 문자열로 SQL를 다룰 때보다 깔끔하고, 컴파일 시점에 오류를 잡아낼 수 있어서 안정적입니다:@Beanpublic ItemWriter&lt;SettlementAggregationResult&gt; querydslBatchWriter() {    return new ItemWriter&lt;SettlementAggregationResult&gt;() {        @Autowired        private SQLQueryFactory queryFactory;                @Override        public void write(Chunk&lt;? extends SettlementAggregationResult&gt; items) {            // QueryDSL을 사용한 배치 업데이트 구현            List&lt;SQLInsertBatch&gt; batch = items.getItems().stream()                .map(this::createBatchUpdate)                .collect(Collectors.toList());                            queryFactory.batch(batch.toArray(new SQLInsertBatch[0])).execute();        }                private SQLInsertBatch createBatchUpdate(SettlementAggregationResult result) {            // 타입 안전한 업데이트 구문 작성            QDailySettlement settlement = QDailySettlement.dailySettlement;            return queryFactory.update(settlement)                .set(settlement.totalOrderCount, result.getTotalOrderCount())                // 기타 필드 설정                .where(settlement.settlementId.eq(result.getSettlementId()));        }    };}성능 모니터링 및 최적화 - 앞으로의 계획현재 사이드 프로젝트에서는 기본적인 배치 인서트 구현에 집중하고 있지만, 향후 성능을 더욱 개선하기 위해 다음과 같은 추가 전략을 적용할 계획입니다:  청크 크기 최적화: 메모리 사용량과 처리 속도의 균형을 고려한 최적의 청크 크기를 찾는 실험을 진행할 예정입니다. 현재는 고정값(size: 1000)을 사용 중이지만, 실제 데이터 특성에 맞는 최적화가 필요합니다.  실행 계획 분석: 배치 처리 시 생성되는 SQL의 실행 계획을 분석하여 인덱스 활용을 최적화하는 작업도 진행하면 좋을 것 같습니다. 현재는 기본 인덱스만 활용 중이지만, 실행 패턴에 따른, 좀 더 세밀한 인덱스 전략이 필요해 보입니다.      Rewrite 배치 전략: 대량 데이터 갱신 시 임시 테이블을 활용한 Rewrite 전략도 검토 중입니다. 이 전략은 다음과 같은 이유로 특히 대규모 업데이트가 필요한 월별 정산 처리에서 효과적일 것으로 예상됩니다:          테이블 잠금 최소화: 개별 UPDATE 문은 해당 레코드에 대한 잠금을 발생시켜 동시성을 저하시킬 수 있지만, 임시 테이블에 데이터를 먼저 준비한 후 한 번에 교체하면 잠금 시간을 최소화할 수 있습니다.      트랜잭션 로그 감소: 수많은 개별 UPDATE 문은 대량의 트랜잭션 로그를 생성하지만, INSERT-SELECT 또는 MERGE 패턴을 사용하면 로그량을 크게 줄일 수 있습니다.      인덱스 부하 감소: 반복적인 UPDATE는 인덱스 재구성 부하를 지속적으로 발생시키지만, 임시 테이블 방식은 인덱스 유지 비용을 일괄 처리할 수 있습니다.      I/O 패턴 최적화: 순차적 I/O는 랜덤 I/O보다 훨씬 효율적인데, 임시 테이블을 생성하고 대량 데이터를 삽입하는 경우는 주로 순차적 I/O 패턴을 따르게 됩니다.      이러한 고급 최적화 전략들은 현재 기본 구현을 완료한 후 성능 테스트를 통해 점진적으로 도입할 예정입니다. 실제 적용 결과와 성능 개선 지표는 후속 포스팅에서 공유하겠습니다.결론Spring Batch 환경에서 대용량 데이터를 효율적으로 처리하기 위해서는 사용 사례에 적합한 기술을 선택하는 것이 중요합니다. JPA는 개발 생산성과 객체 지향적 접근을 제공하지만, 대량 데이터 처리 시에는 성능 제약이 존재합니다. 반면 JDBC 배치 인서트는 로우 레벨 접근을 통해 최적의 성능을 제공하지만 타입 안전성과 유지보수성 측면에서 추가적인 노력이 필요합니다.실무에서는 이러한 트레이드오프를 충분히 이해하고, 처리해야 할 데이터의 볼륨과 성능 요구사항에 따라 적절한 전략을 선택해야 합니다. 특히 대규모 데이터 처리가 필요한 배치 작업에서는 JDBC 배치 인서트와 같은 최적화 기법을 적극적으로 활용하되, QueryDSL과 같은 도구를 함께 도입하여 유지보수성도 함께 확보하는 것이 좋을 것 같습니다.마무리본 포스트가 Spring Batch 환경에서 대용량 데이터 처리 시 발생할 수 있는 성능 병목 현상을 이해하고, 실질적인 최적화 전략을 수립하는 데 도움이 되었으면 좋겠습니다. 해당 사이드 프로젝트를 진행하면서 기술적 편의성과 성능 사이의 균형을 찾는 과정이 엔지니어링의 핵심이라는 것을 다시 한 번 느낄 수 있었습니다. 앞으로도 실무에서 얻은 경험과 학습을 통해 더 나은 기술적 해결책을 모색하고 공유하겠습니다.감사합니다.참고 자료  Hibernate - Enabling statement batching  Batch Insert/Update with Hibernate/JPA  Basic Batch Operations with JdbcTemplate  Optimizing for Random I/O and Sequential I/O]]></content>
      <categories>
        
          <category> Spring Batch </category>
        
      </categories>
      <tags>
        
          <tag> Spring Batch </tag>
        
          <tag> JPA </tag>
        
          <tag> JDBC </tag>
        
          <tag> Batch Insert </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Batch 대용량 처리의 이해]]></title>
      <url>/spring%20batch/2025/03/22/Spring-Batch-high-volume-processing/</url>
      <content type="text"><![CDATA[  스프링 배치를 통한 대용량 처리에 대해 알아봅시다!🚀 들어가기본 포스트에서는 배치 처리의 개념과 사용 시나리오를 중심으로, 일괄 생성, 일괄 수정, 통계 처리에 대해 알아보고 대량 데이터 처리 시 발생하는 일반적인 문제점까지 살펴보도록 하겠습니다.배치 처리의 개념과 사용 시나리오배치(batch) 처리는 특정 시간에 많은 데이터를 일괄적으로 처리하는 프로세스를 의미합니다. 예를 들어, e-커머스 플랫폼에서 오후 4시에 상품 배송 정보를 고객들에게 문자로 일괄 전송하는 경우가 이에 해당합니다. 서버 개발자들은 특정 시간에 일괄 처리해야 하는 작업이 필요할 때 이를 배치 프로세스로 구현하는 것이 일반적입니다.일괄 생성, 일괄 수정, 통계 처리 개념배치 처리는 크게 세 가지 형태로 구분할 수 있습니다.1. 일괄 생성(Read-Create-Write)기존 저장된 정보를 조합해 새로운 정보를 만듭니다. 예를 들어, 주문 정보를 읽어 사용자 정보와 합친 후 문자 정보를 생성하는 경우가 이에 해당합니다.2. 일괄 수정(Read-Update-Write)A 데이터를 읽고 B 데이터를 참고하여 A 데이터를 수정 합니다. 예를 들어, 주문 정보를 읽고 배송 정보를 참고해 주문 정보를 수정합니다.3. 통계 처리(Read-Sum-Write)데이터를 집계하여 통계 형식의 데이터를 만듭니다. 예를 들어, 주문 정보를 GROUP BY 형태로 질의해온 다음 상품별 주문 금액 합산 데이터를 만들 수 있습니다.사이드 프로젝트 소개스프링 배치의 성능을 최적화하는 사이드 프로젝트를 진행했습니다. 이 프로젝트에서는 e-커머스 플랫폼에서 상품을 등록한 판매자에게 일일 정산을 제공하기 위해, 하루 동안 발생한 구매 확정 데이터를 기반으로 매일 특정 시각에 판매자별 일일 정산을 수행하는 배치 프로세스를 구현했습니다.다음 이미지는 일일 정산 배치를 수행하는 각 단계별 상태를 나타낸 다이어그램입니다.  주문-상품 구매 확정 처리: 특정 조건을 만족시키는 주문-상품 건에 대하여 구매 확정 상태로 일괄 수정 합니다.  클레임 처리 완료: 클레임(취소/환불/반품)이 끝난 건에 대하여 클레임 완료 처리 상태로 일괄 수정 합니다.  판매자별 일일 정산 생성: 금일 판매건이 존재하는 판매자에 대하여 일일 정산을 일괄 생성 합니다.  주문-상품 정산 Detail 생성: 구매 확정 상태의 주문-상품 건에 대하여 (+) 정산 처리한 데이터를 일괄 생성합니다.  클레임-환불 정산 Detail 생성: 클레임 처리 완료 건 중 환불건에 대하여 (-) 정산 처리한 데이터를 일괄 생성합니다.  판매자별 정산 Detail 집계: 판매자별로 (+)정산과 (-)정산을 집계하여 통계 데이터를 생성합니다.  판매자별 일일 정산 업데이트: 이전 단계에서 생성해두었던 일일 정산에 집계 데이터를 업데이트합니다.대량 데이터 처리 시 발생하는 일반적인 문제점개발자들은 종종 배치 성능에 상대적으로 무관심한 경향이 있습니다. 이는 배치 작업이 주로 트래픽이 적은 시간대인 새벽에 자동으로 스케줄링되어 실행되기 때문에, 배치 처리 시간이 오래 걸리더라도 큰 문제로 인식되지 않는 경우가 많기 때문입니다. 또한 배치를 개발한 뒤 배포 후 초기에만 로그를 모니터링하다가, 문제가 없다고 확인된 후에는 실제 문제가 발생하기 전까지 잘 살펴보지 않는 관리 소홀로 이어지기도 합니다. 특히 배치 모니터링 환경이 제대로 구축되어 있지 않다면, 문제를 파악하고 해결하는 데 상당한 시간이 소요될 수 있습니다.대량 데이터 처리 시 흔히 발생하는 문제점들은 다음과 같습니다:  처리 시간이 예상보다 크게 길어지는 현상  메모리 부족(OOM, Out Of Memory) 오류 발생  데이터베이스 부하 증가 및 성능 저하  동시에 실행되는 배치 작업 간 자원 경쟁으로 인한 성능 저하  배치 처리 이력 관리 및 모니터링의 어려움사이드 프로젝트를 진행하면서 위 문제들을 어떻게 개선해나갔는지 다음 포스트들에서 하나씩 자세히 살펴보도록 하겠습니다.감사합니다.]]></content>
      <categories>
        
          <category> Spring Batch </category>
        
      </categories>
      <tags>
        
          <tag> Spring Batch </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Batch 효과적인 대량 데이터 리드(Read) 전략]]></title>
      <url>/spring%20batch/2025/03/22/Spring-Batch-Optimized-read-strategy/</url>
      <content type="text"><![CDATA[  스프링 배치를 이용할 때 효과적인 대량 데이터를 읽기 위한 전략에 대해 알아봅시다!🚀 들어가기본 포스트에서는 스프링 배치의 핵심 개념인 청크 프로세싱(Chunk Processing)에 대해 알아보고, 대량의 데이터를 효율적으로 읽기 위해 사이드 프로젝트에서 실제 적용한 전략과 그 성능 개선 결과를 공유하고자 합니다.청크 프로세싱 개념과 중요성1,000만 개의 데이터를 배치 처리한다고 가정해 보겠습니다. 이러한 대량의 데이터를 한 번에 메모리에 로드하는 것은 물리적으로 불가능하거나 심각한 성능 저하를 초래합니다. 따라서 1,000개씩 나누어 총 10,000번에 걸쳐 처리하는 방식이 필요합니다.이렇게 메모리 제약을 고려하여 데이터를 일정 크기로 나누어 순차적으로 처리하는 방식을 청크 프로세싱(Chunk Processing)이라고 합니다. 스프링 배치는 이러한 청크 단위 처리를 기본 아키텍처로 채택하고 있어 대용량 데이터 처리에 최적화되어 있습니다.ItemReader스프링 배치에서 청크 프로세싱을 구현할 때, 데이터 읽기(Read) 역할은 ItemReader 인터페이스를 구현한 구현체가 담당합니다. 저는 초기에 JPA의 개발 편의성과 페이징 기능을 동시에 활용할 수 있는 JpaPagingItemReader를 사용했습니다. 하지만 실제 대용량 데이터 처리 과정에서 다음과 같은 심각한 한계점을 경험했습니다.JpaPagingItemReader의 주요 문제점:  데이터 일관성 문제: 조건절에 사용하는 컬럼이 배치 작업 중 업데이트될 경우 페이지네이션 불일치 발생  JPA 오버헤드: 불필요한 JPA 기능으로 인한 성능 오버헤드  Limit-Offset 방식의 구조적 한계: 오프셋이 증가할수록 기하급수적으로 성능 저하페이지네이션 오류JpaPagingItemReader를 사용하면서 가장 먼저 직면한 문제는 페이지네이션이 정확하게 동작하지 않는 현상이었습니다. 예를 들어, 예상했던 10만 건의 처리 대상 데이터 중 약 2만 건만 처리되고 작업이 종료되는 문제가 발생했습니다.이 문제의 근본 원인은 JpaPagingItemReader의 내부 동작 방식과 OFFSET 기반 페이징의 한계에 있었습니다. 스프링 배치는 각 청크 단위로 읽기(ItemReader) → 처리(ItemProcessor) → 쓰기(ItemWriter) 사이클을 반복합니다. JpaPagingItemReader가 동작할 때 발생하는 주요 이슈는 다음과 같습니다:  JpaPagingItemReader는 자체적으로 EntityManager를 생성하고 관리합니다.  트랜잭션 내에서 처리될 때, 읽어온 엔티티들은 영속 상태로 유지됩니다.  Processor나 Writer에서 엔티티를 업데이트하면, 이 변경사항은 여전히 Reader의 EntityManager에 의해 관리됩니다.  다음 청크를 읽기 위해 ItemReader가 실행될 때, 변경된 내용이 데이터베이스에 반영됩니다.  LIMIT-OFFSET 방식의 페이징은 데이터 변경 후에도 원래 개수만큼 OFFSET을 증가시키므로 데이터 누락이 발생합니다.초기 작성했던 다음 코드는 이러한 문제 상황을 보여줍니다:@Override@NonNullpublic Query createQuery() {    return this.getEntityManager()            .createQuery(                      "SELECT op " +                            "FROM OrderProduct op " +                            "LEFT JOIN Claim cl ON op.orderProductId = cl.orderProductId " +                            "WHERE op.deliveryCompletedAt BETWEEN :startTime AND :endTime " +                            "AND op.deliveryStatus = 'DELIVERED' " +                            "AND op.purchaseConfirmedAt IS NULL " +                            "AND (cl.claimId IS NULL OR cl.completedAt IS NOT NULL) " +                            "ORDER BY op.orderProductId ASC", OrderProduct.class)            .setParameter("startTime", startTime)            .setParameter("endTime", endTime);}위 쿼리와 함께 발생하는 구체적인 문제 시나리오:  조건절에 purchaseConfirmedAt IS NULL 조건이 포함되어 있습니다.  청크 크기를 1,000으로 설정하여 첫 번째 1,000개 레코드(ID 1~1000)를 읽고 처리합니다.  ItemWriter 단계에서 처리된 레코드의 purchaseConfirmedAt 필드가 현재 시간으로 업데이트됩니다.  두 번째 청크 작업 시, JpaPagingItemReader는 다음과 같은 내부 쿼리를 생성합니다:SELECT op FROM OrderProduct opWHERE op.deliveryCompletedAt BETWEEN :startTime AND :endTimeAND op.deliveryStatus = 'DELIVERED'AND op.purchaseConfirmedAt IS NULLAND (cl.claimId IS NULL OR cl.completedAt IS NOT NULL)ORDER BY op.orderProductId ASCOFFSET 1000 LIMIT 1000  그러나 이 시점에서 처음 1,000개 레코드는 이미 purchaseConfirmedAt IS NULL 조건을 더 이상 만족하지 않게 됩니다.  따라서 원래 데이터베이스에서 1001~2000 ID를 가진 레코드들이 아닌, 새롭게 조건을 만족하는 처음 1,000개 중에서 OFFSET 1000을 적용하게 됩니다.  이로 인해 원래 처리되어야 할 데이터 중 일부(OFFSET으로 인해 건너뛰는 데이터)가 누락되는 현상이 발생합니다.이러한 문제의 근본적인 원인은 JpaPagingItemReader가 내부적으로 채택하고 있는 LIMIT-OFFSET 방식의 페이징과 EntityManager의 영속성 관리 방식이 조합되어 발생하는 구조적 한계에 있습니다.따라서 JpaPagingItemReader은 처리 과정에서 조건절의 컬럼이 변경될 경우, 페이지네이션 로직이 정상적으로 동작하지 않아 데이터 누락이 발생합니다.Limit-Offset 방식의 한계JpaPagingItemReader가 사용하는 Limit-Offset 페이지네이션 방식은 대용량 데이터 처리 시 치명적인 성능 저하를 초래합니다. 이 방식의 핵심적인 문제는 오프셋이 커질수록 데이터베이스가 처리해야 하는 작업량이 기하급수적으로 증가한다는 점입니다.예를 들어, LIMIT 1000 OFFSET 9000과 같은 쿼리를 실행할 경우:  데이터베이스는 처음부터 9,000번째 레코드까지 모두 스캔합니다.  그 후 9,001번째부터 10,000번째까지의 1,000개 레코드만 실제로 반환합니다.즉, 오프셋이 커질수록 실제로 필요한 데이터는 일정한데 반해, 스캔해야 하는 데이터는 계속 증가하게 됩니다. 수천만 건의 데이터를 처리해야 하는 배치 작업에서 이러한 방식은 심각한 성능 병목을 야기합니다.Limit-Offset vs Zero-Offset 비교 다이어그램위 다이어그램은 두 방식의 차이점을 명확하게 보여주는 자료입니다. Limit-Offset의 구조적 한계점 때문에 왜 Zero-Offset 방식이 필요한지를 이해할 수 있습니다.Zero-Offset 아이템 리더 구현과 성능 개선JpaPagingItemReader의 한계를 극복하기 위해, 저는 Zero-Offset 방식(또는 키셋 페이징)을 구현한 커스텀 ItemReader를 개발했습니다.Zero-Offset 방식의 핵심 아이디어는 다음과 같습니다:  항상 오프셋을 0으로 유지: 페이지 이동 시에도 항상 OFFSET 0을 사용  ID 기반 필터링: 이전 페이지에서 처리한 마지막 레코드의 ID를 기준으로 다음 데이터를 조회  PK 인덱스 활용: 기본키(PK) 인덱스를 최대한 활용하여 조회 성능 최적화구현을 위한 필수 조건:  PK를 기준으로 데이터를 명시적으로 정렬 (예: ORDER BY id ASC)  각 페이지 조회 시, 이전 페이지의 마지막 ID 값을 기준으로 필터링 조건 추가 (예: WHERE id &gt; :lastId)예를 들어, 첫 번째 페이지에서 1,000건을 조회한 후 마지막 레코드의 ID가 1000이라면, 두 번째 페이지 조회 시에는 다음과 같은 쿼리에 :lastId에 1000을 설정해주면 됩니다.SELECT * FROM order_product WHERE ... AND order_product_id &gt; :lastIdORDER BY order_product_id ASC LIMIT 1000이 방식의 장점은 페이지 번호가 아무리 뒤로 가더라도 항상 인덱스를 타는 효율적인 쿼리가 실행된다는 점입니다. 결과적으로 일관된 성능을 유지할 수 있으며, 데이터 일관성 문제도 해결할 수 있습니다.커서 기반 아이템 리더 활용과 성능 개선Zero-Offset 방식 외에도, 커서 기반(Cursor-based) 아이템 리더를 활용하는 방법도 효과적입니다. 스프링 배치는 JdbcCursorItemReader와 같은 커서 기반 구현체를 제공합니다.커서 기반 방식의 주요 특징:  스트리밍 방식: 데이터베이스 커서를 통해 필요한 만큼만 데이터를 스트리밍 방식으로 가져옴  메모리 효율성: 전체 결과셋을 메모리에 로드하지 않고 필요한 만큼만 가져오므로 메모리 효율적  성능 일관성: Limit-Offset 방식의 성능 저하 없이 대량 데이터 처리 가능🚨 다만, 커서 기반 접근법은 다음과 같은 고려사항이 있습니다:  데이터베이스 연결 유지: 처리가 완료될 때까지 데이터베이스 연결을 유지해야 함. (충분한 Connection-time 설정 필요.)  트랜잭션 범위: 장시간 실행되는 작업의 경우 트랜잭션 관리에 주의 필요.  리소스 관리: 커서를 명시적으로 닫아야 하므로 리소스 관리에 신경 써야 함. (JdbcCursorItemReader는 리소스를 자동으로 정리.)최종 선택: 커서 기반 접근법 JdbcCursorItemReader이론적으로는 Zero-Offset 방식이 매우 효율적이지만, 실제 프로젝트에서는 커서 기반 접근법(Cursor-based approach)을 최종적으로 채택했습니다. 이러한 결정을 내린 주요 이유는 다음과 같습니다:  개발 리소스 효율성: Zero-Offset 방식은 매번 새로운 Reader를 구현할 때마다 상당한 개발 공수가 필요합니다. 반면 Spring Batch에서 기본 제공하는 JdbcCursorItemReader를 사용하면 즉시 활용 가능합니다.  유지보수 용이성: 커스텀 구현체보다 Spring Batch의 공식 컴포넌트를 사용함으로써 유지보수가 용이하고 버전 업그레이드 시 호환성 문제가 적습니다.  성능 대비 비용: Zero-Offset 방식이 이론적으로 약간 더 뛰어난 성능을 보일 수 있지만, 커서 기반 접근법도 충분히 우수한 성능을 제공하면서 개발 비용은 크게 절감할 수 있었습니다.⭐️ 따라서, 성능상 크게 차이가 안 나는 Zero-Offset 방식과 Cursor-Based 방식 중 실용적인 측면을 생각하여 Cursor-based인 JdbcCursorItemReader 를 프로젝트에 적용하였습니다.다음은 실제 구현에 사용한 JdbcCursorItemReader 설정의 코드입니다:@Bean@StepScopepublic JdbcCursorItemReader&lt;OrderProduct&gt; deliveryCompletedJdbcItemReader(        @Value("#{jobParameters['settlementDate']}") String settlementDateStr) {    LocalDate date = JobParameterUtils.parseSettlementDate(settlementDateStr);    LocalDateTime startTime = date.atStartOfDay();    LocalDateTime endTime = date.plusDays(1).atStartOfDay();    // SQL 쿼리 작성    String sql = """        SELECT op.*        FROM order_product op        LEFT JOIN claim cl ON op.order_product_id = cl.order_product_id        WHERE op.delivery_completed_at BETWEEN ? AND ?        AND op.delivery_status = 'DELIVERED'        AND op.purchase_confirmed_at IS NULL        AND (cl.claim_id IS NULL OR cl.completed_at IS NOT NULL)        ORDER BY op.order_product_id ASC        """;    // PreparedStatement 파라미터 설정    Object[] parameters = new Object[] {            startTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss")),            endTime.format(DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"))    };    return new JdbcCursorItemReaderBuilder&lt;OrderProduct&gt;()            .name("deliveryCompletedJdbcItemReader")            .dataSource(dataSource)            .sql(sql)            .preparedStatementSetter(new ArgumentPreparedStatementSetter(parameters))            .rowMapper(new BeanPropertyRowMapper&lt;&gt;(OrderProduct.class))            .build();}  Zero-Offset 방식의 ItemReader를 직접 개발하는 것보다 Spring Batch에서 지원해주는 JdbcCursorItemReader를 사용함으로써 매번 :lastId를 신경쓰지 않고도 편리하게 개발할 수 있습니다.성능 비교: JPA Paging vs Cursor-based실제 동일한 데이터셋(최대 100만 건의 주문 데이터)에 대해 두 가지 방식을 적용한 성능 비교 결과는 다음과 같습니다:참고로, JpaPagingItemReader 읽기가 2만건 처리에 6분이 걸렸고, 그 이상의 데이터는 테스트하기 어려웠던 현실적인 상황을 반영하여 추정치를 표시했습니다.성능 테스트 결과 분석:  JPA Paging: 데이터량이 증가할수록 처리 시간이 기하급수적으로 증가함.  Cursor-based: 빠른 처리 속도를 보이며, 데이터량이 증가하더라도 처리 시간이 선형적으로 증가함.보완할 점현재 구현에서 가장 큰 개선점은 타입 안전성입니다. 문자열 기반 쿼리를 사용하는 현재 방식은 오타나 컬럼명 변경 시 컴파일 타임에 오류를 잡아내기 어렵습니다.향후 개선 방향:  QueryDSL 도입: Java 코드로 타입 안전한 쿼리를 작성하여 컴파일 타임에 오류 감지  테스트 커버리지 강화: 다양한 데이터 패턴에 대한 테스트 케이스 추가  모니터링 기능 개선: 배치 작업의 진행 상황 및 성능 지표 실시간 모니터링Kotlin 기반 프로젝트의 경우, Exposed 라이브러리를 활용하면 더욱 간결하고 타입 안전한 방식으로 쿼리를 작성하실 수 있으니 참고하시면 좋을 것 같습니다.⭐️ 결론대용량 데이터 처리 시 Spring Batch의 ItemReader 전략은 전체 배치 프로세스의 성능과 안정성에 결정적인 영향을 미칩니다. 특히 Limit-Offset 방식의 페이징 대신 Zero-Offset 또는 커서 기반 접근법을 활용함으로써 다음과 같은 이점을 얻을 수 있습니다:  일관된 성능: 처리 데이터량에 관계없이 예측 가능한 성능 유지  데이터 일관성: 배치 처리 중 데이터 변경에도 페이지네이션 정확성 보장  리소스 효율성: 데이터베이스와 애플리케이션 서버의 리소스 효율적 활용대규모 배치 작업을 설계할 때는 단순히 코드 작성의 편의성보다 성능과 확장성을 우선적으로 고려하는 것이 중요합니다. 본 포스트에서 소개한 전략들이 효율적인 배치 시스템 구축에 도움이 되길 바랍니다.참고 자료  Batch Performance 극한으로 끌어올리기: 1억 건 데이터 처리를 위한 노력 / if(kakao)dev2022  Spring Batch 애플리케이션 성능 향상을 위한 주요 팁 (kakao tech)  Spring Batch Documentation: Cursor-based ItemReader Implementations]]></content>
      <categories>
        
          <category> Spring Batch </category>
        
      </categories>
      <tags>
        
          <tag> Spring Batch </tag>
        
          <tag> Zero Offset </tag>
        
          <tag> Limit-Offset </tag>
        
          <tag> Cursor-Based </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Boot + Prometheus + Grafana로 마이크로서비스 모니터링 시스템 구축]]></title>
      <url>/msa/monitoring/prometheus/grafana/2025/02/24/Prometheus-Grafana-Monitoring/</url>
      <content type="text"><![CDATA[  Prometheus와 Grafana를 활용하여 Spring Boot 기반 마이크로서비스를 모니터링 하는 방법에 대해 알아봅시다!📌 들어가기현대의 마이크로서비스 아키텍처는 여러 개의 독립적인 서비스가 유기적으로 협력하여 하나의 큰 시스템을 구성합니다. 마이크로서비스 환경에서는 각 서비스가 독립적으로 배포되고 운영되기 때문에, 특정 서비스의 장애나 성능 저하가 전체 시스템에 영향을 줄 수 있습니다. 만약 모니터링 시스템이 없다면 서비스 장애나 성능 저하 발생 시 원이 파악에 오랜 시간이 소요되어 비즈니스에 심각한 영향을 미칠 수 있습니다. 이런 서비스 장애가 지속되면 사용자 경험이 악화되고, 결과적으로 서비스의 신뢰를 잃어버리는 최악의 상황이 올 수 있습니다. 따라서 MSA 환경에서는 각 서비스의 상태와 성능을 실시간으로 모니터링하는 것이 필수적입니다.본 포스트에서는 Prometheus와 Grafana를 활용하여 Spring Boot 기반 마이크로서비스를 모니터링하는 방법에 대해 알아보도록 하겠습니다.Promethus &amp; Grafana우선 Prometheus와 Grafana가 각각 무엇인지 알아봅시다.Prometheus란?Prometheus는 오픈소스 모니터링 및 경고 도구로, 주로 시계열 데이터를 수집하여 저장, 쿼리, 분석하는 데 사용되는 저장소입니다. 특히 시계열 데이터에 특화된 쿼리 언어인 PromQL을 제공하여, 수집한 메트릭을 조회할 수 있습니다.Grafana란?Grafana는 Prometheus에서 수집한 데이터를 직관적인 대시보드 형태로 시각화하는 시각화 도구입니다. Prometheus 뿐만 아니라 다양한 데이터 소스와의 연동도 지원합니다. 대시보드를 커스텀할 수 있을 뿐만 아니라 경고 시스템도 구축 가능합니다.Spring Boot와 Prometheus 연동Spring Boot는 Actuator 모듈을 통해 애플리케이션의 다양한 상태 정보(metrics)를 쉽게 노출할 수 있습니다. 이를 활용하여 Prometheus가 Pull 방식으로 메트릭 데이터를 수집할 수 있도록 설정할 수 있습니다.Actuator 모듈을 통해 스프링 애플리케이션의 메트릭 정보들을 Prometheus에게 노출하기 위해 다음 의존성을 추가해줘야 합니다.implementation 'org.springframework.boot:spring-boot-starter-actuator'implementation 'io.micrometer:micrometer-registry-prometheus'그리고, Prometheus가 Pull 방식으로 메트릭을 조회할 Actuator 엔드포인트를 활성화시켜 줘야 합니다.management:  endpoints:    web:      exposure:        include: prometheus마지막으로 Promethues.yml 설정 파일을 통해 Prometheus가 Pull 하고자 하는 애플리케이션의 위치 정보를 설정해주어야 합니다.아래 설정은 동일한 도커 네트워크 상에 존재하는 user service, product service, order service에 대한 메트릭을 Pull을 5초 간격으로 진행하겠단 설정입니다.global:  scrape_interval: 15s  external_labels:    monitor: 'codelab-monitor'scrape_configs:  - job_name: 'prometheus'    scrape_interval: 5s    static_configs:      - targets: ['localhost:9090']  - job_name: 'user-actuator'    metrics_path: '/actuator/prometheus'    scrape_interval: 5s    static_configs:      - targets: ['user-service:8080']      - targets: ['product-service:8081']      - targets: ['order-service:8082']만약 로컬에 설치된 Prometheus가 아닌 도커로 실행 시, Prometheus의 /etc/prometheus/prometheus.yml 경로와 커스텀 설정 파일을 볼륨 마운팅 해주어야 합니다.version: '3.8'services:  prometheus:    image: prom/prometheus:latest    container_name: prometheus    volumes:      - ./prometheus.yml:/etc/prometheus/prometheus.yml    ports:      - "9090:9090"위 설정이 완료되면 Prometheus는 http://&lt;호스트&gt;:&lt;포트&gt;/actuator/prometheus 엔드포인트를 통해 Spring Boot 애플리케이션들의 메트릭 데이터를 수집할 수 있습니다.Prometheus 메트릭 -&gt; Grafana 시각화메트릭 데이터들이 Prometheus에 저장되면 이와 연동하여 Grafana를 통해 시각화할 수 있습니다.Grafana 관리 페이지에서 데이터 소스로 Prometheus를 추가한 다음, URL에 Prometheus 서버의 주소(http://&lt;prometheus-host&gt;:9090)를 입력합니다.대시보드를 개인이 직접 커스터마이징할 수도 있지만, 이미 잘 만들어진 대시보드 템플릿을 가져다 사용하면 편리합니다.🚀 결론Prometheus와 Grafana를 활용한 모니터링 시스템은 마이크로서비스 환경에서 서비스의 상태를 실시간으로 확인하고, 장애 발생 시 빠른 대응을 가능하게 합니다. 모니터링 시스템은 필수적이며, 이를 부재할 경우 심각한 비즈니스 리스크를 초래할 수 있다는 것을 명심해야 됩니다.추가로 공부하면 좋을 내용  Grafana Alerting 기능 (특정 메트릭 값이 임계치 초과 시 알림 발송)]]></content>
      <categories>
        
          <category> MSA </category>
        
          <category> Monitoring </category>
        
          <category> Prometheus </category>
        
          <category> Grafana </category>
        
      </categories>
      <tags>
        
          <tag> Prometheus </tag>
        
          <tag> Grafana </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[ELK 스택을 활용한 MSA 중앙 집중식 로그 모니터링]]></title>
      <url>/msa/elk/2025/02/24/ELK-Stack-Log-Analysis/</url>
      <content type="text"><![CDATA[  마이크로서비스 아키텍처에서 ELK(Elasticsearch + Logstash + Kibana) 스택을 활용한 중앙 집중식 로그 모니터링에 대해 알아봅시다!📌 들어가기최근 마이크로서비스 아키텍처가 널리 사용되면서, 다양한 서비스에서 발생하는 로그를 효과적으로 수집 및 분석할 수 있는 방법이 필요해졌습니다. 마이크로서비스 환경에서는 각 서비스마다 별도의 로그 파일이 생성됩니다. 개별 서버마다 분산되어 저장된 로그 파일을 일일이 분석하는 것은 매우 고된 일이 될 것입니다. 그리고 문제 발생 시 신속하게 로그를 추적하여 원인을 파악하고 대응하는 데에 많은 시간이 소모될 것입니다. 이처럼 MSA 환경에서 로그를 중앙 집중식으로 관리하지 않는다면 여러 가지 문제가 발생할 수 밖에 없습니다.이에 따라 ELK(Elasticsearch + Logstash + Kibana)를 활용한 중앙 집중식 로그 모니터링의 필요성이 더욱 부각되고 있습니다.본 포스트에서는 ELK 스택 기반의 로그 모니터링 구성과, Zipkin 로그 트레이싱과의 연계를 통한 효율적인 모니터링 방법에 대해 살펴보도록 하겠습니다. Zipkin 로그 트레이싱과 관련한 포스트가 있으니 참고하시면 좋을 것 같습니다.ELK 도입의 효과Logstash를 통한 로그 수집 및 필터링Logstash는 다양한 소스의 로그를 실시간으로 수집하고, 필터링 및 가공하여 일관된 포맷으료 변환해줍니다. 이를 통해 수집된 로그의 품질을 높이고, 분석의 효율성을 높일 수 있습니다. 개인 사이드 프로젝트에서는 각 서비스가 로그 파일을 각 서버가 저장하도록 했고, 해당 로그 파일을 Logstash가 수집하도록 설정했습니다.Elasticsearch 기반의 빠른 검색Elasticsearch는 분산형 검색 엔진으로, 대용량 로그 데이터를 빠르게 인덱싱 및 검색할 수 있습니다. 덕분에 서비스 장애 시 빠른 속도의 로그 검색으로 인하여 문제를 신속하게 대응할 수 있게 됩니다.Kibana 대시보드를 통한 시각화Kibana는 Elasticsearch에 저장된 로그 데이터를 시각화하여 대시보드를 구성할 수 있게 해줍니다. 이를 통해 로그의 흐름과 이상 징후를 한눈에 파악할 수 있으며, 실시간 모니터링 환경을 구축할 수 있습니다.Logback과 ELK 연동 및 구성 방법Logback이란?스프링 부트는 로깅 시스템의 기본 구현체로 Logback을 사용하도록 설정되어 있습니다. 기본적인 성능과 기능이 좋기 때문에 많이 사용되고 있습니다. 만약 설정을 커스텀하고 싶다면 logback.xml 파일로 설정을 조정하실 수도 있습니다.스프링 부트 프로젝트에서 개발 편의성을 위해 Lombok을 대부분 사용하실 겁니다. 해당 라이브러리에는 Slf4j(Simple Logging Facade For Java)라는 로그 시스템에 대한 추상화 계층을 제공하는 인터페이스가 존재합니다. Slf4j의 기본 구현체는 Logback으로 설정되어 있습니다. Slf4j 자체는 인터페이스이므로 로깅을 수행하지 않고, 실제 로깅을 수행하는 구현체(ex: Logback, Log4j 등)에게 위임합니다. 만약 로깅 구현체를 교체하고 싶다면, 의존성을 변경함으로써 쉽게 교체할 수 있습니다.Logback -&gt; Logstash 로그 전송logback 로깅 시스템을 통해 Logstash로 로그를 전송하기 위해선 다음 의존성부터 추가해줘야 합니다. 그런 다음 logback.xml 설정에 Logstash로 로그를 출력하는 Appender를 추가해주면 됩니다.implementation 'net.logstash.logback:logstash-logback-encoder:8.0'아래 XML 설정은 스프링 애플리케이션에 설정된 logback.xml 설정입니다. 해당 설정은 실시간으로 발생되는 로그를 콘솔, 파일, Logstash에 각각 출력하는 설정입니다.&lt;configuration&gt;    &lt;springProperty scope="context" name="applicationName" source="spring.application.name" defaultValue="defaultAppName" /&gt;    &lt;springProperty scope="context" name="logstashDestination" source="logstash.destination" defaultValue="localhost:5044" /&gt;    &lt;property name="LOG_FILE" value="application.log"/&gt;    &lt;!-- Logstash로 전송할 Appender --&gt;    &lt;appender name="LOGSTASH" class="net.logstash.logback.appender.LogstashTcpSocketAppender"&gt;        &lt;destination&gt;${logstashDestination}&lt;/destination&gt;        &lt;encoder class="net.logstash.logback.encoder.LogstashEncoder" /&gt;    &lt;/appender&gt;    &lt;!-- 콘솔 출력 --&gt;    &lt;appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender"&gt;        &lt;encoder&gt;            &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} %5p [${applicationName:-},%X{traceId:-},%X{spanId:-}] [%thread] %logger{36} - %msg%n&lt;/pattern&gt;        &lt;/encoder&gt;    &lt;/appender&gt;    &lt;!-- 파일 출력 --&gt;    &lt;appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt;        &lt;file&gt;${LOG_FILE}&lt;/file&gt;        &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt;            &lt;fileNamePattern&gt;application.%d{yyyy-MM-dd_HH-mm}.log.gz&lt;/fileNamePattern&gt;            &lt;maxHistory&gt;2&lt;/maxHistory&gt;        &lt;/rollingPolicy&gt;        &lt;encoder&gt;            &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss} %5p [${applicationName:-},%X{traceId:-},%X{spanId:-}] [%thread] %logger{36} - %msg%n&lt;/pattern&gt;        &lt;/encoder&gt;    &lt;/appender&gt;    &lt;!-- Logger 설정 --&gt;    &lt;root level="info"&gt;        &lt;appender-ref ref="CONSOLE" /&gt;        &lt;appender-ref ref="FILE" /&gt;        &lt;appender-ref ref="LOGSTASH" /&gt;    &lt;/root&gt;&lt;/configuration&gt;  참고로 Logstash 의 기본 Port는 5044로 구동됩니다. 사이드 프로젝트에서 도커로 Logstash를 구동하여 실습을 진행했습니다.위 처럼 설정을 마치면 이제 스프링 애플리케이션의 로깅 시스템(Logback)이 자동으로 로그를 Logstash 로 전송할 것입니다.Logstash -&gt; Elasticsearch 로그 저장Logstash는 단순히 데이터 파이프라인 역할만 하기 때문에, 결국 로그를 저장할 데이터베이스가 필요합니다. 여기서 Elasticsearch가 로그를 저장하는 역할을 맡게 됩니다. 엘라스틱서치는 앞서 말했듯이, 대용량 로그 데이터를 빠르게 인덱싱 및 검색할 수 있습니다. 덕분에 찾고자 하는 로그를 빠르게 탐색할 수 있게 되는 것이죠.아래는 Logstash의 설정 파일입니다. 애플리케이션 -&gt; Logstash (수집) -&gt; Elasticsearch (저장)처럼 일련의 데이터 파이프라인의 설정 값입니다.  이를 Logstash를 구동시킬 때 설정해주어야 하는 값입니다.input {    tcp {        port =&gt; 5044        codec =&gt; json    }}output {    elasticsearch {        hosts =&gt; ["http://elasticsearch:9200"]        index =&gt; "application-logs-%{+YYYY.MM.dd}"    }}  input: 해당 설정으로 데이터를 입력받겠다는 정보를 나타냅니다. (TCP 5044, JSON 포맷으로 데이터 입력 받음.)  output: 출력하고자 하는 엘라스틱서치 저장소의 정보를 적어줍니다.          hosts: 엘라스틱서치의 호스트 정보를 적어줍니다. 위 설정은 동일한 도커 네트워크 상에서 구동되고 있는 환경이기 때문에 도커 컨테이너명을 사용했습니다.      index: 엘라스틱서치의 어떤 인덱스에 저장할 지 명시해줍니다. (RDBMS의 테이블명이라고 보시면 됩니다.)      위 처럼 logstash.conf 파일을 설정했다면, 이제 Logstash가 수집한 로그 데이터를 Elasticsearch 저장소로 전송할 것입니다.만약, Logstash를 도커로 구동하신다면 아래 도커 컴포즈 파일처럼 logstash.conf 파일을 볼륨 마운팅 해주셔야 합니다.version: '3'services:  logstash:    image: docker.elastic.co/logstash/logstash:8.10.0    container_name: logstash    ports:      - "5044:5044"    volumes:      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.confElasticsearch -&gt; Kibana 로그 데이터 시각화이제 Elasticsearch에 로그 데이터가 적재된다면, Kibana를 활용하여 로그 데이터를 시각화할 수 있습니다.이때 Zipkin을 활용한 분산 로그 트레이싱을 통합 적용한다면, 여러 서비스에 걸쳐 처리될 때 발생하는 로그의 흐름을 추적할 수 있도록 도와줍니다. 각 요청에 고유한 TraceID를 부여하여, 여러 서비스에 분산되어 기록된 로그들을 Elasticsearch에서 하나의 흐름으로 연결지어 검색할 수 있습니다.Kibana의 Discovery 서비스에서 TraceID를 입력하면 해당 요청에 관련된 모든 로그가 타임스탬프 순서대로 정렬시켜 모니터링 할 수 있습니다. 이를 통해 단일 요청이 서비스 전반에서 어떻게 처리되었는지 한눈에 파악할 수 있게 됩니다.또한, 로그 레벨의 분류를 통해 전체 로그에 대한 통계를 대시보드로 확인도 가능합니다.🚀 결론ELK 스택을 기반으로 한 중앙 집중식 로그 모니터링은 MSA 환경에서 필수적인 요소라고 생각합니다. 분산된 로그를 한눈에 파악할 수 없는 문제를 해결해주고, 실시간 모니터링 대응 능력을 향상시켜줌과 동시에, Zipkin과의 통합을 통해 요청의 전체 흐름을 추적할 수 있다는 점은 시스템 운영의 효율성을 크게 향상시킬 수 있습니다. ELK스택과 Zipkin과 같은 도구들을 적절히 활용한다면, 장애 발생 시 빠른 원인 분석과 문제 해결이 가능해져, 전체 서비스의 안정성 유지에 기여할 수 있을 것입니다.이상으로 ELK 스택과 Zipkin 통합을 통한 중앙 집중식 로그 모니터링의 필요성과 장점에 대해 알아보았습니다. 본 포스트를 참고하여 여러분의 환경에 맞는 최적의 로그 관리 시스템 구축에 도움이 되었으면 좋겠습니다. 감사합니다.📂 참고 자료  인프런 - 개발자에게 필요한 로그 관리  LINE 광고 플랫폼의 MSA 환경에서 Zipkin을 활용해 로그 트레이싱하기]]></content>
      <categories>
        
          <category> MSA </category>
        
          <category> ELK </category>
        
      </categories>
      <tags>
        
          <tag> Elatsticsearch </tag>
        
          <tag> Logstash </tag>
        
          <tag> Kibana </tag>
        
          <tag> Logback </tag>
        
          <tag> Zipkin </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Zipkin을 활용한 마이크로서비스 분산 트레이싱]]></title>
      <url>/spring%20cloud/zipkin/msa/2025/02/23/Zipkin-Distributed-Tracing/</url>
      <content type="text"><![CDATA[  분산 환경에서 Zipkin을 활용한 요청의 전체 흐름을 추적해봅시다!📌 들어가기마이크로서비스 아키텍처는 독립적으로 배포되고 관리되는 작은 서비스들이 모여 애플리케이션을 구성하게 됩니다. 이러한 환경에서는 서비스 간의 통신이 빈번하게 발생하며, 각 서비스가 독자적인 로그를 남기기 때문에 전체 트랜잭션의 흐름을 파악하기가 어렵습니다. 예를 들어, 주문 서비스 -&gt; 결제 서비스 -&gt; 배송 서비스 등의 여러 서비스가 순차적으로 호출되는 과정에서 각 서비스는 별도의 로그를 각자 남기기 때문에 어떤 서비스에서 문제가 발생했는지 확인하기 어렵습니다.이러한 문제점 때문에 분산 트레이싱은 문제 발생 시 원인을 신속하게 파악하고, 성능 병목을 찾아내는 데 필수적인 도구로 등장합니다.본 포스트에서는 분산 트레이싱 도구 중 Zipkin을 활용한 분산 트레이싱을 다루도록 하겠습니다.마이크로서비스 환경의 복잡성마이크로서비스 아키텍처에서는 하나의 사용자 요청이 여러 서비스를 거치며 처리됩니다. 주문, 결제, 배송 서비스 등 여러 서비스를 거치게 되는 것이죠. 이러한 분산된 호출 구조는 여러 문제점을 유발할 수 있습니다.각 서비스마다 별도의 로그를 남기기 때문에 하나의 트랜잭션이 어디에서 지연되거나 장애가 발생했는지 확인하기 어렵습니다. 그리고 전체 흐름에서 어느 부분이 응답 시간을 지연시키는지도 파악하기 어렵습니다.모놀리식(Monolihic)의 경우 하나의 서비스로 애플리케이션이 구동되기 때문에, 전체 흐름에 대한 로그를 하나의 서비스에서 디버깅 할 수 있습니다. 하지만 MSA에서는 문제를 파악하기 위해 관련된 모든 서비스들의 로그를 하나하나 다 살펴봐야 하는 번거로움이 존재합니다.만약 분산 트레이싱 도구가 없다면, 위와 같은 문제를 해결하기 위해 각 서비스에서 개별적으로 로그를 분석해야 하며, 이는 시간 소모적이이고 비효율적일 뿐만 아니라, 문제 진단의 어려움이 따르기 때문에 신속하게 대응하지 못할 위험이 높아집니다.Zipkin을 활용한 요청 추적Zipkin은 오픈 소스 분산 트레이싱 시스템으로, 마이크로서비스 환경에서 서비스 간 호출 흐름을 시각화하고 분석할 수 있도록 도와주는 도구입니다. Zipkin을 활용하면 각 서비스의 호출 데이터를 중앙에서 수집하여 아래 문제점들을 해결해줍니다.  전체 호출 흐름 시각화: 사용자 요청이 어떤 경로로 전달되는지 시각화하여 한 눈에 파악 할수 있도록 해줍니다.  Latency 분석: 각 서비스 간 호출의 응답 시간 정보를 제공하여 어떤 서비스의 어떤 로직에서 성능 병목이 일어나는지 쉽게 식별할 수 있습니다.  장애 원인 분석: 트랜잭션 중 발생한 예외나 오류를 신속하게 탐지하고, 어느 서비스에서 문제가 발생했는지 분석 할 수 있습니다.Zipkin 적용 방법예전에는 Spring Boot 애플리케이션에 쉽게 분산 트레이싱 기능을 추가할 수 있도록 도와주는 Spring Cloud Sleuth라는 라이브러리를 사용했었습니다. 하지만 최근 버전으로 업데이트 되면서 Spring Cloud Sleuth는 Deprecated 될 라이브러리가 되었습니다. 따라서 최근 버전에서는 Micrometer로 분산 트레이싱 기능을 추가하도록 변경되었습니다.Dependency	implementation 'org.springframework.boot:spring-boot-starter-actuator'	implementation 'io.micrometer:micrometer-observation'	implementation 'io.micrometer:micrometer-tracing-bridge-brave'	implementation 'io.zipkin.brave:brave-instrumentation-spring-web'	implementation 'io.zipkin.reporter2:zipkin-reporter-brave'application.ymlspring:  application:    name: product-servicemanagement:  tracing:    sampling:      probability: 1.0    propagation:      type: b3  zipkin:    tracing:      endpoint: ${MANAGEMENT_ZIPKIN_TRACING_ENDPOINT:http://localhost:9411/api/v2/spans}  여기서 spring.application.name은 서비스 태깅에 활용되며, 각 서비스의 이름이 Zipkin UI에 표시되어 호출 흐름을 쉽게 파악할 수 있습니다.  위와 같이 의존성과 설정을 마치면, 자동으로 각 요청에 고유한 Trace ID와 Span ID를 부여하여, 서비스 간의 호출 관계를 추적하게 됩니다.Zipkin UI를 활용한 트레이스 분석서비스 간 호출 흐름 시각화Zipkin UI는 수집된 트레이스 데이터를 기반으로, 서비스 간 호출 흐름을 직관적으로 시각화해줍니다. 개발자는 UI를 통해 각 서비스가 호출한 순서와 관계를 그래픽으로 확인할 수 있습니다. 또한 특정 트랜잭션의 세부적인 스팬 정보와 타임라인을 분석할 수도 있습니다.Latency 분석 및 장애 감지Zipkin UI에서는 각 스팬의 응답 시간을 시각적으로 표시하여, 어느 부분에서 지연이 발생했는지 쉽게 식별할 수 있습니다. 이를 통해 성능 병목 구간을 찾아내고, 장애 발생 시 원인 분석에 필요한 정보를 제공합니다. 만약 이러한 시각화 도구가 없으면, 로그 파일만으로 각 서비스 간의 호출 관계와 지연 시간을 분석해야 하므로, 문제 발생 시 즉각적인 대응이 어려워질 것입니다.Spring AOP를 활용한 미들웨어 간의 요청 추적마이크로서비스에서는 HTTP 호출 외에도 Kafka, Redis와 같은 미들웨어를 활용하는 경우가 많습니다. Zipkin과 Micrometer의 기본 의존성과 설정만으로는 미들웨어 통신 경로를 자동 추적하진 않습니다. 미들웨어간의 호출 흐름까지 추적하기 위해, 추적에 필요한 로직을 스프링 AOP를 활용하여 사용자 정의 트레이싱 코드를 삽입할 수 있었습니다.예를 들어, 스프링 AOP를 활용하여 Kafka를 통한 메시지 발행/구독 흐름도 추적할 수 있습니다. 이를 통해 메시지 큐를 통한 비동기 호출도 명확하게 추적할 수 있으며, 메시지 손실이나 지연 문제를 쉽게 추적할 수 있습니다.만약 이러한 미들웨어 추적 메커니즘이 없다면, 각 미들웨어 간의 호출 흐름이 단절되어 문제 발생 시 원인 파악에 큰 어려움이 따를 것입니다. 이는 디버깅 하는 시간이 급격히 증가할 수 있습니다.🚀 결론마이크로서비스 아키텍처에서는 서비스 간 호출 흐름이 복잡해짐에 따라, 단순 로그 분석만으로는 문제의 원인을 파악하기 어렵습니다. Zipkin과 Micrometer를 활용하면, 전체 트랜잭션의 흐름을 시각화하고 각 서비스의 성능을 모니터링할 수 있어 문제 발생 시 신속하게 대응할 수 있습니다. 이러한 도구들이 없다면, 시스템 장애나 성능 병목을 찾아내는 데에 많은 시간과 리소스가 소모되며, 이는 비즈니스에 심각한 영향을 미칠 것입니다. MSA 환경에서 Zipkin을 통한 분산 트레이싱은 단순히 로그를 남기는 것을 넘어, 시스템의 전체적인 건강 상태를 관리하고, 장애 발생 시 신속한 원인 분석 및 대응 전략을 마련하는 데 필수적인 요소라 볼 수 있습니다.참고 자료  LINE 광고 플랫폼의 MSA 환경에서 Zipkin을 활용해 로그 트레이싱하기  b3-propagation]]></content>
      <categories>
        
          <category> Spring Cloud </category>
        
          <category> Zipkin </category>
        
          <category> MSA </category>
        
      </categories>
      <tags>
        
          <tag> Distributed Tracing </tag>
        
          <tag> b3 propagation </tag>
        
          <tag> Micrometer </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Cloud + Kafka로 구현하는 마이크로서비스 이벤트 아키텍처]]></title>
      <url>/spring%20cloud/msa/kafka/2025/02/22/Spring-Cloud-Kafka-EventArchitecture/</url>
      <content type="text"><![CDATA[  Kafka를 이용한 이벤트 드리븐 아키텍처의 마이크로서비스 통신에 대해 알아봅시다!📌 1. 들어가기1-1. 왜 마이크로서비스 간 통신 방식이 중요할까요?마이크로서비스 아키텍처는 애플리케이션을 작은 독립적인 서비스로 나누어 개발, 배포 및 확장성을 극대화할 수 있게 해줍니다.하지만 서비스 간에 데이터와 명령을 어떻게 주고받을지가 시스템의 확장성, 성능 그리고 신뢰성에 영향을 미칩니다. 그래서 서비스 간 통신 방식에 대한 고민이 중요한 포인트가 됩니다.가장 흔히 사용되는 두 가지 통신 방식은 다음과 같습니다.  REST API 기반 통신 (동기 방식, Synchronous)  Message Queue를 활용한 Event driven 통신 (비동기 방식, Asynchronous)이전에 MSA에 관한 통신 방법들의 장단점을 비교하는 포스트를 작성했었습니다. 이번 포스트와 함께 읽어보시면 좋을 것 같습니다.1-2. REST API 기반 통신의 한계점REST API를 활용한 동기 통신은 마이크로서비스 초기 구현 시 간단하게 적용할 수 있고, HTTP를 이용한 요청/응답 패턴은 친숙하기에 가장 많이 선택되는 방식입니다.하지만, 서비스 규모가 커질수록 REST API 통신의 한계점에 직면하게 됩니다.강한 결합 (Tightly Coupled)  문제: 서비스 A가 서비스 B의 API를 호출해야 한다면, 서비스 B가 반드시 정상 동작하고 있어야 합니다.  예시: Order Service가 Payment Service의 API를 호출했는데, 결제 서비스가 다운되면 주문 생성 자체가 실패하게 됩니다.확장성 및 성능 이슈  문제: 동기식 호출로 인해 요청이 병목 지점이 됩니다.  예시: Order Service가 주문 생성 -&gt; 결제 요청 -&gt; 배송 요청까지 순차적으로(동기식)으로 호출한다면, 전체 응답 시간이 길어지게 됩니다. 결국 전체 트랜잭션이 느려지고, 동시에 많은 요청을 처리하기 어려워질 수 있습니다.장애 전파  문제: 하나의 서비스 장애가 연쇄적으로 다른 서비스에 영향을 줄 수 있습니다. 이는 단일 장애가 전체 시스템의 문제로 확산될 수 도 있습니다.  예시: 결제 시스템이 느려지거나 장애가 발생하면, 주문 처리 시스템도 정상적으로 작동할 수 없습니다.높은 종속성  문제: API 구조 변경 시, 이를 호출하는 모든 서비스에 영향을 미칩니다. 이로 인해 각 서비스의 독립성이 낮아지고, 배포시 서비스 간의 조율이 필요합니다.1-3. ✅ 요약  REST API 방식은 단순하고 직관적이지만, 확장성, 신뢰성, 유연성 측면에서 한계가 발생합니다.2. Kafka 기반 이벤트 드리븐(Event-driven) 아키텍처앞서 살펴본 동기식 통신의 한계점을 극복하기 위해 대규모의 MSA 환경에서는 Kafka 기반의 비동기 이벤트 통신을 사용하는 것이 적합합니다.Kafka는 메시지 브로커 역할을 수행하면서, 서비스 간에 비동기 메시지를 전달해주는 역할을 합니다. 서비스들은 토픽(Topic)에 메시지를 발행(Publish)하거나 구독(Subscribe)함으로써 서로 통신할 수 있게 됩니다.2-1. 이벤트 드리븐(Event-driven) 아키텍처의 장점느슨한 결합 (Loosely Coupled)  장점: 서비스 간 직접적인 호출이 없으므로, 서로의 존재를 몰라도 됩니다. 따라서 특정 서비스가 다운되더라도 전체 시스템에는 영향이 적습니다.  예시: Order Service는 ORDER_CREATED 주문 생성 이벤트만 발행하고, 이를 Payment Service에서 구독해서 처리합니다. 즉, 각 서비스들은 독립적으로 이벤트를 처리합니다.비동기 처리로 인한 성능 향상  장점: 요청을 큐에 넣고 바로 응답을 줄 수 있어, 빠른 처리가 가능합니다. 빠른 응답은 사용자의 경험을 개선해줍니다.  예시: 사용자가 주문을 생성하면, 주문 ID를 바로 응답받고, 결제 및 배송처리는 백엔드에서 비동기로 진행됩니다.장애 격리 및 내결함성 (Fault Tolerance)  장점: Kafka에 발행된 메시지는 디스크에 저장되므로, 구독자가 잠시 다운되어도 메시지 재처리가 가능합니다. 따라서 서비스 장애 발생 시에도 데이터 유실 없이 복구가 가능합니다.  예시: Payment Service가 다운되어도, Kafka에 쌓인 ORDER_CREATED 메시지를 서비스 복구 후 다시 처리할 수 있습니다.확장성과 유연성  장점: 서비스 간에 직접적인 통신을 하지 않기 때문에 확장성이 높아지고, 독립적으로 서비스 배포가 가능합니다. 이로써 새로운 서비스를 쉽게 추가할 수 있습니다.  예시: Order Service에서 발행하는 이벤트를 구독하는 새로운 서비스를 추가해도 기존 시스템에 영향이 없습니다.2-2. ✅ 요약  Kafka 기반 이벤트 드리븐 아키텍처는 확장성, 유연성, 내결함성 등 여러 측면에서 유리합니다.3. 개선 과정: REST API → Kafka 기반 아키텍처로 전환3-1. 기존 구조: REST API 기반Order -&gt; Payment -&gt; Delivery 흐름을 REST API로 처리하면 다음 과정들이 동기식으로 이루어집니다.  사용자가 주문 생성 요청 -&gt; Order Service  Order Service가 결제 요청 -&gt; Payment Service  결제 성공 시 배송 요청 -&gt; Delivery ServiceREST API 기반 방식의 문제점은 서비스 간 강한 결합으로 인해, 일부 과정 중 장애가 발생 시 전페 프로세스가 실패하게 됩니다. 즉 전체 트랜잭션이 모든 서비스들의 성공 여부에 종속하게 됩니다.3-2. 개선된 구조: Kafka 이벤트 드리븐 기반Kafka를 도입하여 각 단계에서 이벤트를 발행하고, 필요한 서비스가 이를 구독하는 구조로 전환합니다.  사용자가 주문 생성 요청을 하면 Order Service는 ORDER_CREATED 이벤트를 Kafka에 발행합니다.  Payment Service가 주문 생성 메시지를 구독하여 결제 처리를 진행합니다.  결제 성공 시 PAYMENT_COMPLETED 결제 성공 이벤트를 발행합니다.  Delivery Service가 결제 성공 이벤트를 구독하여 배송 요청을 처리합니다.Kafka를 도입함으로써 비동기 처리로 인한 빠른 사용자 응답이 가능해집니다. 서비스 간 느슨한 결합과 Kafka의 내결함성 덕분에 장애 전파 영향도 최소화 됩니다. 그리고 각각의 서비스들이 독립적으로 배포가 가능하고, 신규 서비스 추가가 용이해집니다.4. 🚨 Kafka 기반 이벤트 아키텍처 설계 시 고려사항메시지 중복 처리  이벤트가 중복으로 전달될 수 있기 때문에, 수신자 측에 중복 방지 로직을 구현해야 합니다.데이터 정합성  비동기 구조로 인해 데이터 정합성이 깨질 수 있습니다.  해결책으로 Sage 패턴 또는 Outbox 패턴을 도입할 수 있습니다.에러 핸들링 및 보상 트랜잭션  특정 이벤트 처리 실패에 대해 비즈니스 실패 케이스를 처리하기 위한 보상 트랜잭션 설계가 필요합니다.Kafka 모니터링 및 운영  Kafka의 토픽, 메시지 적재량, Lag 등을 모니터링하여 적절한 대응을 할 수 있도록 해야 합니다.  Kafka가 SPOF(Single Point Of Failure)이 되지 않도록 멀티 클러스터로 운영할 필요가 있습니다.5. 추가로 공부하면 좋을 내용  Kafka의 At-Least-Once/Exactly-Once 보장 방식  Kafka 멀티 클러스터 구성  Saga 패턴과 보상 트랜잭션6. 💡 정리  REST API 기반 동기식 통신은 초기에는 빠르게 개발 가능하지만, 확장성과 유연성 측면에서 한계가 존재합니다.  Kafka를 활용한 이벤트 드리븐 아키텍처는 서비스 간 결합도를 낮추고, 장애 격리 및 확장성 측면에서 뛰어난 이점을 제공합니다.  하지만, 이벤트 기반 시스템을 설계할 때는 중복 처리, 데이터 정합성, 에러 핸들링과 같은 추가적인 고려 사항을 반드시 신경써야 합니다.  궁극적으로 서비스 규모가 커질수록 Kafka 기반의 이벤트 드리븐 아키텍처는 성능과 유연성 측면에서 REST API 방식보다 장점이 많다고 볼 수 있습니다.]]></content>
      <categories>
        
          <category> Spring Cloud </category>
        
          <category> MSA </category>
        
          <category> Kafka </category>
        
      </categories>
      <tags>
        
          <tag> REST API </tag>
        
          <tag> Event-driven </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Redisson을 활용한 분산 락 그리고 AOP 적용]]></title>
      <url>/msa/redis/2025/02/22/Redisson-Lock-and-AOP/</url>
      <content type="text"><![CDATA[  분산 시스템에서 Redis를 이용한 분산 락의 필요성과 분산 락 적용 부분의 AOP 활용 방법을 알아봅시다.📌 들어가기MSA와 같은 분산 시스템에서는 여러 인스턴스가 동시에 하나의 리소스에 접근하는 경우가 빈번하게 발생할 수 있습니다. 예를 들어, 주문이 동시다발적으로 발생하여 상품의 재고를 차감하기 위해 여러 인스턴스가 재고에 접근하는 경우가 그렇습니다.이때 동시성 이슈가 발생하여 데이터 무결성에 문제가 발생합니다. 이를 방지하기 위해 분산 락(Distributed Lock)이 필요한 것입니다.본 포스트에서는 Redis기반의 분산 락을 구현하기 위해 Redisson라이브러리를 활용하는 방법과, 스프링 AOP(Aspect Oriented Programming)를 통해 락 처리 로직을 분리하여 코드의 응집도와 유지보수성을 높이는 방법을 살펴보도록 하겠습니다.Redis 라이브러리: Redisson을 활용한 분산 락Redis는 인메모리 데이터 저장소로 높은 성능과 빠른 응답 속도를 제공합니다. 이를 활용하여 여러 프로세스 혹은 서버 간에 분산 락을 구현할 수 있으며, 분산 환경에서의 동시성 제어와 데이터 무결성 확보에 큰 역할을 하게 됩니다.Redisson은 Redis를 Java 애플리케이션에서 쉽게 사용할 수 있도록 지원하는 Redis 클라이언트 라이브러리입니다. Redisson은 분산 락, 세마포어 등 여러 동시성 관련 기능을 제공하며, 복잡한 락 로직을 단순화시켜 줍니다.Lettuce vs RedissonSpring Data Redis 의존성을 추가하면 기본적으로 Lettuce 라이브러리를 Redis 클라이언트로 사용하게 됩니다. 기본적으로 제공하는 Lettuce가 있음에도 분산 락에 Redisson을 사용하는 이유는 무엇일까요?분산 락에 Lettuce 대신 Redisson을 사용하는 이유는 락 획득 방식 차이점에 있습니다.Lettuce는 스레드가 락 획득 대기 상태일 경우, spin lock 방식으로 대기하게 됩니다. 많은 스레드가 스핀 락으로 Redis에 락을 요청하게 될 경우 Redis에 큰 부하가 생길 수 있습니다. 반면에 Redisson은 락 획득 방식이 pub/sub 방식으로 구현되어 있기 때문에 스핀 락 방식보다는 Redis에 부하 부담이 줄어들게 됩니다. 더불어 Redisson은 락 획득 재시도를 기본 로직으로 제공해주기 때문에 편리하게 Lock을 사용할 수 있게 해줍니다.Redisson Lock 예제 코드아래는 사이드 프로젝트에서 Redisson을 활용하여 분산 락을 구현한 코드입니다. 동시성 이슈가 발생할 수 있는 작업을 수행하기 전에 분산 락을 획득하고, 작업을 마쳤다면 락을 반납하면 됩니다. 이로써 여러 스레드가 동시에 접근하더라도 동시성 문제가 발생하지 않게 됩니다.@Service@Slf4j@RequiredArgsConstructorpublic class RedissonLockService {    private final RedissonClient redissonClient;    public boolean tryLock(String key, long waitTime, long leaseTime) {        RLock lock = redissonClient.getLock(key);        try {            return lock.tryLock(waitTime, leaseTime, TimeUnit.SECONDS);            // waitTime: 락을 기다리는 시간            // leaseTime: 락이 자동으로 해제되기까지 유지되는 시간        } catch (InterruptedException e) {            log.error("Failed to acquire lock", e);            return false;        }    }    public void unlock(String key) {        RLock lock = redissonClient.getLock(key);        if (lock.isHeldByCurrentThread()) {            lock.unlock();        }    }}  tryLock 메서드를 통해 락 획득 시도를 하게 됩니다. 획득에 성공한다면 true를 반환하고, waitTime동안 락을 획득하지 못한다면 예외가 발생하고 false를 반환하게 됩니다.  tryLock 메서드는 블락킹(blocking) 메서드로, 락을 획득하지 못 한 경우 스레드가 대기 상태에 놓이게 됩니다.  락 획득 key는 동시성 이슈를 예방하고자 하는 대상의 고유한 식별자가 되어야 합니다. ex: productId  waitTime동안 락 획득 대기 상태에 놓이게 되며, 락을 사용하던 스레드가 락을 해제하면 pub/sub 방식을 통해 대기 상태에 놓여있던 스레드에게 락 획득 시도를 하라고 알림을 보내게 됩니다.  만약 락을 획득하고 작업 시간이 leaseTime을 넘어가게 되면 자동으로 락이 해제됩니다. 따라서 leaseTime 이내로 작업이 끝내는 것을 보장해야 동시성 이슈가 발생하지 않습니다.  작업을 마친 스레드는 unlock을 통해 락을 해제하게 됩니다.분산 락 활용 예제 코드 (재고 차감)상품의 재고 수량을 변경시키는 작업은 동시성 이슈가 발생하는 대표적인 예시로 볼 수 있습니다. 여러 스레드가 상품 재고의 수량을 변경하기 위해 동시에 접근할 수 있기 때문입니다.앞서 살펴본 Redisson을 활용한 분산 락을 재고 수량을 변경하는 로직 전후로 락을 획득/해제를 한다면 동시성 이슈를 예방할 수 있을 겁니다.@Transactional@Overridepublic Product decreaseStock(String productId, Integer quantity) {    long leaseTime = 5;    long waitTime = 10;    String lockKey = "lock:product:stock:" + productId;    try {      if (redissonLockService.tryLock(lockKey, waitTime, leaseTime)) { // 락 획득 시도        Optional&lt;Product&gt; findProduct = productRepository.findByProductId(productId);        if (findProduct.isEmpty()) {            throw new IllegalArgumentException("Product not found");        }        Product product = findProduct.get();        product.decreaseStock(quantity); // 재고 차감        return product;      }    } finally {      redissonLockService.unlock(lockKey); // 락 해제     }}  상품 고유 식별자(productId)를 사용하여 상품 별로 분산 락을 제어할 수 있도록 했습니다.  락을 획득한 경우에만 재고 수량을 차감시킬 수 있게 됩니다.  락 획득에 실패하게 되면 예외가 발생하여, 재고 수량 차감 로직은 수행하지 못 하게 됩니다.  락 획득 후 작업을 완료했다면 락을 해제해줍니다.분산 락 적용 부분의 AOP 분리애플리케이션에서 분산 락과 같은 횡단 관심사(cross-cutting concern)는 여러 서비스 메서드에서 반복적으로 구현되기 때문에, 이를 개별 비즈니스 로직과 분리하면 코드의 가독성 및 유지보수성이 크게 향상됩니다. 스프링 AOP를 활용하면 메서드 호출 전후에 자동으로 락을 획득하고 해제하는 로직을 삽입할 수 있습니다.  아래 내용은 AOP에 대한 이해가 필요합니다!AOP 적용 방법스프링 AOP를 적용하는 방법에 대해 단계별로 알아보겠습니다.어노테이션 정의락이 필요한 메서드에 적용할 커스텀 어노테이션(@StockLock)을 정의합니다.@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface StockLock {    long waitTime() default 5;    long leaseTime() default 10;}Aspect 클래스 작성@Around 어드바이스를 활용하여 어노테이션이 붙은 메서드의 실행 전후에 분산 락 획득 및 해제 로직을 삽입합니다.@Aspect@Slf4j@Component@Order(value = Integer.MAX_VALUE - 1) // AOP 우선순위를 지정합니다.@RequiredArgsConstructorpublic class StockAspect {    private final RedissonLockService redissonLockService;    @Around("@annotation(stockLock) &amp;&amp; args(productId,..)")    public Object doLock(ProceedingJoinPoint joinPoint, StockLock stockLock, String productId) throws Throwable {        log.info("StockLockAspect.doLock {}", joinPoint.getSignature());        long leaseTime = stockLock.leaseTime();        long waitTime = stockLock.waitTime();        String lockKey = "lock:product:stock:" + productId;        if (redissonLockService.tryLock(lockKey, waitTime, leaseTime)) {            try {                return joinPoint.proceed();            } finally {                redissonLockService.unlock(lockKey);            }        } else {            log.error("락을 획득하지 못하여 종료합니다. productId={}", productId);            return false;        }    }}  🚨 AOP 우선 순위 지정  스프링의 트랜잭션 처리도 AOP를 활용합니다. 이때 분산 락의 AOP와 트랜잭션의 AOP의 순서가 굉장히 중요합니다. 만약 트랜잭션 AOP가 먼저 실행된다면, 여전히 동시성 이슈가 발생할 가능성이 존재하게 됩니다. 따라서 분산 락 AOP를 먼저 적용하기 위해 @Order를 통해 순서를 지정해줍니다. 예제 코드에서는 Integer.MAX_VALUE - 1 값을 지정해주었는데, 이는 트랜잭션의 우선순위는 기본적으로 제일 후순위(INTEGER.MAX_VALUE)이기 때문입니다.비즈니스 로직과 분리실제 서비스 메서드는 락 관련 코드를 포함하지 않고, AOP가 이를 대신 처리하게 됩니다.    @Transactional    @Override    @StockLock    public Product decreaseStock(String productId, Integer quantity) {        log.info("product-service: 재고 차감");        Optional&lt;Product&gt; findProduct = productRepository.findByProductId(productId);        if (findProduct.isEmpty()) {            throw new IllegalArgumentException("Product not found");        }        Product product = findProduct.get();        product.decreaseStock(quantity);        return product;    }AOP 적용의 장점락 획득 및 해제 로직을 한 곳에서 관리하여 여러 서비스 메서드에 중복된 코드를 제거할 수 있습니다. 만약 락 처리 로직을 변경할 경우, Aspect 클래스만 수정하면 되므로 관리가 용이합니다. 그리고 서비스 메서드에서는 비즈니스 로직에만 집중할 수 있어 코드의 가독성이 좋아집니다.결론분산 시스템에서 동시성 문제를 해결하기 위한 분산 락은 필수적인 요소입니다. Redis의 Redisson을 활용하면 높은 성능과 편리한 분산 락 구현이 가능합니다. 또한 AOP를 적용함으로써 락 관련 로직을 분리하면 코드의 유지보수성과 가독성이 크게 향상됩니다. 본 포스트를 참고하여 분산 락을 실제 구현하는 데 도움이 되면 좋겠습니다. 감사합니다.참고 자료  인프런 - 재고시스템으로 알아보는 동시성이슈 해결방법  카카오페이는 어떻게 수천만 결제를 처리할까? 우아한 결제 분산락 노하우 / if(kakaoAI)2024  풀필먼트 입고 서비스팀에서 분산락을 사용하는 방법 - 마켓 컬리]]></content>
      <categories>
        
          <category> MSA </category>
        
          <category> Redis </category>
        
      </categories>
      <tags>
        
          <tag> AOP </tag>
        
          <tag> Distributed Lock </tag>
        
          <tag> Redisson </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[MSA에서 데이터 일관성 유지를 위한 Saga 패턴]]></title>
      <url>/spring%20cloud/msa/2025/02/22/MSA-SAGA-Pattern/</url>
      <content type="text"><![CDATA[  Saga 패턴이 무엇이고, MSA 환겨에서 왜 필요한지 알아봅시다!📌 들어가기마이크로서비스 아키텍처에서는 각 서비스가 독립적인 데이터베이스를 가지고 있으며, 서비스 간 통신을 통해 비즈니스 로직이 연결되는 경우가 많습니다.이때, 데이터 일관성을 유지하는 것이 중요한데, 전통적인 분산 트랜잭션 방식으로는 한계가 있습니다.이번 포스트에서는 Saga 패턴의 필요성과 이를 도입하지 않을 경우 발생할 수 있는 문제점에 대해 살펴보도록 하겠습니다.1. 전통적인 분산 트랜잭션MSA 환경에서 각 서비스는 독립적인 데이터베이스를 가지고 있기 때문에 데이터 정합성을 유지하기가 훨씬 어려워집니다.전통적인 분산 트랜잭션은 데이터 정합성 불일치 문제를 해결하기 위한 초기 접근 방식으로, 여러 데이터베이스에 걸쳐 원자성(Atomicity)을 보장하려는 방식이었습니다.그러나 이 방식은 한계점과 문제점을 동반하게 되며, 결과적으로 Saga 패턴과 같은 대안이 등장하게 되었습니다.전통적인 분산 트랜잭션 핵심: 2-Phase Commit(2PC)MSA 다중 데이터베이스 환경에서 하나의 비즈니스 로직이 수행될 때, 여러 데이터베이스에 걸쳐 관련 데이터들이 저장됩니다. 전통적인 분산 트랜잭션에서 가장 널리 사용된 프로토콜로 2-Phase Commit(2PC)가 있습니다. 이는 트랜잭션 관리자(Transaction Coordinator)가 존재하여 전체 트랜잭션을 조율하는 방식입니다.전통적인 분산 트랜잭션 한계점전통적인 분산 트랜잭션 한계점은 다음과 같습니다.  락킹(Locking)으로 인한 성능 저하, 병목 지점이 될 수 있습니다.  트랜잭션은 관리하는 코디네이터가 단일 실패 지점이 될 수 있습니다.  네트워크 문제로 인해 트랜잭션 관리자의 지시를 받지 못하면, 교착 상태(Deadlock)에 빠질 수도 있습니다.  관리하는 노드가 많아질수록 트랜잭션을 조율하기 복잡해집니다. 이로 인해 시스템의 확장성이 저하됩니다.💡 위와 같은 한계점들로 인해 전통적인 2PC 방식은 단일 시스템이나 적은 수의 서비스에 적합하지만, 확장성, 고가용성, 비동기성이 중요한 마이크로서비스 환경에는 적합하지 않습니다.❗️ 참고로, 이번 포스트의 주제는 전통적인 분산 트랜잭션의 한계점으로 인한 Saga 패턴 도입의 필요성을 다루는 포스트입니다. 따라서 전통적인 분산 트랜잭션에 대한 내용은 추후 다른 포스트에서 자세히 다루도록 하겠습니다.2. 전통적인 분산 트랜잭션의 대안전통적인 분산 트랜잭션의 대안으로 Saga 패턴과 Outbox 패턴이 있습니다.두 패턴 중 이번 포스트에서는 Spring Cloud 사이드 프로젝트를 진행하면서 Saga 패턴을 적용한 내용을 다루겠습니다.3. Saga 패턴이란?Saga 패턴은 마이크로서비스 간 분산 트랜잭션을 관리하기 위한 패턴입니다.각 서비스의 로컬 트랜잭셩을 통해서 전체 비즈니스 로직의 일관성을 유지하는 방법이죠.Saga 패턴의 핵심 아이디어는 하나의 큰 트랜잭션을 여러 개의 트랜잭션으로 나누고, 문제가 발생하면 보상 트랜잭션(Compensating Transaction)을 수행하여 일관성을 맞추는 것입니다.3-1. Saga 패턴 예시Saga 패턴이 무엇인지 이해하기 쉽도록 예시를 들어보겠습니다.예를 들어, Order Service -&gt; Payment Service -&gt; Product Service 의 순서로 주문과 결제 그리고 재고 차감이 이뤄지는 비즈니스 로직이 있다고 하겠습니다.  Order Service: 주문 생성 -&gt; ORDER_CREATED 이벤트 발행.  Payment Service: 결제 요청 -&gt; 결제 성공 시 PAYMENT_COMPLETED 이벤트 발행.  Product Service: 재고 차감 -&gt; 만약 재고 부족 시, 실패 이벤트 발행.🎯 만약 Product Service에서 재고 부족으로 인해 실패 이벤트가 발생하면, 보상 트랜잭션으로 주문 취소 및 결제 취소를 진행해주게 됩니다. 이처럼 실패에 대한 보상 처리를 해주는 것이 Saga 패턴입니다.4. Saga 패턴의 종류Saga 패턴에는 종류가 존재합니다. 크게 Choreography(코레오그래피) 방식과 Orchestration(오케스트레이션) 방식으로 나뉘게 됩니다.각 방식에는 장단점이 존재하며, 프로젝트 환경에 따라 적절한 패턴을 적용하면 될 것 같습니다.그럼, 각 패턴 종류의 장단점을 살펴보겠습니다.4-1. Saga pattern - Choreography(코레오그래피)이벤트 기반으로 각 서비스가 직접 이벤트를 구독하고 다음 작업을 수행하는 패턴입니다. 이는 중앙 조정자가 없다는 것이 특징입니다.  장점: 단순한 구현, 낮은 복잡성  단점: 서비스 간 의존성 증가, 복잡한 이벤트 플로우 관리 어려움👉 예시  Order Service -&gt; ORDER_CREATED 주문 생성 이벤트 발행  Payment Service 가 주문 생성 구독 후 결제 처리 -&gt; PAYMENT_COMPLETED 결제 성공 이벤트 발행  Delivery Service가 결제 성공 구독 후 배송 요청 처리.4-2. Saga Pattern - Orchestration(오케스트레이션)중앙 조정자(Service Orchestration)가 각 서비스에 요청을 보내고 전체 트랜잭션을 관리 및 조율하는 패턴입니다.  장점: 서비스 간 의존성 감소, 중앙 집중식으로 전체 플로우 관리 가능.  단점: Orchestrator(중앙 조정자)를 추가 구현해야 하며, 단일 실패 지점(SPOF)이 될 수 있음.👉 예시  Orchestrator가 Order Service 에 주문 생성 요청  주문 생성 -&gt; 결제 요청 -&gt; 재고 차감 -&gt; 배송 요청 순으로 전체 이벤트 발행 및 트랜잭션 관리  실패 시 보상 트랜잭션을 직접 호출✅ 4-3 Saga Pattern 종류 정리Saga 패턴에는 Choreography와 Orchestration 두 방식이 존재합니다.두 방식은 각각의 장단점이 상반되며, 프로젝트 환경에 알맞게 적절한 패턴 종류를 선택하면 됩니다.참고로 저는 사이드 프로젝트를 진행할 때 단순한 구현과 낮은 복잡성으로 구성되도록 원했고, 중앙 조정자 도입이 오히려 서비스가 많아질 수록 중앙 조정자를 관리하기 어려워 질 것이란 판단하에 Choreography(코레오그래피) 방식을 적용하였습니다.5. Saga 패턴 적용 예제 코드사이드 프로젝트에서 Saga 패턴(Choreography)을 적용한 예제 코드를 보여드리겠습니다.참고로 실제 코드 전부가 아닌 Saga 패턴 이해를 위한 간소화된 코드로 보여드리겠습니다.Order Service - ORDER_CREATED 이벤트 발행@Transactional@Overridepublic OrderDto createOrder(OrderDto orderDto) {    log.info("Order-service: 주문 생성");        // 주문 생성 로직 ...    // ORDER_CREATED 이벤트 발행    OrderCreatedEvent orderCreatedEvent = new OrderCreatedEvent(            order.getOrderId(),            BigDecimal.valueOf(totalPrice),            orderDto.getPaymentInfos(),            orderDto.getDeliveryInfo());    orderEventProducer.send(ORDER_CREATED, orderCreatedEvent);}Payment Service - ORDER_CREATED 이벤트 구독 후 결제 처리@KafkaListener(topics = "ORDER_CREATED", groupId = "${spring.kafka.consumer.group-id:payment-service-group}")public void consume(ConsumerRecord&lt;String, String&gt; record) throws JsonProcessingException {    try {        String message = record.value();        log.info("Consumed message: {}", message);        OrderCreatedEvent orderCreatedEvent = objectMapper.readValue(message, OrderCreatedEvent.class);        log.info("Order created event: {}", orderCreatedEvent);        paymentHandler.handle(orderCreatedEvent); // 결제 처리     } catch (Exception e) {        log.error("Error Consume OrderCreatedEvent", e);    }}만약, 결제 처리 실패 시 보상 트랜잭션을 수행합니다.PAYMENT_FAILED 이벤트 발행 -&gt; Order Service에서 결제 실패를 구독하여 주문 취소를 진행합니다.public void handle(Event event) {    try {        handleEvent(event);    } catch (Exception e) {        handleException(event, e);    }}private void handleException(Event event, Exception e) {    log.error("Error handling payment", e);    if (event instanceof OrderCreatedEvent orderCreatedEvent) {        paymentEventProducer.send(PAYMENT_FAILED, new PaymentFailedEvent(                orderCreatedEvent.getOrderId(),                null,                FAILED, e.getMessage()));    }    ...}6. Saga 패턴 도입 시 장점분산 트랜잭션 처리를 위해 Saga 패턴을 도입하면 다음과 같은 장점이 있습니다.6-1. 데이터 정합성 보장서비스 간 비동기 처리에도 Eventually Consistent(최종 일관성) 상태를 유지할 수 있습니다. 예를 들어, 주문 생성 후 결제를 실패한다면, 주문을 취소하여 데이터 불일치를 방지할 수 있습니다.이처럼, 보상 트랜잭션을 통해 데이터 일관성이 유지됩니다.6-2. 마이크로서비스 독립성 유지각 서비스는 로컬 트랜잭션만 관리하면 되므로, 강한 결합(Tightly Coupled)을 피할 수 있습니다.6-3. 확장성 향상서비스 간의 직접적인 의존성이 줄어들어, 새로운 서비스 추가 및 확장이 쉬워지게 됩니다.7. Saga 패턴을 도입하지 않으면 발생하는 문제점7-1. 데이터 불일치실패 시 적절한 보상 트랜잭션이 없다면, 결제 실패나 재고 부족 등의 시나리오에서, 주문 상태는 “완료”이지만 실제로는 결제가 되지 않은 주문이 발생할 수 있습니다.7-2. 사용자 경험 저하주문은 완료 상태이고 결제는 실패했을 때, 제대로된 보상 트랜잭션 처리를 하지 않는다면 “주문 정상 처리”라는 잘못된 정보를 사용자에게 전달할 수도 있습니다.🎯 결론  Saga 패턴은 마이크로서비스 환경에서 데이터 정합성을 유지하기 위한 필수 설계 패턴으로 볼 수 있습니다.  보상 트랜잭션을 통해 실패 시에도 전체 시스템의 일관성을 유지할 수 있으며, 서비스 간 결합도를 낮춰 확장성과 유연성을 확보할 수 있도록 도와줍니다.Kafka와 같은 이벤트 드리븐 기반의 MSA 환경에서 Saga 패턴을 도입하지 않으면, 데이터 불일치 및 비즈니스 오류가 발생할 가능성이 높습니다. 안정적인 시스템을 만들기 위해서는 반드시 Saga 패턴과 같은 분산 트랜잭션 관리 기법이 필요합니다.💡 추가로 공부하면 좋을 내용  CAP 이론  2-Phase Commit  Outbox 패턴]]></content>
      <categories>
        
          <category> Spring Cloud </category>
        
          <category> MSA </category>
        
      </categories>
      <tags>
        
          <tag> Saga pattern </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Cloud Gateway와 JWT - 분산 시스템의 중앙집중식 인증]]></title>
      <url>/spring%20cloud/msa/2025/02/19/Spring-Cloud-Gateway-Security/</url>
      <content type="text"><![CDATA[  Spring Cloud Gateway와 JWT를 활용한 중앙 집중식 인증에 대해 알아봅시다!들어가기분산 시스템에서 JWT와 같은 토큰 기반 인증을 사용하지 않는다면 어떻게 될까요?이번 포스트에서는 분산 시스템에서 Spring Cloud Gateway와 JWT 토큰 기반 인증을 사용의 이점에 대해 알아보는 시간을 갖도록 하겠습니다.분산 시스템(No Token &amp; No Gateway)우선, 분산 환경에서 API Gateway와 토큰 기반 인증 기술을 결합한 중앙집중식 인증 시스템을 도입하지 않는 경우 발생하는 문제점들에 대해 먼저 살펴보도록 하겠습니다.      중앙집중식 인증 시스템이 없다면, 각 서비스마다 별도의 인증/인가 로직을 구현해야 합니다. 이는 코드의 중복과 유지보수의 어려움이 발생할 수 있습니다.        토큰 대신 사용하는 세션 기반 인증 방식은 서버에 상태 정보를 저장해야 하기 때문에, 분산 환경에서의 서버 간 세션 동기화 문제가 발생할 수 있습니다. 이는 시스템 확장성에 어려움을 증가시킵니다.  분산 시스템(Gateway + Token 기반)그렇다면 Spring Cloud Gateway와 JWT 결합의 중앙 집중식 인증 시스템은 어떠한 장점이 있는지 살펴보겠습니다.      모든 요청에 대한 검사를 API Gateway에서 한 번에 수행할 수 있어, 개별 서비스마다 중복된 인증 로직을 구현할 필요가 없습니다. 이를 통해 보안 설정의 일관성이 확보되고, 유지보수가 용이해집니다.        JWT는 토큰 기반 인증 방식으로, 별도의 세션 관리를 하지 않고도 사용자 인증 정보를 안전하게 전달 할 수 있습니다. 이는 분산 시스템 환경에서 효율적인 리소스 사용과 확장성을 보장합니다.  예제 코드이번에는 사이드 프로젝트에서 직접 Spring Cloud Gateway와 JWT를 결합한 인증 필터를 살펴보겠습니다.다음 코드는 Spring Cloud Gateway 에서 사용자 요청의 담겨 있는 Authorization 헤더의 JWT 토큰이 유효한지 검증하는 코드입니다.@Component@Slf4jpublic class AuthorizationHeaderFilter extends AbstractGatewayFilterFactory&lt;AuthorizationHeaderFilter.Config&gt; {    public static class Config {    }    private Environment env;    public AuthorizationHeaderFilter(Environment env) {        super(Config.class);        this.env = env;    }    @Override    public GatewayFilter apply(Config config) {        return (exchange, chain) -&gt; {            ServerHttpRequest request = exchange.getRequest();            if (!request.getHeaders().containsKey(HttpHeaders.AUTHORIZATION)) {                return onError(exchange, "Authorization header is missing", HttpStatus.UNAUTHORIZED);            }            String authorizationHeader = request.getHeaders().get(HttpHeaders.AUTHORIZATION).get(0);            String jwt = authorizationHeader.replace("Bearer", "").trim();            if (isNotValidJwt(jwt)) {                return onError(exchange, "JWT is not valid", HttpStatus.UNAUTHORIZED);            }            return chain.filter(exchange);        };    }    // WebFlux method to handle errors    private Mono&lt;Void&gt; onError(ServerWebExchange exchange, String err, HttpStatus httpStatus) {        exchange.getResponse().setStatusCode(httpStatus);        log.error(err);        return exchange.getResponse().setComplete();    }    private boolean isNotValidJwt(String jwt) {        boolean returnValue = false;        String secretKey = env.getProperty("token.secret");        byte[] keyBytes = Base64.getDecoder().decode(secretKey);        SecretKey key = Keys.hmacShaKeyFor(keyBytes);        String subject = Jwts.parser()                .setSigningKey(key)                .build()                .parseClaimsJws(jwt)                .getBody()                .getSubject();        if (subject == null || subject.isEmpty()) {            returnValue = true;        }        return returnValue;    }}  위 코드처럼 Gateway에서 모든 요청에 대한 인증을 한 번에 처리함으로써 보안 관리가 용이해집니다.  그리고 각 서비스에서 별도의 세션 관리를 하지 않아도 됩니다.  인증/인가 로직을 게이트웨이에서 처리하므로 각 백엔드 서비스의 중복된 코드를 줄일 수 있습니다.이처럼 Spring Cloud Gateway와 JWT 인증의 결합은 복잡한 분산 환경에서 안정적이고 효율적인 인증/인가를 가능하게 해줍니다.추가로 고려해볼 만한 내용  JWT 사용 시 발생할 수 있는 보안 이슈와 이에 대한 대응 방법요약Spring Cloud Gateway와 JWT 결합 사용의 이유 및 기대 효과는 다음과 같습니다.  중앙 집중식 보안 관리로 유지보수가 용이  분산 환경에 적합한 stateless 인증 방식  확장성과 유연성 확보  효율적인 리소스 사용]]></content>
      <categories>
        
          <category> Spring Cloud </category>
        
          <category> MSA </category>
        
      </categories>
      <tags>
        
          <tag> JWT </tag>
        
          <tag> Spring Cloud Gateway </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Cloud Gateway를 활용한 API Gateway 구축]]></title>
      <url>/spring%20cloud/msa/2025/02/18/Spring-Cloud-Gateway/</url>
      <content type="text"><![CDATA[  Spring Cloud Gateway의 역할, 라우팅 및 필터 처리 방법에 대해 알아봅시다!📌 들어가기Gateway란 무엇인가?Gateway는 클라이언트와 내부 시스템(ex: 마이크로서비스) 간의 단일 진입점 역할을 하는 미들 웨어입니다.모든 외부 요청은 먼저 Gateway를 통해 들어오며, 이곳에서 인증, 인가, 로깅, 모니터링 등 공통 기능을 처리한 후 적절한 내부 서비스로 요청을 전달합니다.이를 통해 개별 서비스는 본연의 비즈니스 로직에 집중할 수 있게 됩니다.왜 Spring Cloud Gateway를 사용해야 할까?      Spring Cloud Gateway는 Spring Boot, Spring Security 등과 자연스럽게 연동되므로, 기존 Spring 기반 시스템에 쉽게 도입할 수 있습니다.        Netty 기반의 Spring Cloud Gateway는 비동기, 논블로킹 방식으로 동작하여 높은 동시성 처리가 가능하며, 성능과 확장성 면에서 우수합니다.        application.yml 설정 파일에 선언적 설정을 통해 쉽게 라우팅 규칙을 정의하고, 전/후 처리 필터를 적용할 수 있습니다. 이는 다양한 요구사항에 맞춰 Gateway를 유연하게 구성할 수 있게 해줍니다.  이와 같이 Spring Cloud Gateway는 Spring 생태계와의 원활한 통합을 지원하며, 마이크로서비스 환경에서 발생할 수 있는 다양한 문제(코드 중복, 보안 취약점, 성능 및 확장성 문제 등)를 효과적으로 해결해줍니다.기본 라우팅 설정application.yml 파일에 선언적 라우팅 설정을 통해, 요청 URL에 따라 내부 서비스로 요청을 전달합니다.spring:  application:    name: gateway-service  cloud:    gateway:      routes:        - id: user-service          uri: http://localhost:8080 # lb://USER-SERVICE          predicates:            - Path=/user-service/test/**            - Method=GET  위 게이트웨이 라우팅 설정은 HTTP 메서드가 GET 요청이고, 경로가 /user-service/test/**에 매칭되는 경우 http://localhost:8080 에서 실행중인 user-service 로 라우팅해줍니다.  만약 디스커버리 서비스인 Eureka를 사용한다면, 라우팅 URI를 lb://USER-SERVICE 처럼 서비스 이름으로 작성할 수 있습니다. (lb://{application.name})전역 필터spring:  application:    name: gateway-service  cloud:    gateway:      default-filters:        - name: MyGlobalFilter          args:            preFilter: true            postFilter: true  spring.cloud.gateway.default-filters: 전역 필터 클래스를 지정해줄 수 있습니다. 이때 필터 객체에 인수(args)를 전달해줄 수 있습니다.          name: 전역 필터 클래스명      args: 명시된 인수값들이 전역 필터 내부의 중첩 클래스인 Config와 매핑된 후, 필터 기능을 하는 apply 메서드의 인자로 전달됩니다. (코드를 보면 쉽게 이해하실 수 있을 겁니다.)        💡 참고로 필터에 인자 값을 전달하려면 필터에 name 속성값을 적용해야 합니다. 그 다음 아랫줄에 args 를 작성하면 됩니다.MyGlobalFilterSpring Cloud Gateway에서 동작하는 필터 클래스는 만들기 위해서는 AbstractGatewayFilterFactory를 상속하고 apply 메서드를 오버라이딩 해줘야 합니다.그리고, args를 전달 받을 중첩 클래스(MyGlobalFilter.Config)를 부모 클래스의 제네릭으로 전달 해줍니다.@Slf4j@Componentpublic class MyGlobalFilter extends AbstractGatewayFilterFactory&lt;MyGlobalFilter.Config&gt; {    public MyGlobalFilter() {        super(Config.class);    }    @Override    public GatewayFilter apply(Config config) {        return (exchange, chain) -&gt; {            // Pre Filter            if (config.isPreFilter()) log.info("Global Pre Filter executed");            // Post Filter            return chain.filter(exchange).then(Mono.fromRunnable(() -&gt; {                if (config.isPostFilter()) log.info("Global Post Filter executed");            }));        };    }    @Data    public static class Config {        private boolean preFilter;        private boolean postFilter;    }}  MyGlobalFilter 클래스 내부의 중첩 클래스인 Config 객체에 application.yml 에서 설정한 args 를 전달받게 됩니다.  apply 메서드 내부에서 return 전 영역은 PreFilter 이고, return 구문은 PostFilter 영역입니다.  요청을 라우팅 하기 전에 PreFilter가 동작하고, 요청된 라우팅이 마이크로서비스에서 처리를 마친 후 응답이 돌아오면 그제서야 PostFilter가 동작합니다.Custom Filterspring:  application:    name: gateway-service  cloud:    gateway:      default-filters:        - name: MyGlobalFilter          args:            preFilter: true            postFilter: true      routes:        - id: user-service          uri: http://localhost:8080          predicates:            - Path=/user-service/**          filters:            - RemoveRequestHeader=Cookie            - RewritePath=/user-service/(?&lt;segment&gt;.*), /$\{segment}            - `MyCustomFilter`            - name: MyLoggingFilter              args:                baseMessage: MyLoggingFilter start                preLogger: true                postLogger: true  spring.cloud.gateway.routes.filters: 해당 속성에 선언된 필터들은 라우팅 조건을 만족하는 요청에 대해서만 필터가 작동합니다. 즉, 다른 라우팅 조건을 만족하는 요청에 대해선 필터가 동작하지 않습니다.          예를 들어, 위 설정에서는 uri가 http://localhost:8080 이고, path(경로)가 /user-service/** 에 매칭되는 요청에 대해서만 filters 들이 적용됩니다.        RemoveRequestHeader, RewritePath와 같이 기본적으로 스프링에서 지원하는 선언적 설정도 존재하고, MyCustomFilter, MyLoggingFilter와 같이 개발자가 정의한 커스텀 필터를 지정할 수도 있습니다.  ✅ 설정 파일(application.yml) 내에서 나열된 라우트(route)나 각 라우트에 정의된 필터(filters)는 선언한 순서(위에서 아래)대로 처리되는 것이 일반적입니다. 즉, 별도의 order 속성을 지정하지 않는 경우에는 위에서 아래로 우선순위가 적용됩니다.Discovery Service와 Spring Cloud GatewaySpring Cloud Netflix Eureka를 사용 환경이라면, Spring Cloud Gateway는 로드밸런서의 역할도 수행할 수 있습니다.앞서 살펴봤던 user-service 의 Uri 경로를 lb://USER-SERVICE 처럼 애플리케이션 네임을 적어주면 디스커버리 서비스로부터 해당 서비스의 위치를 탐색하여 로드밸런싱 할 수 있습니다.spring:  application:    name: gateway-service  cloud:    gateway:      routes:        - id: user-service          uri: lb://USER-SERVICE          predicates:            - Path=/user-service/test/**            - Method=GET이때, 만약 user-service 의 인스턴스가 다중화 되어 있다면, Spring Cloud Gateway는 라운드 로빈(Round Robin) 알고리즘으로 로드밸런싱을 처리하게 됩니다. 이와 같이 Spring Cloud Gateway는 단순한 API 게이트웨이 기능을 넘어, 서비스 디스커버리와 통합되어 여러 인스턴스 간의 로드밸런싱 기능을 제공합니다.따라서 시스템의 부하를 효과적으로 분산시켜, 서비스의 가용성과 안정성을 높여주고 마이크로서비스 아키텍처에서의 운영 효율성을 크게 향상시켜 줍니다.🎯 요약  클라이언트와 내부 시스템 간의 단일 진입점 역할을 수행하며, 인증, 인가, 로깅, 모니터링 등 공통 기능을 중앙집중적으로 처리.  Spring 생태계와의 원활한 통합, Netty 기반의 비동기 논블로킹 처리, 선언적 설정을 통한 유연한 라우팅 및 필터 관리 등으로 높은 성능과 확장성을 제공.  Spring Cloud Netflix Eureka와 연동하여, lb:// 접두사를 사용함으로써 동적 로드밸런싱(라운드 로빈 알고리즘)을 수행, 여러 인스턴스 간의 부하 분산과 시스템 안정성을 확보📂 참고 자료  Spring Cloud Gateway Docs  Netty]]></content>
      <categories>
        
          <category> Spring Cloud </category>
        
          <category> MSA </category>
        
      </categories>
      <tags>
        
          <tag> Spring Cloud Gateway </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Cloud Config를 이용한 환경 변수 관리]]></title>
      <url>/spring%20cloud/msa/2025/02/17/Spring-Cloud-Config-Server/</url>
      <content type="text"><![CDATA[  Spring Cloud Config로 분산 환경에서 환경 변수 설정을 관리하는 방법에 대해 알아봅시다!Spring Cloud Config란?Spring Cloud Config는 마이크로서비스 아키텍처 환경에서 각 서비스의 설정을 중앙 집중식으로 관리할 수 있게 도와주는 도구입니다.설정 정보를 별도의 Config Server에서 관리하고, 각 서비스는 Config Server로부터 설정을 전달받아 애플리케이션 구성을 하게 됩니다.왜 Spring Config Server가 필요한가?일관성MSA 환경에서는 각 서비스가 개별적으로 설정을 관리할 경우 설정의 중복 및 불일치 문제가 발생할 수 있습니다.Config Server를 통해 중앙 집중식으로 설정을 관리하게 되면 설정 정보의 일관성을 유지할 수 있게 됩니다.그러므로 분산 시스템의 설정 복잡성을 해소할 수 있게 되는 것이죠.동적 대응애플리케이션의 새로운 설정을 적용하려면, 보통 애프리케이션을 재시작해야만 합니다.하지만, Config Server는 각 서비스 애플리케이션들이 재시작하지 않고도 동적으로 설정을 변경할 수 있도록 해줍니다.이로써 운영 중 발생할 수 있는 이슈에 빠르게 대응할 수 있다는 장점이 있습니다.버전 관리 용이Config Server가 중앙 집중식으로 관리하는 설정 파일들의 저장소로 Git을 사용할 수 있습니다.Git과 같은 버전관리 시스템과 연동하여 변경 이력을 관리하게 되면, 추후 필요에 따라 원하는 버전으로 롤백이 용이합니다.만약 Config Server가 없다면?설정 관리각 서비스마다 개별적으로 설정을 관리하면, 환경별로 일관된 설정 유지가 어려워집니다.운영 효율성 저하분산된 설정 파일을 하나하나 수정하고 배포해야 하므로, 관리 비용이 대폭 증가하여 운영 효율성이 떨어집니다.동적 반영 불가설정을 변경하려면 애플리케이션을 재시작해야 합니다.그리고 변경 사항이 즉각 반영되지 않기 때문에 서비스 중단이나 오류에 대해 대응이 느려질 수 밖에 없습니다.  이와 같이 Spring Cloud Config는 분산 환경에서의 설정 관리를 단순화하고, 운영 안정성을 높여주는 핵심 기술이라고 생각합니다.Config Server 설정하기Dependencydependencies {	implementation 'org.springframework.cloud:spring-cloud-config-server'}  Config Server 사용을 위해 의존성을 추가해줍니다.@EnableConfigServer 적용하기@SpringBootApplication@EnableConfigServerpublic class ConfigServiceApplication {	public static void main(String[] args) {		SpringApplication.run(ConfigServiceApplication.class, args);	}}  Config Server의 자동 구성을 위해 @EnableConfigServer 애너테이션을 적용해줍니다.application.ymlspring:  application:    name: config-service  cloud:    config:      server:        git:          uri: https://github.com/Seung-IL-Bang/spring-cloud-ecosystem-remote-config.git#          username: # private 저장소인 경우#          password: # private 저장소인 경우server:  port: 8888  위와 같이 설정하면 spring.cloud.config.server.git.uri 경로의 레포지토리로부터 설정 파일들을 Config Server에서 읽어올 수 있습니다.  private 저장소인 경우 접근 가능한 Git 계정의 username과 password를 입력해줍니다. (암호화해서 입력해야 안전합니다.)Config Server 설정 파일 조회  public git 저장소에 위와 같이 test.yml 파일 하나를 업로드 해놨습니다. 파일 내용은 다음과 같습니다.test:  value: 123  위 파일의 설정 값을 Config Server 에서 조회할 수 있습니다.{HOST:PORT}/{filename}/{profile}로컬 환경에서 테스트하는 경우 http://localhost:8888/test/default 경로로 Config Server에 GET 요청을 하시면 됩니다.배포 환경에 따라 프로파일(profile)별로 설정 파일들을 다르게 조회할 수 있습니다.파일명에 프로파일을 명시하지 않은 경우 기본 값으로 default 가 적용 됩니다.예시  운영 환경: {HOST:PORT}/test/prod  개발 환경: {HOST:PORT}/test/dev  로컬 환경: {HOST:PORT}/test/local  위 이미지는 Config Service로부터 test.yml 설정 파일을 읽어들인 화면입니다.  Config Server는 저장소(Git)로부터 설정 파일을 중앙 집중 관리하여 다른 서비스들에게 설정값들을 제공하는 역할을 합니다.📌 요약  Spring Cloud Config는 분산 환경에서 각 서비스의 설정을 중앙 집중식으로 관리해 일관성과 동적 반영을 가능하게 합니다.  Git 연동을 통해 버전 관리와 롤백이 용이하여 운영 효율성을 크게 향상시킵니다.🚀 정리이번 포스트에서는 Spring Cloud Config Server의 개념과 필요성에 대해 알아보았습니다.다음 포스트에서는 Spring Cloud 기반 MSA 환경에서 각 마이크로 서비스들이 Config Server로부터 파일을 읽어오는 방법과,Spring Boot Actuator 와 Spring Cloud Bus를 이용한 실시간 설정 반영 방법에 대해서도 살펴보도록 하겠습니다.]]></content>
      <categories>
        
          <category> Spring Cloud </category>
        
          <category> MSA </category>
        
      </categories>
      <tags>
        
          <tag> Spring Cloud Config </tag>
        
          <tag> Git </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Config Server와 Spring Cloud Bus를 통한 동적 설정 관리]]></title>
      <url>/spring%20cloud/msa/2025/02/17/Spring-Cloud-Bus/</url>
      <content type="text"><![CDATA[  분산 환경에서 각 마이크로서비스가 Config Server로부터 설정 파일을 읽어오는 방법을 알아봅시다!📌 들어가기이전에 Spring Cloud Config Server에 관한 내용을 다룬 포스트가 있습니다.이번 포스트에서는 Config Server를 사용하는 환경에서, Spring Cloud Bus와 Spring boot Actuator를 이용하여 애플리케이션을 재시작하지 않고도 설정 변경을 실시간으로 반영하는 방법에 대해 탐구해보겠습니다!실시간 설정 변경이 왜 중요한가?분산된 마이크로서비스 환경에서는 각각의 서비스가 개별적으로 재시작 없이 설정을 업데이트해야 운영 효율성이 높아집니다.동적 설정 변경을 통해 서비스 중단 없이 빠르게 환경 변화에 대응할 수 있기 때문입니다.이로 인해 설정 값들의 일관성을 쉽게 유지할 수 있게 됩니다.해당 기술이 없다면?설정을 변경할 때마다 각 서비스마다 재시작해야 하는 번거로움이 존재합니다. 이 과정에서 오타나, 잘못된 값이 설정된다면 또 다시 재시작 과정을 반복해야 하죠.또한 변경 사항이 즉각적으로 반영되지 않기 때문에(재시작으로 인해), 설정 불일치로 인한 오류가 발생할 수 있습니다.마지막으로, 분산 환경에서 개별적으로 설정을 관리하게 된다면 유지보수의 비용이 크게 증가할 것입니다.  Config Server를 사용한다면 애플리케이션을 재시작하지 않고도 설정 변경을 실시간으로 반영할 수 있도록 지원하는 Spring Cloud Bus, Spring Boot Actuator 기술 도입이 필수라고 생각합니다.마이크로서비스와 Config Server 연동하기클라이언트(마이크로서비스)는 Config Server에서 설정 파일을 읽어올 수 있어야 하기 때문에, Config Server와 연결을 진행해야 합니다.Dependencyimplementation 'org.springframework.cloud:spring-cloud-starter-config'implementation 'org.springframework.boot:spring-boot-starter-actuator'  Config Server와 연결을 위해 spring-cloud-starter-config 의존성을 추가해줍니다.  Config Server에게 설정 파일 조회 또는 갱신 요청을 위해 actuator 의존성을 추가해줍니다.  actuator의 엔드포인트로 /refresh, /busrefresh 를 사용할 예정입니다.application.ymlspring:  cloud:    config:      name: # 읽어올 파일명  config:    import: optional:configserver:http://localhost:8888  profiles:    active: # 프로파일명    management:  endpoints:    web:      exposure:        include: refresh, busrefresh  spring.cloud.config.name: Config Server로 부터 읽어올 파일명을 지정해줍니다.  spring.config.import: Config Server 연결 URL을 지정해줍니다. (로컬 테스트: Config Server가 port 번호 8888로 실행 가정)  spring.profiles.active: 읽어올 파일의 프로파일을 지정할 수 있습니다. (기본 프로파일: default)  management.endpoints.web.exposure.include: actuator에서 활성화시킬 엔드포인트 입니다.          /refresh: 개별 클라이언트만 설정을 갱신합니다.      /busrefresh: Bus(RabbitMQ 또는 Kafka)에 연결된 모든 클라이언트의 설정을 갱신합니다.      여기까지 설정을 마치면, /refresh 엔드포인트 요청으로 Config Server로 부터 설정 파일을 읽어와서 클라이언트의 설정을 동적으로 실시간 변경할 수 있게 됩니다.하지만, 이것은 개별 클라이언트에만 변화가 생깁니다.만약 변경된 설정 값을 여러 마이크로서비스들에게 공통적으로 동시에 적용하고 싶다면 어떡해야 할까요?개별 클라이언트마다 /refresh 요청을 날려야 할까요? (아마도 매우 번거롭고 귀찮은 작업일 것입니다.)이럴 때 사용하는 것이 Spring Cloud Bus 입니다.Spring Cloud Bus란?Spring Cloud Bus는 메시지 브로커(RabbitMQ, Kafka 등)를 이용해 여러 마이크로서비스 간에 설정 변경 이벤트를 실시간으로 전파합니다.이제 Spring Cloud Bus 도 추가 설정하여 동시에 여러 마이크로서비스들의 설정 값들을 실시간으로 반영하도록 해봅시다.Bus 작동 테스트를 위해 RabbitMQ와 Kafka 두 개를 이용하여 테스트를 진행해봤습니다.두 메시지 브로커 모두 Bus를 쉽게 구성할 수 있었습니다.하지만 백엔드 개발자라면, 기술을 채택하는 데 있어 근거를 가지고 있어야겠죠?결과적으로 저는 개인 사이드 프로젝트를 위해 Kafka를 적용하긴 했습니다만, 두 메시지 브로커 설정 방식에 대해 모두 설명 해드리겠습니다.Bus로 사용하기 적합한 메시지 브로커?설정 방식을 알아보기 전에, Spring Cloud Bus의 메시지 브로커로 RabbitMQ와 Kafka를 선택할 때 고려할 수 있는 장단점을 비교해보겠습니다.RabbitMQ장점  비교적 간단하게 설치 및 운영할 수 있으며, 관리 UI도 직관적입니다.  AMQP를 기반으로 하여 Pub/Sub, Work Queue, RPC 등 다양한 패턴을 쉽게 구현할 수 있는 유연한 메시징 패턴을 제공합니다.  일반적인 애플리케이션 환경에서는 충분한 성능을 제공하며, 소규모 시스템에 적합합니다.단점  대용량 데이터 처리에는 적합하지 않습니다.  Kafka에 비해 수평 클러스터 확장성이 떨어집니다.Kafka장점  대용량 메시지를 빠르게 처리할 수 잇으며, 클러스터 확장이 용이합니다.  메시지를 디스크에 지속적으로 저장하여 내구성을 제공합니다.  스트리밍 데이터 처리에 최적화되어 있습니다.단점  초기 설정과 운영 및 모니터링이 RabbitMQ보다 복잡합니다.  메시지 시스템의 개념과 운영 전략에 대한 Kafka 학습 비용이 필요합니다.  단순한 시스템에 도입하기에는 오버엔지니어링이 될 수 있습니다.선택 기준 정리  소규모 시스템이나 간단한 메시징 패턴이 필요하다면 RabbitMQ가 빠르고 쉽게 도입할 수 있는 선택입니다.  대규모 분산 환경이나 높은 처리량, 내구성이 요구되는 경우에는 Kafka가 적합합니다.두 메시지 브로커 모두 Spring Cloud Bus를 활용한 동적 설정 변경을 지원하므로, 시스템 규모와 메시지 처리량, 확장성을 고려하여 적절한 브로커를 선택하시면 됩니다.Spring Cloud Bus 설정Kafka를 이용한 BusDependencyimplementation 'org.springframework.cloud:spring-cloud-starter-bus-kafka'  클라이언트가 Kafka를 통해 Bus 이벤트를 수신 및 발신할 수 있도록 의존성 추가해줍니다.클라이언트 application.ymlspring:  kafka:    bootstrap-servers: http://localhost:9092 # Kafka URL  cloud:    bus:      destination: bus-topic # Bus 이벤트를 주고받을 토픽  spring.kafka.bootstrap-servers: Bus로 이용할 Kafka 브로커 서버의 URL (클러스터링이 된 경우 ,를 기준으로 여러 브로커 URL을 명시하면 됩니다.)  spring.cloud.bus.destination: Bus 이벤트를 Pub/Sub 할 토픽을 명시해줍니다.  ⚠️ 위 설정은 Config Server도 마찬가지로 동일하게 설정되어 있어야 합니다!management:  endpoints:    web:      exposure:        include: refresh, busrefresh  클라이언트의 actuator 엔드포인트 /busrefresh를 이용하면 메시지 브로커의 설정된 토픽으로 이벤트가 발송됩니다.  해당 토픽을 Consume(소비/구독)하는 클라이언트들은 모두 Config Server로 부터 설정 파일의 갱신을 요청하게 됩니다.  이로써 동시에 여러 마이크로서비스들의 설정 값을 실시간으로 업데이트 할 수 있게 됩니다./busrefresh/busrefresh 성공 응답으로 204(no-content) 응답이 옵니다.bus topic Log/busrefresh 이벤트가 발생하여 Kafka 토픽에 메시지가 도달하면 아래와 같은 JSON 포맷으로 기록되는 것을 확인하실 수 있습니다.type 필드와 originService 를 살펴보면 어떤 클라이언트에서 이벤트를 발송했고, 어떤 클라이언트가 이벤트를 수신했는지 확인할 수 있습니다.  type: 어떤 클라이언트에서 /busrefresh 요청이 발생했고, 어디서 그 이벤트를 수신했는지 확인할 수 있습니다.          RefreshRemoteApplicationEvent: /busrefresh를 호출한 이벤트입니다.      AckRemoteApplicationEvent: /busrefresh 요청을 수신했다는 응답 이벤트입니다.        originService: 이벤트를 처리한 클라이언트(마이크로서비스)의 애플리케이션 이름 정보가 담겨있습니다.[  {    "type": "RefreshRemoteApplicationEvent",    "timestamp": 1738394015516,    "originService": "order-service:default:8082:48ea6afcb8f6479b592a8cf041d2927b",    "destinationService": "**",    "id": "d52e5139-9137-4371-8da9-f5152a3ed8f0"  },  {    "type": "AckRemoteApplicationEvent",    "timestamp": 1738394015600,    "originService": "order-service:default:8082:48ea6afcb8f6479b592a8cf041d2927b",    "destinationService": "**",    "id": "f7c31aa1-a1df-4952-9e6c-a2ddd51c65f9",    "ackId": "d52e5139-9137-4371-8da9-f5152a3ed8f0",    "ackDestinationService": "**",    "event": "org.springframework.cloud.bus.event.RefreshRemoteApplicationEvent"  },  {    "type": "AckRemoteApplicationEvent",    "timestamp": 1738394018002,    "originService": "config-service:8888:5cadfc3cecb50346571c4e594886d273",    "destinationService": "**",    "id": "ce0d38d8-bd5a-43ce-b9cf-ea96456265d1",    "ackId": "d52e5139-9137-4371-8da9-f5152a3ed8f0",    "ackDestinationService": "**",    "event": "org.springframework.cloud.bus.event.RefreshRemoteApplicationEvent"  },  {    "type": "AckRemoteApplicationEvent",    "timestamp": 1738394021382,    "originService": "user-service:default:8080:4938cf08c833b263a04aa56c247721d3",    "destinationService": "**",    "id": "5bb5898f-a570-475d-b6c6-3efa7516b4b6",    "ackId": "d52e5139-9137-4371-8da9-f5152a3ed8f0",    "ackDestinationService": "**",    "event": "org.springframework.cloud.bus.event.RefreshRemoteApplicationEvent"  },  {    "type": "AckRemoteApplicationEvent",    "timestamp": 1738394022065,    "originService": "product-service:8081:2dc75521f12081519f46be8f3441ee00",    "destinationService": "**",    "id": "949d77da-9861-4066-8aca-b9b3aaacb678",    "ackId": "d52e5139-9137-4371-8da9-f5152a3ed8f0",    "ackDestinationService": "**",    "event": "org.springframework.cloud.bus.event.RefreshRemoteApplicationEvent"  }]  위 Json 파일을 살펴보면, order-service에서 /busrefresh를 호출한 것을 알 수 있습니다.  그리고 order-service, config-service, user-service, product-service 가 확인 응답을 하여, 설정 파일이 갱신되었다는 것을 알 수 있습니다.  참고로 Kafka는 도커 이미지를 이용하여 구동하였습니다. 이번 포스트는 Kafka에 대한 포스트가 아니므로, Kafka 도커 실행에 대한 내용은 생략하도록 하겠습니다.여기까지 Kafka를 이용한 Spring Cloud Bus 를 알아보았습니다.RabbitMQ를 이용한 Bus이번에는 RabbitMQ를 이용한 Spring Cloud Bus 설정 방법을 알아보겠습니다.동작 흐름은 Kafka와 동일하기 때문에, RabbitMQ는 설정법만 간단히 짚고 넘어가도록 하겠습니다.spring:  application:    name: user-service  config:    import: optional:configserver:http://localhost:8888  profiles:    active: default  cloud:    config:      name: test  rabbitmq:    host: localhost # RabbitMQ Host    port: 5672 # RabbitMQ Port    username: guest     password: guestmanagement:  endpoints:    web:      exposure:        include: refresh, busrefresh  Spring Cloud Bus를 위해서 Config Server와 연결 설정과 RabbitMQ 설정 그리고 액추에이터의 /busrefresh 엔드포인트를 설정해주기만 하면 됩니다.  /busrefresh 를 호출하면 RabbitMQ에 메시지를 발송하여, RabbitMQ(Bus 역할)에 연결된 서비스들의 설정 파일들을 갱신시키게 됩니다.  RabbitMQ는 도커 이미지를 이용하여 구동하였습니다.🚀 정리이번 포스트에서는 Spring Cloud Bus와 Spring Boot Actuator를 활용하여 Config Server에서 읽어온 설정 파일을 애플리케이션 재시작 없이 동적으로 갱신하는 방법을 다뤘습니다.Kafka와 RabbitMQ 두 가지 메시지 브로커를 통한 Bus 설정 방법과 각각의 장단점에 대해서도 알아봤습니다./refresh와 /busrefresh 엔드포인트를 활용해 개별 서비스와 전체 마이크로서비스에 동시에 설정 변경 이벤트를 전파하는 과정도 살펴봤습니다.🎯 요약  Spring Cloud Bus와 Actuator를 사용하면 Config Server의 설정 변경을 각 마이크로서비스에 실시간으로 적용할 수 있습니다.  Kafka는 대용량 처리와 확장성이 뛰어나고, RabbitMQ는 설치와 운영이 간편하다는 장점을 제공합니다.]]></content>
      <categories>
        
          <category> Spring Cloud </category>
        
          <category> MSA </category>
        
      </categories>
      <tags>
        
          <tag> Spring Cloud Config </tag>
        
          <tag> Spring Cloud Bus </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Cloud Netflix - Service Discovery Eureka]]></title>
      <url>/spring%20cloud/msa/2025/02/16/Service-Discovery-Eureka/</url>
      <content type="text"><![CDATA[  서비스 디스커버리가 무엇이고, Eureka가 무엇인지 알아봅시다!📌 들어가기서비스 간 통신을 하기 위해서는 서로의 위치(IP Address 혹은 도메인)를 알고 있어야 합니다.MSA 환경에는 수 많은 서비스들이 존재할 수 있습니다.그렇다면 한 서비스에서 다른 서비스들과 통신 하기 위해 다른 서비스들의 모든 주소들을 알아야 할까요?Netflix 가 Spring Cloud 재단에 기증한 서비스 디스커버리(탐색) 기술인 Eureka는 모든 주소들을 기억해야 하는 문제점을 깔끔히 해결해 줍니다.이번 포스트에서는 마이크로서비스 환경에서 서비스 등록과 탐색이 왜 중요한지, 그리고 동적 서비스 등록 및 로드밸런싱의 필요성에 대해 알아보겠습니다.서비스 디스커버리란?마이크로서비스 환경에서는 개별 서비스들이 독립적으로 배포되고 실행이 됩니다.이때 서비스 인스턴스들의 위치(Host, Port etc)가 동적으로 변경되기 때문에, 클라이언트가 서비스의 위치를 고정된 주소로 참조하기 어려운 문제가 발생합니다.서비스 디스커버리는 클라이언트가 서비스의 실제 위치를 동적으로 탐색하고, 최신 정보를 반영할 수 있도록 하는 역할을 합니다.서비스 디스커버리 패턴서비스를 동적으로 탐색하는 패턴에는 두 가지가 있습니다.Client Side Discovery클라이언트가 직접 서비스 레지스트리에서 정보를 가져와 로드밸런싱을 수행합니다.Server Side Discovery클라리언트의 요청이 로드 밸런서를 통해 전달되며, 이 로드 밸런서가 서비스 레지스트리(Eureka Server)에서 실제 서비스 인스턴스를 선택하게 됩니다.Spring Cloud Gateway 는 로드 밸런싱 기능을 지원하기 때문에 서버 사이드 디스커버리 패턴을 사용할 경우 Spring Cloud Gateway를 활용할 수 있습니다.게이트웨이는 유레카 서버로부터 서비스 인스턴스의 위치를 가지고와 직접 로드밸런싱을 수행합니다.Eureka Server 와 ClientEureka는 서버와 클라이언트 두 개로 나뉘어 역할이 각각 다릅니다.Eureka Server유레카 서버의 역할은 다음과 같습니다.  모든 서비스 인스턴스들의 등록 정보를 관리합니다.  주기적으로 서비스 인스턴스의 상태를 확인하여, 정상 동작하지 않는 인스턴스는 레지스트리에서 제거합니다.  클라이언트나 다른 서비스가 현재 사용 가능한 인스턴스 정보를 조회할 수 있도록 API를 제공합니다.Eureka Client유레카 클라이언트의 역할은 다음과 같습니다.  애플리케이션이 기동되면서 자동으로 Eureka 서버에 자신의 정보를 등록합니다.  등록된 정보를 주기적으로 갱신하여, 서버 측에서 정상 동작 여부를 판단할 수 있도록 합니다.  필요한 경우 Eureka 서버에서 다른 서비스의 정보를 가져와, 로드 밸런싱 및 동적 호출에 활용합니다.Eureka 특장점  서비스 인스턴스가 늘어나거나 줄어들 때 자동으로 반영됩니다.  유레카 서버 이중화 구성을 통해 장애 발생 시에도 서비스 디스커버리 지속성이 보장됩니다.  등록 주기와 타임아웃을 활용하여, 정상 상태를 유지하지 않는 인스턴스들은 자동으로 제거되고, 다시 회복된 인스턴스는 다시 등록되는 복구 메커니즘이 있습니다.동적 서비스 등록 메커니즘서비스가 기동 시 Eureka Client가 Eureka Server에 HTTP 요청을 보내어 자신을 등록합니다.이후 주기적으로 등록 정보를 갱신하며, 만약 갱신이 실패하면 일정 시간이 지난 후 자동으로 레지스트리에서 제거됩니다.Eureka 대시보드(보통 http://localhost:8761)를 통해 현재 등록된 인스턴스 목록과 상태 정보를 확인하실 수 있습니다.Eureke와 결합되어 동작하는 Spring Cloud GatewaySpring Cloud Gateway는 Eureka와 결합되어 로드 밸런서의 역할을 수행합니다.클라이언트의 요청이 게이트웨이에 도달하게 되면, Eureka에서 조회한 인스턴스 목록을 바탕으로 라운드 로빈 방식으로 요청을 분산 처리합니다.Eureka 서비스 클러스터링 방법과 장점단일 Eureka 서버로 운영하면 장애 시 전체 서비스 디스커버리 시스템이 마비될 수 있습니다.따라서 여러 대의 Eureka 서버를 클러스터링하여 운영할 필요가 있습니다.Eureka 서버를 클러스터링하게 되면 여러 Eureka 서버가 협력하여 서비스를 관리하기 때문에 특정 Eureka 서버의 장애가 전체 시스템에 미치는 영향을 최소화할 수 있습니다.실제 운영 환경에서는 두 대 이상의 서버로 구성하면, 리버스 프록시 또는 로드 밸런서를 통해 어느 서버에 접근하든지 간에 서비스 디스커버리 기능을 수행할 수 있게 됩니다.또한 다수의 Eureka 서버가 요청을 분산 처리함으로써, 높은 트래픽 환경에서도 안정적인 서비스를 제공할 수 있습니다.Eureka 서버 클러스터링 구성 예시application.yml 설정 파일에 Eureka 서버 간의 동기화 방식 설정(Peer Awareness)을 true 값으로 설정하고, Eureka 서버들이 호스팅되고 있는 위치들을 defaultZone 에 작성해주면 됩니다.eureka:  instance:    hostname: localhost  client:    register-with-eureka: false    fetch-registry: false  server:    enable-self-preservation: false  peer:    awareness:      enabled: true    eureka:      serviceUrl:        defaultZone: http://localhost:8761/eureka/,http://localhost:8762/eureka/  동기화 방식 설정과 관련하여 공식 문서에서 더 자세히 확인하실 수 있습니다.정리마이크로서비스 환경에서 서비스 디스커버리의 필요성과 Netflix Eureka를 통한 인스턴스들의 동적 위치 탐색 방안에 대해 알아보았습니다.Eureka를 사용하면 서비스 인스턴스가 동적으로 등록되고 자동으로 관리됩니다.이로써 서비스 디스커버리는 클라이언트가 서비스의 실제 위치를 동적으로 탐색할 수 있게 해주어 서비스 통신이 원활하게 이루어지도록 해줍니다.그리고 클러스터링을 통해 고가용성을 확보하는 방법에 대해서도 살펴보았습니다.참고 자료  Spring Cloud Netflix]]></content>
      <categories>
        
          <category> Spring Cloud </category>
        
          <category> MSA </category>
        
      </categories>
      <tags>
        
          <tag> Service Discovery </tag>
        
          <tag> Eureka </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Cloud FeignClient 로깅 및 에러 핸들링]]></title>
      <url>/spring%20cloud/msa/2025/02/15/Spring-Cloud-FeignClient/</url>
      <content type="text"><![CDATA[  Feign Logger와 ErrorDecoder로 API 통신 로깅 및 에러 핸들링 방법에 대해 알아봅시다!들어가기Spring Cloud의 FeignClient는 REST API 호출을 간편하게 구현할 수 있는 강력한 도구입니다.하지만 API 통신 시 발생할 수 있는 문제들을 효과적으로 디버깅하고, 사용자 정의 에러 핸들링을 구현하기 위해서는 추가적인 설정이 필요합니다.이번 포스트에서는 Feign Logger와 ErrorDecoder를 통해 API 통신의 로깅과 에러 핸들링을 강화하는 방법에 대해 살펴보겠습니다.Feign LoggerFeign Logger는 FeignClient가 호출하는 API의 요청과 응답을 로깅할 수 있는 기능을 제공합니다.이를 통해 API 통신 시 발생하는 문제를 보다 쉽게 파악할 수 있습니다.로그 레벨에 따라 출력되는 정보가 다릅니다.Feign Logger.Level기본적으로 아무것도 설정하지 않은 경우에는 NONE 상태입니다.  NONE: 로깅을 하지 않음  BASIC: 요청 메서드, URL, 응답 상태 코드 등 기본 정보만 로깅  HEADERS: BASIC 정보에 HTTP 헤더도 로깅  FULL: 모든 요청과 응답 바디를 포함한 상세 정보 로깅Feign Logger 설정하기아래 코드는 Logger.Level 을 FULL 로 설정하여 모든 요청과 응답 바디를 로깅하도록 하는 설정 코드입니다.@Configurationpublic class FeignConfiguration {    @Bean    Logger.Level feignLoggerLevel() {        // FULL 로그 레벨 설정        return Logger.Level.FULL;    }}ErrorDecoderErrorDecoder는 FeignClient 호출 시 에러 응답을 커스터마이징하여 처리할 수 있게 해주는 인터페이스입니다.사용 목적으로는 기본적인 예외 처리 외에, 사용자 정의 예외로 변환하여 보다 의미 있는 에러 처리를 할 수 있습니다.FeignClient 를 통해 발생하는 모든 예외는 ErrorDecoder를 거치게 됩니다.Custom ErrorDecoder 구현🚨 커스터마이징된 ErrorDecoder 가 동작하기 위해서는 스프링 Bean 으로 등록되어 있어야 합니다.아래 코드는 사이드 프로젝트에서 ErrorDecoder 인터페이스를 구현한 클래스입니다.@Componentpublic class FeignErrorDecoder implements ErrorDecoder {    @Override    public Exception decode(String methodKey, Response response) {        switch (response.status()) {            case 400:                // 400 예외의 경우, 별도의 사용자 정의 예외 처리 가능                break;            case 404:                // 404 예외의 경우, 사용자 정의 예외 메시지 응답                if (methodKey.contains("getOrders")) {                    return new ResponseStatusException(HttpStatus.valueOf(response.status()), "User's orders are not found");                }                break;            default:                return new Exception(response.reason());        }        return null;    }}String methodKey이 파라미터는 FeignClient 인터페이스의 특정 메서드를 식별하는 고유한 키입니다.호출한 메서드의 이름과 관련 정보를 포함하여, 에러 발생 시 어떤 메서드 호출에서 문제가 발생했는지 식별할 수 있도록 도와줍니다.커스터마이징된 에러 처리 로직에서 특정 메서드에 따른 에러 핸들링을 다르게 적용할 수도 있습니다.예시: MyFeignClient#getUser 처럼 인터페이스 이름과 메서드 이름을 조합한 형태로 전달됩니다.Response response이 객체는 FeignClient가 API 호출을 통해 받은 HTTP 응답을 나타냅니다.HTTP 상태 코드, 헤더, 바디 등의 정보를 확인할 수 있습니다.또한 응답 데이터를 기반으로 에러 상황에 맞는 예외를 생성하거나 추가 로깅을 할 때 활용됩니다.Spring Cloud Retry와 ErrorDecoderFeignClient를 통해 API 호출 시 만약 예외가 발생한다면, 기본 또는 커스터마이징된 ErrorDecoder를 거치게 됩니다.예외가 발생한 경우 재시도 처리를 고려해볼 수 있을 것입니다.Spring Cloud는 재시도 처리에 대한 인터페이스를 제공하고, Resilience4j 는 Retry 의 구현체를 제공합니다.만약 Spring Cloud Retry 설정도 적용되어 있다면 재시도 처리와 ErrorDecoder 중 어떤 것이 더 먼저 처리될까요?FeignClient에서 요청이 실패할 경우의 동작 순서는 다음과 같습니다1. 오류 발생API 호출 결과 HTTP 오류(예: 4xx, 5xx)가 발생하면 FeignClient은 먼저 해당 응답을 받습니다.2. ErrorDecoder 처리받은 응답은 ErrorDecoder의 decode 메서드를 통해 처리됩니다.이때 ErrorDecoder는 응답에 따른 적절한 예외(Exception)를 생성합니다.만약 커스터마이징된 ErrorDecoder 빈이 존재한다면, ErrorDecoder 인터페이스의 기본 구현체(Default)를 대체합니다.3. RetryableException 여부에 따른 처리  만약 ErrorDecoder가 반환하는 예외가 RetryableException인 경우, Feign에 설정된 Retryer가 해당 예외를 감지하여 재시도 로직을 실행합니다.  반면, RetryableException이 아닌 일반 Exception이라면, 재시도 없이 호출이 실패한 것으로 처리됩니다.ErrorDecoder.DefaultErrorDecoder 인터페이스 내부에 중첩 클래스로 기본 구현체인 Default 클래스가 존재합니다.Default 의 decode 메서드를 살펴보겠습니다.public static class Default implements ErrorDecoder {    private final RetryAfterDecoder retryAfterDecoder = new RetryAfterDecoder();        ...    public Exception decode(String methodKey, Response response) {        FeignException exception = FeignException.errorStatus(methodKey, response, this.maxBodyBytesLength, this.maxBodyCharsLength);        Long retryAfter = this.retryAfterDecoder.apply((String)this.firstOrNull(response.headers(), "Retry-After"));        return (Exception)(retryAfter != null ? new RetryableException(response.status(), exception.getMessage(), response.request().httpMethod(), exception, retryAfter, response.request()) : exception);    }    private &lt;T&gt; T firstOrNull(Map&lt;String, Collection&lt;T&gt;&gt; map, String key) {        return map.containsKey(key) &amp;&amp; !((Collection)map.get(key)).isEmpty() ? ((Collection)map.get(key)).iterator().next() : null;    }}  decode 메서드의 return 구문을 살펴보면 최종적으로 RetryableException 예외를 반환하는 지, 아니면 다른 Exception 을 반환하는지에 따라 재시도 처리가 결정됩니다.  만약 Retry 가 적용되어 있고, ErrorDecoder 가 RetryableException 예외를 발생 리턴하면, 재시도 처리가 수행됩니다.  Long 타입의 retryAfter 변수는 클라이언트에게 재시도를 하기 전에 얼마나 기다려야 하는지(예: 초 단위)를 알려주는 정보입니다. 해당 정보는 응답 헤더에 담겨서 전달됩니다.📌 요약  Feign Logger로 상세 로그를 기록하고, Custom ErrorDecoder로 사용자 정의 예외 처리를 구현함으로써 서비스의 신뢰성과 유지보수성을 향상시킬 수 있음.  FeignClient 요청 실패 시 먼저 ErrorDecoder를 거쳐 예외가 생성되고, 그 예외가 RetryableException인 경우에만 재시도 메커니즘이 동작.참고 문헌  Spring Cloud OpenFeign  Feign Logging Configuration]]></content>
      <categories>
        
          <category> Spring Cloud </category>
        
          <category> MSA </category>
        
      </categories>
      <tags>
        
          <tag> Feign Logger </tag>
        
          <tag> ErrorDecoder </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Cloud CircuitBreaker]]></title>
      <url>/spring%20cloud/msa/2025/02/11/Spring-Cloud-Client-CircuitBreaker/</url>
      <content type="text"><![CDATA[  마이크로 서비스 간 통신 장애 대처 방식 중 CircuitBreaker 사용 방법에 대해 알아보자!들어가기MSA 통신 중 장애가 발생한 경우, 장애 대처 방식 중 하나인 CircuitBreaker 에 대해 알아보겠습니다.CircuitBreaker 를 직역하자면, 회로 차단기로 볼 수 있습니다.회로 차단기를 전류의 흐름을 막거나 흐르게 하는 흐름 제어기입니다.CircuitBreaker는 API 요청의 흐름을 막거나 흐르게 하는 API 요청 제어기로 볼 수 있습니다.만약 어떤 서버가 다운되버렸다면, 그 서버에 API 호출을 하더라도 응답을 받지 못할 것입니다.단순히 응답을 받지 못하는 것에 그치지 않고, 이 에러는 MSA 환경에서 전파가 됩니다.예를 들어, 사용자가 자기 자신의 주문 정보를 조회하는 API가 User Service에 있다고 가정해보겠습니다.클라이언트의 요청은 User Service -&gt; Order Service 서비스를 거치면서 필요한 정보들을 조회하는 API를 연달아 호출합니다. 이때 Order Service에 장애가 발생한다면, User Service는 응답을 받지 못 할 것이고 결국 User Service를 호출한 클라이언트도 에러를 응답받게 될 것 입니다.이런 상황의 경우, Order Service가 회복되기 전까지 반복적으로 요청을 해봤자 에러 응답만 받을 뿐입니다.그리고 주문 관련 데이터를 조회 못 했을 뿐, 사용자의 정보는 User Service에서 조회가 가능합니다. 단지, 주문 데이터를 조회 못 했다는 이유로 사용자에게 에러 응답을 보내는 것은 부자연스런 처리라고 생각합니다.이런 문제점들을 해결하고자, Spring Cloud 는 CircuitBreaker의 추상화 계층을 제공하며 CircuitBreak의 구현체 클래스로 Resilience4j가 널리 사용되고 있습니다.그러면 Resilience4j의 많은 기능 중 CircuitBreaker에 대해 알아보도록 하겠습니다.Spring Cloud CircuitBreaker스프링 클라우드는 CircuitBreaker의 추상화 계층을 제공합니다.아래 코드는 스프링 클라우드가 제공하는 CircuitBreaker의 인터페이스 입니다.package org.springframework.cloud.client.circuitbreaker;import java.util.function.Function;import java.util.function.Supplier;public interface CircuitBreaker {    default &lt;T&gt; T run(Supplier&lt;T&gt; toRun) {        return this.run(toRun, (throwable) -&gt; {            throw new NoFallbackAvailableException("No fallback available.", throwable);        });    }    &lt;T&gt; T run(Supplier&lt;T&gt; toRun, Function&lt;Throwable, T&gt; fallback);}CircuitBreaker의 사용법에 대해 간단히 알아보겠습니다.  우선, CircuitBreaker 인터페이스를 구현한 객체를 생성합니다.  구현체를 생성하고 run 메서드에 Supplier와 Function을 인자로 전달합니다.  Supplier 은 실제로 처리하고자 하는 비즈니스 로직을 작성해주면 됩니다.  Function 은 만약 Supplier가 실패하거나 회로가 차단된 경우에 실행하는 fallback 메서드를 작성해주면 됩니다. 즉, 에러를 응답하는 대신 별도의 로직을 처리하는 로직을 전달합니다.CircuitBreakerFactory스프링 클라우드는 CircuitBreaker의 Factory 패턴의 인터페이스도 제공합니다.Resilience4j는 해당 Factory 인터페이스의 구현체를 제공합니다.package org.springframework.cloud.client.circuitbreaker;public abstract class CircuitBreakerFactory&lt;CONF, CONFB extends ConfigBuilder&lt;CONF&gt;&gt; extends AbstractCircuitBreakerFactory&lt;CONF, CONFB&gt; {    public CircuitBreakerFactory() {    }    public abstract CircuitBreaker create(String id);    public CircuitBreaker create(String id, String groupName) {        return this.create(id);    }}Resilience4j의 의존성을 추가하면 CircuitBreakerFactory 구현체를 사용할 수 있게 됩니다.Resilience4j CircuitBreaker우선 Resilience4j 의존성을 추가해줍니다.// gradledependencies {  implementation 'org.springframework.cloud:spring-cloud-starter-circuitbreaker-resilience4j:3.2.0'  }그 다음으로 CircuitBreaker의 설정을 구성해줍니다.마지막으로 해당 설정을 가진 커스텀 CircuitBreakerFactory를 Bean 으로 등록하여 사용하면 됩니다.아래 코드는 CircuitBreaker의 설정을 커스텀한 CircuitBreakerFactory를 Bean 으로 등록하는 코드입니다.@Beanpublic Customizer&lt;Resilience4JCircuitBreakerFactory&gt; defaultCustomizer() {    return factory -&gt; factory.configureDefault(id -&gt; new Resilience4JConfigBuilder(id)            .timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(3)).build())            .circuitBreakerConfig(CircuitBreakerConfig.ofDefaults())            .build());}  기본적으로 설정값으로 제공하는 CircuitBreakerConfig 기본 베이스로 설정해줍니다.  그 외 커스텀마이징 하고 싶은 값들은 빌더(Builder) 패턴으로 각각 설정할 수 있습니다.  위 코드에서는 커스텀마이징한 timeLimiterConfig 설정은 지정한 시간 내에 작업이 완료되지 않으면 타임아웃을 발생시키는 설정입니다. 타임아웃이 계속 발생하다가 임계치를 넘어가게 되면, 회로가 차단(Closed)됩니다.기본 설정 값들은 CircuitBreakerConfig.ofDefault()는 아래와 같은 설정  값들로 구성되어 있습니다.각 설정들에 대한 의미는 공식 문서에 잘 정리되어 있습니다.public Builder() {    this.currentTimestampFunction = CircuitBreakerConfig.DEFAULT_TIMESTAMP_FUNCTION;    this.timestampUnit = CircuitBreakerConfig.DEFAULT_TIMESTAMP_UNIT;    this.recordExceptions = new Class[0];    this.ignoreExceptions = new Class[0];    this.failureRateThreshold = 50.0F;    this.minimumNumberOfCalls = 100;    this.writableStackTraceEnabled = true;    this.permittedNumberOfCallsInHalfOpenState = 10;    this.slidingWindowSize = 100;    this.recordResultPredicate = CircuitBreakerConfig.DEFAULT_RECORD_RESULT_PREDICATE;    this.waitIntervalFunctionInOpenState = IntervalFunction.of(Duration.ofSeconds(60L));    this.transitionOnResult = CircuitBreakerConfig.DEFAULT_TRANSITION_ON_RESULT;    this.automaticTransitionFromOpenToHalfOpenEnabled = false;    this.slidingWindowType = CircuitBreakerConfig.DEFAULT_SLIDING_WINDOW_TYPE;    this.slowCallRateThreshold = 100.0F;    this.slowCallDurationThreshold = Duration.ofSeconds(60L);    this.maxWaitDurationInHalfOpenState = Duration.ofSeconds(0L);    this.createWaitIntervalFunctionCounter = 0;}사이드 프로젝트 적용 코드사이드 프로젝트에서 CircuitBreaker를 적용한 코드를 간단히 살펴 보도록 하겠습니다.아래 코드는 userId 를 통해 사용자 정보와 주문 정보를 합쳐서 응답하는 간단한 메서드입니다.@Overridepublic UserDto getUserByUserId(String userId) {    log.info("user-service: 회원ID로 회원정보 및 주문 목록 조회");    Optional&lt;Users&gt; findUser = userRepository.findByUserId(userId);    if (findUser.isEmpty()) {        throw new NoSuchElementException("User not found");    }    UserDto userDto = modelMapper.map(findUser.get(), UserDto.class);    List&lt;ResponseOrder&gt; orders = orderClient.getOrdersByUserId(userId);    userDto.setOrders(orders);    return userDto;}  userId 로 user 엔티티를 조회합니다.  만약 userId에 해당하는 User가 없다면 에러가 발생합니다.  user 엔티티를 DTO로 매핑합니다.  CircuitBreaker를 적용한 OrderClient를 통해 user의 주문 목록을 조회해옵니다.  주문 목록을 DTO에 설정해주고 반환합니다.여기서 UserService -&gt; OrderService 는 에러가 발생할 수 있기 때문에, 주문 목록을 조회해오지 못 하고 에러가 전파될 가능성이 있습니다.아래 코드는 CircuitBreakerFactory 로 부터 CircuitBreaker 객체를 생성하고, 주문 목록을 조회해오는 API 호출 로직(Supplier)과 호출 실패 또는 회로 차단의 경우 대신할 로직(Fallback Function)을 CircuitBreaker 의 인자로 전달하는 코드입니다.@Service@RequiredArgsConstructorpublic class OrderClient {    private final OrderApiService orderApiService;    private final CircuitBreakerFactory circuitBreakerFactory;    public List&lt;ResponseOrder&gt; getOrdersByUserId(String userId) {        CircuitBreaker circuitBreaker = circuitBreakerFactory.create("circuitbreaker");        ResponseEntity&lt;List&lt;ResponseOrder&gt;&gt; orderResponse = circuitBreaker.run(                () -&gt; orderApiService.getOrdersByUserId(userId),                throwable -&gt; new ResponseEntity&lt;&gt;(new ArrayList&lt;&gt;(), null, HttpStatus.INTERNAL_SERVER_ERROR));        return orderResponse.getBody();    }}  회로가 차단되어 있지 않은 상태에서 정상적으로 API 응답을 받으면 기존 흐름대로 정상 응답합니다.  만약 Order Service가 응답에 실패하거나 회로가 차단되어 있는 상태라면 fallback 메서드를 수행하여 에러를 응답하는 대신 위 코드에서는 빈 리스트(Empty List)를 반환하도록 구성했습니다.  회로의 상태 전이, 실패 임계치 등 더 자세한 정보는 공식 문서를 통해 확인하실 수 있습니다.스프링 AOP와 CircuitBreakerCircuitBreaker 의 역할은 API 요청에 대한 흐름 제어입니다.기존 흐름대로 흘러가게 할 지, 아니면 기존 요청을 차단하고 다른 응답을 대신 처리 할 지를 결정합니다.만약 CircuitBreaker의 역할이 담긴 코드가 핵심 비즈니스 로직과 섞여있다면 가독성이 떨어지고 유지보수가 어려워질 것입니다.따라서 스프링은 애너테이션을 활용한 @CircuitBreaker 를 제공합니다.이는 프록시 패턴을 활용한 스프링 AOP를 적용한 사례입니다.아래 코드는 @CircuitBreaker를 활용한 간단한 예제 코드입니다.@RestControllerpublic class SampleController {    // name: 회로 차단기 이름, fallbackMethod: 실패 시 호출할 메서드명    @GetMapping("/process")    @CircuitBreaker(name = "myCircuitBreaker", fallbackMethod = "fallbackProcess")    public String process() {                // 핵심 비즈니스 로직                // 임의로 예외 발생        if (Math.random() &lt; 0.5) {            throw new RuntimeException("Simulated exception");        }        return "Operation succeeded!";    }    // 폴백 메서드: 원래 메서드와 같은 반환 타입, 동일 파라미터 목록에 Throwable 추가    public String fallbackProcess(Throwable throwable) {        // 예외 메시지를 로깅하거나 별도의 처리 가능        return "Fallback response: " + throwable.getMessage();    }}  위 코드를 보면 CircuitBreaker의 역할인 회로 차단에 관한 코드가 전혀 보이질 않습니다. 즉, 스프링 AOP에 의해 관심사가 완전히 분리된 것입니다.  따라서 CircuitBreaker를 적용하면서도 비즈니스 로직에 더 집중 할 수 있게 됩니다.🚀 추가 개발 및 고려하면 좋을 내용  Resilience4j의 메트릭 및 모니터링 기능(prometheus, micrometer와의 연동)  fallback 로직 외에도, 재시도(Retry)나 백오프(Backoff) 전략 조합📌 요약CircuitBreaker 개념:서비스 장애 시, 지속적인 요청으로 인한 오류 전파를 막고 대체 로직(fallback)을 실행하여 시스템 안정성을 높이는 역할을 합니다.Spring Cloud CircuitBreaker:다양한 CircuitBreaker 구현체(예: Resilience4j)를 추상화하여 사용하기 쉽고, Spring 생태계와 통합되어 있으며, AOP를 활용해 비즈니스 로직과 관심사를 분리합니다.Resilience4j CircuitBreaker:세밀한 설정과 최적화된 구현체를 제공하며, 커스텀마이징한 설정과 함께 사용할 수 있습니다.]]></content>
      <categories>
        
          <category> Spring Cloud </category>
        
          <category> MSA </category>
        
      </categories>
      <tags>
        
          <tag> Circuit Breaker </tag>
        
          <tag> Resilience4j </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Spring Cloud 기반 마이크로 서비스 간 통신]]></title>
      <url>/spring%20cloud/msa/2025/02/10/Spring-Cloud-%EA%B8%B0%EB%B0%98-%EB%A7%88%EC%9D%B4%ED%81%AC%EB%A1%9C%EC%84%9C%EB%B9%84%EC%8A%A4-%EA%B5%AC%EC%B6%95-%EA%B3%BC%EC%A0%95/</url>
      <content type="text"><![CDATA[  Spring Cloud를 활용한 마이크로 서비스 간 통신 방식에는 무엇이 있는지 살펴보겠습니다!📌 들어가기e-commerce 플랫폼을 주제로 MSA와 Spring Cloud 기반의 사이드 프로젝트를 진행했습니다.모놀리식이 아닌 사용자, 주문, 상품, 결제, 배달 등 각 도메인 별로 MSA 기반으로 개발했습니다.모놀리식의 경우 각 도메인 서비스별로 소통을 위해 네트워크를 거칠 필요가 없습니다.이에 반해, MSA 기반의 마이크로 서비스들은 도메인 서비스끼리 통신을 하기 위해서 네트워크 통신이 필수이다.다들 아시다시피, 대표적인 네트워크 통신에는 HTTP 통신이 있습니다.현대 대부분의 통신은 HTTP을 이용한 REST API로 이뤄진다고 봐도 무리가 아니죠.그 외에도 TCP 통신, gRPC 등 다양한 방식이 존재합니다.저는 사이드 프로젝트를 진행하면서 마이크로 서비스 간 통신 방식을 여러 번 수정해왔습니다.처음에는 RestTemplate 을 이용한 API 통신, 그 다음으로 Spring Cloud에서 지원하는 OpenFeign 을 통한 통신.마지막 종착지로, 마이크로 서비스 간 종속성을 없애기 위한 Kafka 이벤트 기반 비동기 통신을 적용했습니다.각 방식에는 장단점이 있으므로, 장단점을 이해하고 해결하고자 하는 문제에 어떤 방식이 최선일 지 결정하는 것이 중요합니다. (개발자의 덕목)API 소개우선, 사이드 프로젝트에서 개발한 API들에 대해 먼저 소개 해드리겠습니다.각각의 도메인 서비스들은 서로 API 통신을 통해 유기적으로 비즈니스 로직을 처리하게 됩니다.그리고 각 서비스 별로 발생하는 데이터들은 각각 고유한 데이터베이스에 데이터를 저장하게 됩니다.참고로, 해당 프로젝트의 핵심 목표는 분산 환경에서의 중앙 집중식 로깅 관리 및 트레이싱이기 때문에 각 서비스 별로 필요한 비즈니스 로직들에 비약한 부분이 존재할 수 있습니다.User Service  회원 가입  로그인  유저 정보 조회  유저 목록 조회Product Service  상품 목록 조회  재고 차감  재고 증가Order Service  주문  주문 조회  주문 목록 조회Payment Service  결제  결제 취소Delivery Service  배송 접수  배송RestTemplateRestTemplate은 Spring 에서 기본으로 제공하는 HTTP 요청을 보낼 때 사용하는 도구입니다.장점  간단하고 직관적이다.  동기 방식이기 때문에 흐름을 따라가기 쉽다.단점  요청을 보낸 후 응답을 받을 때까지 대기한다. (try-catch로 예외 처리 + 타임아웃 설정이 필수 ⚠️)  부하가 큰 시스템에서는 병목 현상이 발생할 수 있다.  예외 발생 시 장애가 전파됨.예제 코드@Service@RequiredArgsConstructorpublic class UserServiceImpl implements UserService {    private final RestTemplate restTemplate;    ...    @Override    public UserDto getUserByUserId(String userId) {      ...      ResponseEntity&lt;List&lt;ResponseOrder&gt;&gt; ordersResponse = restTemplate.exchange(        "http://example.com/orders/" + userId,        HttpMethod.GET,        null,        new ParameterizedTypeReference&lt;&gt;() {}      );    }}  RestTemplate 을 사용한 코드 일부입니다. (사용자의 주문 목록을 조회하는 요청)  몇 개의 인자를 전달함으로써 간단하게 HTTP 요청을 보낼 수 있습니다.          요청 URL      HTTP 메서드      HttpEntity (요청 본문 및 헤더)      Class&lt;?&gt; responseType (응답 타입)            🚨 다만, 동기 방식(Synchronous)으로 동작하기 때문에, 특정 서비스가 중단되거나 응답이 지연되면 요청을 보낸 서비스도 영향을 받는다는 것을 주의해야 합니다.  OpenFeignOpenFeign은 HTTP 요청을 보내는 작업을 자동으로 해주는 도구입니다. RestTemplate과 비슷하지만, 인터페이스만 만들면 요청 코드가 자동으로 생성되는 편의성을 제공합니다.장점  코드가 간결함 (인터페이스 정의만 하면 됨)  내부적으로 로드 밸런싱 가능 (Eureka와 연동 시 URL 필요 없음)  Feign + Resilience4j로 Circuit Breaker(회로 차단기) 적용 가능. (Fallback 기능 포함)  Spring Cloud 생태계와 연동이 쉬움.단점  기본적으로 동기 방식이므로 응답을 기다려야 함.  추가적인 설정이 필요할 수도 있음 (Feign 인터셉터, 로깅 등)  예외 발생 시 장애가 전파됨.예제 코드@Service@RequiredArgsConstructorpublic class UserServiceImpl implements UserService {    private final OrderServiceClient orderServiceClient;    ...    @Override    public UserDto getUserByUserId(String userId) {      ...      List&lt;ResponseOrder&gt; orderList = orderServiceClient.getOrders(userId);    }}@FeignClient(name = "order-service")public interface OrderServiceClient {    @GetMapping("/orders/{userId}")    List&lt;ResponseOrder&gt; getOrders(@PathVariable("userId") String userId);}  OpenFeign 을 사용할 경우 인터페이스만 구현하면 되기 때문에 RestTemplate 보다 더 간결하고 직관적이다.  Eureka(디스커버리 서비스)와 연동 시, 서비스의 이름만으로 요청을 보낼 수 있다.KafkaKafka는 비동기 메시지 큐 시스템입니다. 비동기이기 때문에 응답을 기다릴 필요가 없습니다. 메시지 처리도 바로 할 필요가 없습니다.장점  비동기 방식이라 대량의 데이터를 효율적으로 처리 가능.  여러 서비스 간 독립적인 운영이 가능 (서로 느슨한 결합이 됨)  데이터 복구 및 확장성이 뛰어남.단점  실시간 응답이 필요한 경우에는 적절하지 않음.  러닝 커브 존재 (프로듀서, 컨슈머 개념 등 Kafka에 대한 이해 필요)  메시지 처리 순서를 보장하려면 추가적인 설정이 필요.예제 코드@Service@Slf4j@RequiredArgsConstructorpublic class OrderServiceImpl implements OrderService {    private final OrderRepository orderRepository;    private final OrderEventProducer orderEventProducer;    ...    @Transactional    @Override    public OrderDto createOrder(OrderDto orderDto) {        ...        orderRepository.save(order); // 주문 데이터 저장         orderEventProducer.send(ORDER_CREATED, orderCreatedEvent); // ORDER_CREATED 메시지 발행        ...    }  주문이 생성되면 주문 데이터를 저장하고 이어서 주문 생성 메시지를 발행합니다.  메시지는 KafkaTemplate 을 이용한 Producer(프로듀서)를 통해 발행합니다.@Service@Slf4j@RequiredArgsConstructorpublic class OrderCreatedEventConsumer {    private final ObjectMapper objectMapper;    private final PaymentHandler paymentHandler;    @KafkaListener(topics = "ORDER_CREATED", groupId = "${spring.kafka.consumer.group-id:payment-service-group}")    public void consume(ConsumerRecord&lt;String, String&gt; record) throws JsonProcessingException {        try {            String message = record.value();            log.info("Consumed message: {}", message);            OrderCreatedEvent orderCreatedEvent = objectMapper.readValue(message, OrderCreatedEvent.class);            log.info("Order created event: {}", orderCreatedEvent);            paymentHandler.handle(orderCreatedEvent);         } catch (Exception e) {            log.error("Error Consume OrderCreatedEvent", e);        }    }}  주문 생성 메시지가 결제 서비스의 컨슈머(Consumer)에 도달하면, 주문의 결제 처리를 진행합니다.  즉, 주문 -&gt; 결제의 과정이 동기식이 아닌 비동기식으로 진행됩니다.🚀 요약비교 정리 및 실행 흐름            방식      요청 방식      응답 처리      특징                  RestTemplate      직접 HTTP 요청      동기 (응답 받을 때까지 대기)      간단하지만 응답 기다려야 함              OpenFeign      인터페이스 기반 HTTP 요청      동기 (설정 시 비동기 가능)      코드가 간결, Spring Cloud와 연동 용이              Kafka      메시지 전송      비동기 (나중에 메시지 수신)      대량의 데이터 처리에 적합      실제 사용 예시            사용 사례      어떤 방식을 선택할까?                  REST API에서 데이터를 가져올 때      RestTemplate 또는 OpenFeign 사용              마이크로서비스 간 통신이 많을 때      OpenFeign 추천 (Spring Cloud 연동)              이벤트 기반 비동기 처리      Kafka 사용 (비동기 메시징)      ]]></content>
      <categories>
        
          <category> Spring Cloud </category>
        
          <category> MSA </category>
        
      </categories>
      <tags>
        
          <tag> RestTemplate </tag>
        
          <tag> OpenFeign </tag>
        
          <tag> Kafka </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[성적 관리 모델링 연습 (3)]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/06/18/grades-manage3/</url>
      <content type="text"><![CDATA[과목-개강-선생님 관계도  과목과 선생님은 M:N 관계이다. 그 가운데서 비즈니스 로직을 담당하는 관계 테이블인 개강 테이블이 존재한다.  개강 테이블(OpenLecture)의 PK는 TeacherID , SubjectId , OpenDate 가 합쳐진 복합키이다. 이처럼 상속형 PK를 사용하는 이유는 한 선생님이 동일한 학년 과목을 중복으로 개강할 수 없기 때문이다.  개강 테이블의 Seq 대체키를 선언하여 조회를 편리하게 돕도록 했다.현재까지 완성된 ERD학년 마스터 테이블이 개강 관계 테이블과 1:M 관계를 맺었다.  학년별로 각 과목이 존재하기 때문에 개강 테이블 PK에 SchoolGradeId 를 추가해준다. 이렇듯 관계 테이블에는 여러 집단으로부터 참조되는 테이블이다.선생님 테이블의 PK가 반 테이블의 FK로 1:M 관계를 맺고 있다.  선생님 테이블의 PK가 반 테이블의 FK로 1:M 관계를 맺고 있지만, 원칙적으로 1:1 관계이다. 하지만 FK를 설정해준 목적은 담임 선생님이 없는 반 데이터를 만들지 않기 위함이다. 즉, 외래키 제약 조건을 이용하여 반드시 담임 선생님이 존재하도록 FK로 TeacherId 를 설정한 것이다. (관계가 1:M 이라서 설정한 것이 아닌 것을 짚고 넘어가자.) 이렇듯 관계가 1:M은 아니지만, PK와 FK 설정을 맺어줌으로써 FK로 사용되는 컬럼의 도메인 값을 기준 테이블(선생님 테이블) 의 도메인으로 한정짓고 반드시 값이 입력되도록 하기 위해서 사용되기도 한다.  TeacherID 가 반 테이블에 들어옴으로써 언제 담임을 맡게 됐는지에 대한 정보가 존재해야지만 테이블 해석이 가능해진다. 따라서 AssignedDate 라는 담임을 맡게된 날짜 컬럼을 추가해준다.  참고로 StudentId 타입을 bigint 로 변경해줘야 한다. (초기 설계 오류)          또는 entrance_date 를 추가하면 매년 신입생이 입학할 때마다 StudentId 가 1번부터 다시 시작하는 방법도 존재한다. 이렇게하면 smallint 로 그대로 사용해도 된다.        TearcherId 의 타이블 tinyint 에서 int 로 바꿔주자. 새로 발령오거나 퇴직하는 상황이 발생하면 tinyint 만으로는 부족할 것이다. (초기 설계 오류)최종 ERD - 자동 정렬  학생과 개강 테이블 간의 시험 점수 관계 테이블을 생성해준다.(M:N 관계)          학생ID와 개강 테이블의 보조키(Seq)를 PK 복합키로 잡는다.      시험 점수 테이블(TB_Score)에 시험 점수(score) 속성을 추가해준다.      최종 ERD - 위치 변경  ERD 테이블 위치 선정 팁          마스터 테이블 또는 기준 테이블들은 바깥에 위치시킨다.      관계 테이블들은 가운데에 위치하도록 한다. 관계가 많이 맞물려있을 수록 가운데에 놓도록 하자. 핵심 비즈니스 관계 테이블일 확률이 높다.        OpenLecture 개강 테이블의 경우 학년, 과목, 선생님 테이블들의 관계 테이블 역할을 수행하면서, 동시에 시험 점수 테이블의 마스터 테이블로도 이용되고 있다.  1:M, M:N 관계를 모르는 상태에서 설계를 하면 십중팔구 조인 테이블(=결과 테이블, 엑셀)을 설계한다. 잘못된 설계를 하면 데이터 이상 현상(중복, 삭제, 갱신) 등이 발생할 수 있다.erdcloud 툴을 이용하여 정리한 ERD~~ERDCloud 툴에는 개강 테이블의 보조키를 성적 테이블의 PK로 잡는 방법이 없는 것 같다. ~~]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[RDB 1:1 관계]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/06/18/1-1-relation/</url>
      <content type="text"><![CDATA[  1:1 관계에 대해 알아보자.1:1 관계는 부부 관계이다.1 : 1 관계란 양측 입장에서 상대를 보더라도 반드시 단 하나의 관계를 가지는 것을 말한다.1:1 관계는 부부관계라고 말한다.가장 단순한 관계로 보이지만 중급 이상의 고급 기술에 해당된다.1:M, M:N 관계로 해결할 수 없는 많은 문제들이 이 관계를 통해서 해결되는 것을 알 수 있다.신랑, 신부 1대1 관계를 나타낸 엑셀 표            신랑ID      신랑명      신부ID      신부명                  1      홍길동      B      이난영              2      김종남      A      황금자              3      이하선      C      박상순      위 테이블은 조인 테이블의 형태이다. 1:1 관계를 나타나는 데 PK가 신랑ID, 신부ID 두 개로 잡아야 하는데 PK의 최소성을 만족하지 않는것 같다.테이블을 다시 설계해보자.(부부 테이블 생성)부부 테이블            열 이름      데이터 형식      NULL 허용                  부부Id      int      x              신랑명      varchar(50)      o              신부명      varchar(50)      o      INSERT INTO 부부VALUES (1, '홍길동', '어우동'), (2, '김길동', '황진이');                   부부Id      신랑명      신부명                  1      1      홍길동      어우동              2      2      김길동      황진이        우리가 부부 테이블fh 하고 싶은 것은 이 하나의 테이블을 1:1 관계를 유지하되 두 개의 객체(신랑, 신부)로 분리하는 것이다.  왜 분리해야 되는가?  신랑, 신부별로 각각 자신만의 특성이 존재하기 때문에 서로 마스터 테이블로 분리하는 것이 좋다.1:1 관계 테이블의 PK  1:1 관계는 서로의 PK를 FK로 참조하진 않지만, 서로의 PK를 자신의 PK로 사용하는 형태이다. 일심동체서로의 PK를 자신의 PK로 사용1:1 관계를 맺은 두 테이블을 조인하면 위에서 살펴봤던 부부 테이블의 결과(엑셀) 테이블과 동일한 결과를 얻어낼 수 있다.부부 테이블 VS 신랑,신부 개별 테이블부부 테이블에 모든 속성들을 추가해서 사용해도 문제는 없지만 부부 테이블이 다른 관계 테이블과 관계를 맺게되면 조회 시 쓸데 없는 컬럼들을 제거 해줘야 하는 불편함이 존재할 수도 있다.신랑, 신부 개별 테이블로 분리하는 것이 각각 속성들을 관리하기도 편리하고,다른 관계 테이블들과 관계를 확장해나갈 수 있다는 장점이 존재하기 때문에 개별 테이블로 분리하여 관리하는 것이 더 좋다고 본다.  ⭐️ 정리하면, 1:1 관계는 원래 일심동체로 하나이다. 하지만 테이블은 두 개로 관리되는 것이 유용한 편이다.]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
          <tag> Relation </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[DB에서 트랜잭션이란 무엇인가?]]></title>
      <url>/database/transaction/db%20engineering/2024/06/15/Transaction/</url>
      <content type="text"><![CDATA[정의하나의 작업 단위로 취급되는 SQL 쿼리들의 모음.트랜잭션 LifecycleTransaction BEGINTransaction COMMITTransaction ROLLBACKTransaction unexpected ending  = ROLLBACK (e.g. crash)만약 1000개의 쿼리, 1000개의 변경을 커밋한다면, 이 모든 변경을 실제로 디스크에 모두 쓰는 걸까?아니면 기다렸다가 모든 것을 메모리에 넣은 다음 커밋할 때 한꺼번에 디스크에 쓰는 걸까?각각의 방법에는 장단점이 있다.첫 번째 방법은 커밋을 빠르게 만든다.두 번째 방법은 커밋을 느리게 만든다.결국 이러한 장단점을 파악하면서 개발을 해야한다.트랜잭션 역할  트랜잭션은 보통 데이터를 수정하거나 업데이트할 때 사용된다.  읽기 전용 트랜잭션도 존재한다.]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> Transaction </category>
        
          <category> DB Engineering </category>
        
      </categories>
      <tags>
        
          <tag> Transaction </tag>
        
          <tag> Database </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Isolation 고립성]]></title>
      <url>/database/db%20engineering/transaction/2024/06/15/Isolation/</url>
      <content type="text"><![CDATA[Read Phenomena여러 트랜잭션이 동시에 실행된다면 데이터를 읽어들일 때마다 값이 변할까?변화된 값을 읽는게 좋을까 아니면 일관된 값을 읽어들이는 것이 좋을까?사실, 정답은 없지만 일관된 값을 읽어들이는 것이 더 바람직하다.트랜잭션이 동시에 수행되면서 여러 가지 읽기 현상들이 존재하는데 이것을,Read Phenomena 라고 한다.여러 트랜잭션이 수행되더라도 예상되고 바람직한 결과를 얻기 위해서 트랜잭션 간 고립성(Isolation)이 필요한 것이다.Dirty ReadsDirty 의 의미는 데이터가 완전히 flush 되지 않았거나 완전히 commit 되지 않았다는 것을 의미한다. 즉, 다른 트랜잭션이 쓴 내용을 읽지만 실제로 아직 커밋되지 않은 것을 읽는 현상을 말한다.Non-repeatable Reads트랜잭션 내에서 어떤 데이터를 읽은 이후, 동일한 자원을 다시 읽으려고 할 때 그 값이 변경되어져 있는 현상을 말한다.Phantom Reads범위의 해당하는 값을 읽어들이는 쿼리가 있을 때, 첫 번째 범위 쿼리로부터 얻었던 결과에 없었던 값이 두 번째 범위 쿼리에는 새로운 행이 삽입되어 처음에는 없던 값을 읽는 현상을 말한다. 즉, 다른 트랜잭션의 쓰기 작업으로 인해 처음에 읽지 않았던 것을 두 번째 쿼리에서 읽게 된 것을 말한다.Lost Updates한 트랜잭션 내에서 수정한 데이터 값을 커밋하기 전에 다른 트랜잭션이 커밋 전의 데이터를 읽는 현상을 말한다. 결국 커밋 이후 시점에서 보면 다른 트랜잭션은 과거의 데이터를 읽어버린 꼴이다.Dirty Reads 예시Dirty Reads 예시  TX1 Read =&gt; (Product1, 50), (Product2, 80)  TX2 Update =&gt; Product1이 5개가 더 팔려서 QNT 5개를 추가하였다.(10 -&gt; 15)  TX1 Read =&gt; TX2의 커밋되지 않은 값을 읽어서 155 합의 결과를 도출했다. (일관성이 있으려면 사실 130이라는 합이 나왔어야 했다.)  TX2 Rollback =&gt; TX2의 롤백으로 인해 결국 TX1이 읽은 155값(Dirty Value)에 대한 원인도 사라진 셈이다.Non-repeatable Read 예시Non-repeatable Read 예시  TX1 Read =&gt; (Product1, 50), (Product2, 80)  TX2 Update =&gt; Product1이 5개가 더 팔려서 QNT 5개를 추가하였다.(10 -&gt; 15)  TX2 COMMIT  TX1 Read =&gt; TX2 커밋 이후 read 했기 때문에 Dirty Value를 읽어들인 것은 아니다. 하지만 여전히 일관성이 깨진 155이라는 합을 내놓고 있다.  비즈니스 요구사항과 성능 사이의 trade-off 를 고려했을 때, Non-repeatable read를 허용하더라도 문제가 생기지 않는 상황이라면 Non-repeatable read를 허용할 수도 있다.Phantom Read 예시Phantom Read(유령 읽기)는 범위 쿼리에서 발생한다. 특히 where 절이 존재할 경우 유령읽기가 발생하지 않을 수 있는 예시를 생각해 내기 어려울 정도로 흔하게 발생 가능한 현상이다.Phantom Read 예시  TX1 Read =&gt; (Product1, 50), (Product2, 80)  TX2 Insert =&gt; (Product3, 10개, 1$) 새로운 상품 삽입  TX2 COMMIT  TX1 Read =&gt; 새로 삽입된 행까지 조회하기 때문에 일관성이 깨진 $140의 값을 도출한다.  Non-repeatable Read VS Phantom Read  반복되지 않는 읽기는 기존에 읽었던 데이터가 그 다음에 읽을 때 일관성이 깨지는 경우이고, 유령 읽기는 기존에 없던 데이터가 그 다음에 읽을 때 새로 생기는 데이터 때문에 일관성이 깨지는 경우이다.Lost Updates 예시Lost Update 예시  두 개의 트랜잭션 TX1, TX2가 거의 동시에 BEGIN하여 둘 다 Product1의 QNT를 10으로 바라본다.  TX1 Update =&gt; Product1의 QNT에 10을 더한다. (10 -&gt; 20)  TX2 Update =&gt; Product2의 QNT에 5를 더한다. (10 -&gt; 15)  TX2 COMMIT =&gt; TX1에서 업데이트한 값이 TX2이 업데이트한 값으로 덮어쓰여졌다.  TX1 Read =&gt; TX1이 업데이트한 값을 토대로 $180 값을 얻어야 하지만 TX2의 덮어쓰기로 인해 TX1의 업데이트 값을 잃어버리고 일관성이 깨진 $155 값을 얻게 되었다.일관성(Consistence)에 도달하기 위해한 트랜잭션 내에서 읽는(read) 데이터들의 값들은 일관성(consistence)이 유지되길 기대한다. 다시 말해, 한 트랜잭션 내에서 특정 데이터에 대해 100번의 조회를 하더라도 100번 전부 같은 값을 얻을 수 있어야 한다는 것이다. 쉽게 생각하면 다른 트랜잭션의 영향없이 본인 트랜잭션 내에서만 데이터 정보가 일관되어야 한다고 볼 수 있다. 하지만 실제 시스템은 여러 트랜잭션이 동시에 실행된다. 여기서 우리는 일관성을 얻기 위해 고립이 필요한 것이다.Isolation Levels  Isolation Level(격리 수준)을 구현하는 방식은 각 DBMS마다 다르다.성능과 데이터의 일관성, 둘 사이의 trade-off를 고려하여 고립 수준을 정하면 된다.Read Uncommitted  고립이 전혀 없는 단계이다. 커밋 여부에 상관없이 다른 트랜잭션에 의해 발생된 변화가 즉각적으로 보여진다.  Dirty read가 발생하기 때문에 일관성이 깨진 값을 조회할 수 있다.  트랜잭션의 처리 속도는 가장 빠르다. 데이터를 처리할 때 트랜잭션 간 어떠한 제약도 없기 때문이다.Read Committed  한 트랜잭션 안의 각 쿼리들은 다른 트랜잭션이 커밋 완료된 변화들만 볼 수 있다.  커밋된 변경사항만 읽지만, Non-Repeatable Read 와 같은 일관성이 깨질 수도 있다.Repeatable Read  트랜잭션이 실행되는 동안 쿼리가 행을 읽을 때, 해당 행이 변경되지 않도록 트랜잭션이 보장한다.  반복되지 않는 읽기(Non-repeatable Read)를 해결하기 위해 고안된 격리 수준이다.  동일한 트랜잭션 내에서는 값을 몇 번이나 읽든 항상 같은 값을 읽게 된다.Snapshot  트랜잭션의 각 쿼리는 트랜잭션 시작 시점까지 커밋된 변경 사항만 볼 수 있다. 이것은 그 순간의 데이터베이스 스냅샷 버전의 데이터를 읽는 것과 같다.  PostgreSQL의 Repeatable Read는 Snapshot 격리 수준으로 동작한다.  트랜잭션이 시작하면 타임 스탬프로 버전을 표시한 스냅샷을 생성하고 트랜잭션이 실행중에 항상 해당 버전 스냅샷의 데이터들을 읽게 된다. 이로써 유령 읽기를 해결할 수 있다.Serializable  트랜잭션은 하나씩 차례대로 직렬화된 것처럼 실행된다.  모든 Read Phenomena 현상들이 사라진다.  속도가 가장 느린 격리 수준이다.Snapshot vs SerializableSnapshot Isolation트랜잭션이 시작될 때 데이터베이스의 스냅샷(데이터 읽기 전용 복사본)을 생성한다. 트랜잭션 동안 모든 읽기 작업은 이 스냅샷을 기준으로 수행되며, 트랜잭션이 수행되는 동안 다른 트랜잭션의 변경사항을 보지 않는다.특징  Non-blocking reads: 읽기 작업은 다른 틀내잭션의 쓰기 작업을 차단하지 않는다.  Non-blocking writes: 쓰기 작업은 다른 트랜잭션의 읽기 작업을 차단하지 않는다.  Write Skew: 두 트랜잭션이 동시에 같은 데이터를 업데이트하려 할 때 충돌이 발생할 수 있으며, 이는 쓰기 스큐(write skew) 현상으로 이어질 수 있다. 이는 데이터 불일치 문제를 유발할 수 있다.사용 사례많은 읽기 작업이 필요하지만 약간의 쓰기 스큐를 허용할 수 있는 시스템에서 사용된다.Serializable Isolation트랜잭션이 직렬적으로(즉, 순차적으로) 실행된 것처럼 보이도록 보장한다. 이는 트랜잭션 간의 완전한 격리를 보장하며, 한 트랜잭션이 다른 트랜잭션의 중간 상태를 보지 않도록 한다.특징  Strict isolation: 가장 높은 수준의 격리를 제공하며, 트랜잭션 간의 상호 간섭을 방지한다.  Locking: 일반적으로 잠금(Locking)을 사용하여 동시성을 제공한다. 이로 인해 잠금 경합(lock contention)이 발생할 수 있으며, 이는 성능 저하로 이어질 수 있다.  No anomalies: 읽기 스큐, 유령 읽기, 반복되지 않는 읽기와 같은 이상 현상을 방지 한다.사용 사례데이터 일관성이 매우 중요하고 트랜잭션 간의 충돌을 허용할 수 없는 시스템에서 사용된다.주요 차이점  동시성: Snapshot 격리 수준은 높은 동시성을 제공하지만, 쓰기 스큐와 같은 데이터 불일치 문제를 유발할 수 있다. Serializable 격리 수준은 동시성을 제한하지만, 데이터 일관성을 반드시 보장한다.  잠금: Serializable 격리 수준은 잠금을 사용하여 트랜잭션 간의 간섭을 방지하는 반면, Snapshot 격리 수준은 스냅샷을 사용하여 동일한 데이터를 읽는 동안에도 트랜잭션이 서로 간섭하지 않도록 한다.  성능: Snapshot 격리 수준은 일반적으로 성능이 더 우수하며, 특히 읽기 작업이 많은 환경에서 더욱 효과적이다. 반면, Serializable 격리 수준은 잠금 경합으로 인해 성능이 저하될 수 있다.요약Snapshot 격리 수준은 높은 동시성을 제공하지만 일부 데이터 불일치 문제를 허용할 수 있는 환경에서 유용하며, Serializable 격리 수준은 데이터 일관성이 최우선인 환경에서 사용된다.Isolation Levels vs Read Phenomena아래 이미지는 격리 수준에 따른 Read Phenomena 발생 여부를 나타낸 표이다.  Snapshot level도 위 4가지 Read Phenomena 현상을 예방한다.Database Implementation Of Isolation각 DBMS는 격리 수준을 서로 다른 방식으로 구현한다.데이터베이스 격리 수준을 구현하는 방식은 낙관적(Optimistic) 접근법과 비관적(Pessimistic) 접근법으로 나뉜다. 두 접근법은 트랜잭션 간의 동시성을 제어하는 두 가지 상반된 전략으로 구분된다.비관적(Pessimistic) 접근법트랜잭션이 충돌한 가능성이 있다고 가정하여, 데이터에 접근할 때마다 잠금을 걸어 충돌을 방지한다.비관적(Pessimistic) 구현 방식은 동시성을 제어하는 것인데, 구체적으로 잠금(Lock)을 사용하는 방법이다.row levle locks, table locks, page locks 등을 사용하여 Lost Updates를 예방할 수 있다.락을 사용하는 것은 비용이 많이 들 뿐더러 다른 트랜잭션들이 대기 상태에 놓일 수 있기 때문에 성능 이슈를 야기할 수 있다.특징  Lock-based: 데이터에 접근하는 시점에서 읽기 또는 쓰기 잠금을 설정하여 다른 트랜잭션이 동시에 동일한 데이터에 접근하지 못하게 한다.  충돌 방지: 잠금을 통해 트랜잭션 간의 충돌을 미리 방지하며, 충돌이 발생할 가능성이 낮다.  성능: 잠금으로 인한 오버헤드와 잠금 경합(lock contention)으로 인해 성능이 저하될 수 있다. 쓰기 작업이 많고 충돌 가능성이 높은 환경에서 적합하다.  장점: 트랜잭션 간의 충돌을 효과적으로 방지할 수 있으며, 데이터 일관성을 보장한다.  단점: 잠금으로 인한 교착 상태가 발생할 수 있으며, 높은 동시성을 제공하기 어렵다.사용 사례  은행 거래 시스템낙관적(Optimistic) 접근법트랜잭션이 충돌하지 않을 것이라고 가정하여, 주로 트랜잭션 종료 시점에 충돌 검사를 수행한다.낙관적(Optimistic) 구현 방식은 락을 사용하지 않기 때문에 다른 트랜잭션들이 대기하지 않으므로 비관적 방식에 비해 성능 부분의 이점이 있다. 많이 선호되는 방식이며 NoSQL 데이터베이스에서 선호되는 방식이다.변경 사항들을 메모리에 유지했다가 트랜잭션 간 충돌이 발생하면 트랜잭션을 롤백시킨다. 이렇게 충돌하여 발생한 오류를 직렬화 오류라고 부른다.특징  Lock-free: 데이터에 대한 접근 시점에서 잠금을 걸지 않으며, 대신 트랜잭션이 커밋되기 전까지 변경 사항을 추적한다.  충돌 검사: 트랜잭션이 커밋될 때, 다른 트랜잭션이 동일한 데이터를 수정했는지 확인하고 충돌이 발생할 경우 트랜잭션을 롤백시킨다.  성능: 읽기 작업이 많고 쓰기 작업이 상대적으로 적은 시스템에서 성능이 뛰어나다. 충돌이 적은 환경에서 특히 유리하다.  장점: 높은 동시성을 제공하며, 잠금으로 인한 교착 상태(deadlock)가 발생하지 않는다.  단점: 충돌이 빈번하게 발생하는 경우 롤백이 자주 일어나며, 이로 인해 오버헤드가 증가할 수 있다.사용 사례  온라인 협업 도우 (예: Google Docs)주요 차이점  잠금 방식          낙관적 접근법: 잠금을 사용하지 않으며, 트랜잭션 종료 시점에 충돌을 검사한다.      비관적 접근법: 데이터 접근 시점에 잠금을 설정하여 충돌을 방지한다.        충돌 처리          낙관적 접근법: 트랜잭션 종료 시점에 충돌을 검사하고, 충돌이 발생하면 롤백한다.      비관적 접근법: 잠금을 통해 충돌을 미리 방지한다.        동시성          낙관적 접근법: 높은 동시성을 제공하며, 읽기 작업이 많은 환경에 적합하다.      비관적 접근법: 잠금으로 인해 동시성이 낮아질 수 있으며, 쓰기 작업이 많은 환경에 적합하다.        성능          낙관적 접근법: 충돌이 적은 경우 성능이 뛰어나며, 교착 상태가 발생하지 않는다.      비관적 접근법: 잠금 경합으로 인해 성능이 저하될 수 있으며, 교착 상태가 발생할 수 있다.      요약각 데이터베이스 시스템은 특정 격리 수준을 구현하는 방식에 따라 비관적 또는 낙관적 접근법을 사용할 수 있으며, 이는 시스템의 동시성 요구 사항과 성능 특성에 따라 다르다.낙관적 접근법은 충돌이 드문 환경에서 높은 성능과 동시성을 제공하는 반면,비관적 접근법은 데이터 일관성이 매우 중요하고 충돌 가능성이 높은 환경에서 안정성을 제공한다.애플리케이션의 특성에 따라 적절한 접근법을 선택하는 것이 중요하다.]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> DB Engineering </category>
        
          <category> Transaction </category>
        
      </categories>
      <tags>
        
          <tag> Transaction </tag>
        
          <tag> Database </tag>
        
          <tag> Isolation </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Atomicity 원자성]]></title>
      <url>/database/db%20engineering/transaction/2024/06/15/Atomicity/</url>
      <content type="text"><![CDATA[정의트랜잭션 내 모든 쿼리는 성공해야 한다는 규칙이다.어떠한 이유로든 단 하나의 쿼리라도 실패한다면 롤백되어야 한다. 이것이 원자성의 규칙이다.마찬가지로, 트랜잭션이 실행 도중 데이터베이스가 중단되더라도 다시 시작할 때 롤백되어야 한다.원자성은 일관된 데이터베이스 상태를 위해 반드시 필요하다.]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> DB Engineering </category>
        
          <category> Transaction </category>
        
      </categories>
      <tags>
        
          <tag> Actomicity </tag>
        
          <tag> Database </tag>
        
          <tag> Transaction </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[성적 관리 모델링 연습 (2)]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/06/10/grades-manage2/</url>
      <content type="text"><![CDATA[스토어드 프로시저 생성문 예시CREATE PROCEDURE SP_Insert_SchoolClass	-- ADD the parameters for the stored procedure here	&lt;@Param1, sysname, @p1&gt; &lt;Detatype_For_Param1, , int&gt; = &lt;Defualt_Value_For_Param1, , 0&gt;,	&lt;@Param2, sysname, @p2&gt; &lt;Detatype_For_Param2, , int&gt; = &lt;Defualt_Value_For_Param2, , 0&gt;ASBEGIN	-- SET NOCOUNT ON added to prevent extra result sets from	-- interfering with SELECT statements	SET NOCOUNT ON;		-- INSERT statements for procedure here	SELECT &lt;@Param1, sysname, @p1&gt;, &lt;@Parama2, sysname, @p2&gt;END반 테이블의 저장 프로시저 생성문CREATE PROCEDURE SP_Insert_SchoolClass	@SchoolGradeId tinyint,	@SchoolClassId tinyint,	@SchoolClassName varchar(50)ASBEGIN	INSERT INTO TB_SchoolClass (SchoolGradeId, SchoolClassId, SchoolClassName)	VALUES (@SchoolGradeId, @SchoolClassId, @SchoolClassName)END스토어드 프로시저 실행exec SP_Insert_SchoolClass 1, 1, '1반'; -- ()괄호 필요없음exec SP_Insert_SchoolCalss 1, 2, '2반';SELECT * FROM TB_SchoolClass;                   SchoolGradeId      SchoolClassId      SchoolClassName                  1      1      1      1반              2      1      2      2반      반 테이블의 업데이트 프로시저  SP_Update_SchoolClassName  SP_Update_UseFlag반 테이블의 조회 프로시저  SP_SchoolClass_GetById(gradeId, classId);  SP_SchoolClass_GetAll();  SP_SchoolClass_GetByGrade(gradeId);학년-반-학생 관계도  신입생의 경우 아직 반 배정을 받지 못했지만 학생 테이블에 등록해야 되는 경우를 위해서 SchoolClassId 와 StudentSeqNo는 NULL 허용을 해준다.  SchoolGradeId , SchoolClassId , SchoolSeqNo 를 묶어서 후보키로 설정하여 NULL은 허용하지만 UNIQUE 를 만족시키기 때문에 반과 반별 학생 번호의 중복 생성 체계가 가능해진다.  학생 테이블에 entrance_date 입학날짜와 graduate_date 졸업날짜를 속성으로 설정 가능하다. 입학날짜가 없는 학생은 없으므로 NOT NULL 이지만, 아직 졸업하지 못한 학생들은 졸업날짜가 NULL이다. 만약 졸업날짜에 날짜 데이터가 들어가 있으면 졸업생인 것을 인지할 수 있다.  반 테이블의 UseFlag 는 현재 년도에 해당 반을 사용하지 않는다는 정보를 파악하기 위한 속성이다. 사용하지 않는다고 해당 반 테이블을 삭제할 수는 없다. 해당 반에 배정되어 있던 학생들이 부모가 없는 자식이 되기 때문에, 삭제 되신 UseFlag 의 업데이트문을 사용하여 반의 사용여부를 결정하는 것이 좋다.]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[성적 관리 모델링 연습 (1)]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/05/27/grades-manage1/</url>
      <content type="text"><![CDATA[  중학교 학생들의 성적을 관리하는 시스템의 DB를 설계 해보자!성적 관리 요구 사항  중학교(남녀공학) 학생 성적관리 프로젝트  과목은 학년별로 담당 선생님이 따로 있다.  시험은 중간고사, 기말고사 두 가지가 있다.  학생들은 학년, 반에 배정되며 반별로 학생들에게 고유 번호를 부여하고 있다.  각 반에는 담임 선생님이 배정되어 있다. 선생님 중에는 담임을 맡지 않는 선생님도 있다.  한 반의 학생은 대략 40명 정도이고 남녀 공학이다.  석차는 남녀 공통 1등부터 순서대로 정한다.마스터 테이블  학생(반당 40명정도, 반별 고유 번호 존재), 학년, 반  과목(학년별 담당 선생님 별도로 존재), 선생님(담임/비담임)  성적(반 석차/ 학년 석차) ⇒ 시험에 의한 객체 생성  시험(중간고사, 기말고사)관계 테이블  시험(보다), 배정(하다), 개강(하다)  만약 실무에서 시나리오에 모르는 단어가 있는 경우, 고객에게 그 개념을 물어봐서 확실히 개념을 잡고 설계한다.논리적 설계온전한 명사 선별하기가장 먼저 해야할 것은 마스터 테이블로 존재할 수 있는 객체들을 선별하는 것이다.종속성, 수식어 등을 파악하여 마스터 테이블의 객체를 정리해나간다. 예를 들어, 학생과 고유 번호가 있을 때 고유 번호는 학생에 종속되므로 학생 객체의 속성으로 들어가는 것이 맞다. 또한, 담임과 선생님을 생각해보면, 담임은 선생님의 수식어이기 때문에 선생님 객체의 속성으로 들어가는 것이 맞다. 이렇게 종속성 또는 수식어 등을 파악하여 명사로 온전히 존재할 수 있는 객체를 가려낼 수 있다. 만약 속성으로 들어갔던 컬럼이 알고보니 복잡도가 높다고 판단되면 마스터 테이블로 분리해내면 된다.  더 이상 분열할 수 없을 만큼 작은 단위로 객체를 마스터 테이블로 만들면 된다. 그렇다고 마스터가 아닌 것을 마스터로 만들면 굉장히 어색해진다.1:M 관계 찾기그 다음 해야할 것은 마스터 테이블 간의 1:M(부자지간) 관계가 있는지 살펴보는 것이다.  1:M 관계는 직관적으로 보이기 마련이다.데이터베이스 스키마 설계는 많은 경험치가 필요한 일이다. 처음부터 잘 못하더라도 괜찮다.기준 테이블자식의 포지션이 되지 않는 테이블을 절대 마스터 테이블이라고 하고 다른 말로 기준 테이블 이라고 한다.기준 테이블에서 오류가 발생하면 그 이하 연결된 자식 테이블에 모두 문제가 발생한다. 기준 테이블같은 경우에는 한 번 생성되면 거의 변경이 없는 성격을 가진다. 그렇기 때문에 DBA 입장에서 가장 신경 써야 하는 것은 기준 테이블에 오류가 있느냐를 파악해야 한다. 예제에서 기준 테이블은 학년으로 볼 수 있다.  기준 테이블의 경우 한 번 생성되면 그 이후에는 변경될 일이 거의 없다. 따라서 생성된 이후에는  INSERT , DELETE , UPDATE 를 막아버리는 것이 좋다. 즉, SELECT 만 열어줘서 read-only 로 설정하는 것이 좋다.고정 관념 버리기데이터 타입을 정할 때 주의해야 할 점은 고정 관념에 빠져서 잘못된 데이터 타입을 설정해주는 것이다.예를 들어, 반이름의 경우 보통 1반, 2반, … 이런 식일텐데, 개나리반, 무궁화반 처럼 이럴수도 있다. 이럴때 varchar의 글자수를 전자에 맞췄을 경우 글자수 부족 문제가 발생할 수도 있다. 따라서, 고정 관념에 빠져서 데이터 타입을 설정하는 것에 주의해야 한다. 이런 것은 확장성을 고려하지 않은 설계 방식이다.코드 레벨에서 할 일확장성을 고려한 설계 이후에는 개발 단계에서 요구 사항에 맞도록 검증 로직을 추가하면 된다. 예를 들어, 반의 개수는 최대 12반 까지 있어야 한다면, tinyint로 충분하다. 그리고 코드 레벨에서 1반 부터 12반까지만 입력되도록 검증 로직을 추가하면 된다. 즉, 입력되는 값에 대한 컨트롤은 설계 단계에서 할 필요가 없다.ERD 그리기학년과 반 1:M 관계  반ID가 학년별로 1반, 2반 … 체계를 갖기 위해 상속형 PK를 사용했다.앞서 말했듯이 학년 기준 테이블은 거의 변경될 일이 없으므로 미리 값을 넣어놔서 DML을 사용하지 않도록 한다.INSERT INTO 학년 VALUES (1, '1학년'), (2, '2학년'), (3, '3학년');SELECT * FROM 학년;학년 테이블                   학년ID      학년명                  1      1      1학년              2      2      2학년              3      3      3학년      반면에  반 테이블은 값이 바뀔 수 있는 있기 때문에 DML을 허용해야 한다. 이처럼 비즈니스 상황에 따라 또는 고객의 요구사항에 따라 DML을 허용해야 할 지 파악하고 데이터를 미리 넣을지 말지 결정하면 된다.학년, 반, 학생 객체학년, 반, 학생 객체들에 대해 PK와 최소한의 속성으로만 ERD를 구성했다.  PK에 어떤 의미를 부여하는 행동은 지양해야 한다. 확장성과 유연성을 망가뜨리는 행위이다. PK는 유니크하고 낫널이면 충분하다.  예약어와 같은 혼란을 빚을만한 단어로는 테이블명으로 사용하지 않는다. (ex. 학급, 반 → class 사용 금지)  각 테이블명들은 일관성있게 작성하는 것이 좋다. (ex. 학년 → school_grade, 반 → school_class)  참고로 작명할 때 테이블은 접두어로 TB, 뷰는 VW, 스토어드 프로시저는 SP 가 붙는 경우도 있다. 이름만 보고 테이블인지 뷰인지 스토어드 프로시저인지 바로 파악하기 편리하다.]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[도서대출관리 모델링 연습 (1)]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/05/13/books-loan-management/</url>
      <content type="text"><![CDATA[  도서대출관리 모델링을 통해 1:M, M:N 관계 설계를 연습해보자.도서관의 도서 대출 관리 요구 사항  도서관에는 각 서고/서가에 많은 책들이 있다  고객들은 인터넷을 통해서 로그인한 후 도서 목록을 조회할 수 있다  고객들은 원하는 책을 대출 받을 수 있다.  고객은 책이 있을 경우 대출 예약을 할 수 있으며 대출을 위해서는 직접 방문해서 책을 찾아서 대출을 해야 한다마스터 테이블  서고, 서가, 책, 고객, 도서 목록관계 테이블  로그인, 조회, 대출, 대출 예약설계 순서논리적 설계 → 물리적 설계  초기에는 1:M 관계를 찾는 것에 집중하자. 마스터 테이블 간 어떤 비즈니스 행위가 존재하는지 확신할 수 없는 상황에서 M:N 을 따져봤자 헛수고이다.논리적 설계  한글로 작성하여 가독성을 높인다.  쓸데없는 컬럼 들어가지 않는다.  PK, FK 및 없어서는 안되는 속성 정도로만 구성한다.물리적 설계  영어로 실제 컬럼 이름을 짓는다.  데이터 형식을 제대로 잡아준다.  NOT NULL, UNIQUE 등 제약을 설정해준다.  구체적인 속성들을 작성해준다.서고와 서가PK를 어떻게 잡는지에 따라 서가ID의 생성체계가 달라진다. 서가2는 서고ID가 null 허용이다.서가1  서고를 반드시 입력해야 하는 경우 상속형 PK를 사용한다.(에러가 없지만, 융통성이 없다.)  서가 ID가 각 서고별로 1번부터 시작한다.서가2  서가를 먼저 등록해야되는 경우에는 독립형 PK를 설정해줘야 한다.(융통성은 있지만 에러가 있을 수 있다. 나중에 업데이트를 잊는 경우 문제가 발생할 수 있다. 가비지 데이터가 된다.)  서고에 상관없이 서가ID는 계속 증가한다.  DMBS 가 보기에는 독립형 PK를 사용하는 서가2의 FK가 NULL인 데이터는 전혀 문제가 없다. 하지만 사람들의 비즈니스 로직으로 봤을 때는 서고 ID(FK)가 NULL인 것은 오류로 해석되는 것이다.상속형 PK를 사용하는 것이 안전하니 상속형 PK를 사용한다고 가정해보자.서가와 책 1:M 관계책과 목차 1:M 관계  상속형 PK 테이블의 특징  손자격인 책 테이블은 서가의 PK를 FK로 물려받는다. 즉, 서가의 PK는 복합키(서고ID + 서가ID)이므로 책의 FK로 그대로 계승된다.상속형 PK는 안전하지만 그 만큼 융통성이 없다고 볼 수 있다.서가 - 책 - 목차를 살펴보면, 복합키가 계속 눈덩이처럼 커져서 계승되고 있는 것을 볼 수 있다.즉, 상속형 PK는 계승될 수록 PK에 참여하는 컬럼 개수가 늘어난다는 단점이 있다.(책 - 대출 - 고객) M:N 관계 테이블  책Seq(AK, Alternate Key)를 선언해줘서 책 조회와 대출 테이블의 FK로 편리하게 활용 가능하다. AK를 사용하지 않는다면 조회시 PK(책, 서고, 서가 ID)를 모두 작성해야 한다. 마찬가지로 대체키가 없다면 대출 테이블에도 전부 작성해야 한다.  대출일(REG_DATE)은 관계 테이블의 필수 요소이다. 고객이 어떤 책을 언제 빌렸다. 를 대출 테이블이 표현하고 있고, 이 한 문장을 표현할 수 있도록 책Seq , 고객ID , 대출일 을 복합키로 잡았다.1:1 관계를 완성하기 전 초안 ERD책과 도서목록은 1:1 관계이다.1:1 관계를 완성해야지 설계가 최종적으로 완성이 된다.1:1 관계 없이는 설계 완성이 안 될 수도 있다.1:1 관계는 깊이를 요구하는 고난이도 기술이 요구 되기도 한다.]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[RDB M:N 관계]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/05/12/M-N-relation/</url>
      <content type="text"><![CDATA[  관계형 데이터베이스에서 M:N 관계에 대해 알아보자!M:N 관계M:N 관계는 한 쪽에서 보면 1:M 관계로 보인다. 반대 편에서도 마찬가지로 1:M 관계로 보여진다.M:N 관계 예제학생과 과목이라는 테이블이 있을 때 학생이 여러 개 과목을 수강하는 상황을 가정해보자.학생 입장에서는 여러 개 과목을 수강할 수 있고, 과목 입장에서는 여러 학생이 해당 과목을 선택할 수 있다. 양쪽에서 모두 1:M 관계로 볼 수 있으므로 M:N 관계가 성립된다.개념상으로는 M:N이 가능하다.아래 이미지는 학생과 과목이 M:N으로 관계를 맺고 있다.학생이 여러 과목을 수강 가능하다.M:N 관계를 테이블로 표현 시 문제점🚨 M:N 관계를 풀어서 학생 테이블과 과목 테이블로 표현해보면 PK 중복 문제가 발생한다.기존 두 테이블로는 M:N 관계 표현 불가능M:N 관계는 양쪽에서 PK 중복 문제가 발생하기 때문에 기존 두 개의 관계형 테이블로는 표현할 방법이 없다.  💡 관계라는 것은 테이블의 일반 속성끼리 연결해서 관계를 맺는 것이 아니라, PK와 FK 간의 연결을 통해 관계를 맺는 것이다. 즉, 관계를 위해선 FK 가 필요하다.  💡 M:N 관계에 참여하는 두 객체의 테이블끼리는 선천적으로 관계가 없다. 각 테이블은 스스로 존재한다. 그런데 이들 사이에 어떤 관계를 맺어줌으로써 M:N 관계를 테이블로 표현할 수 있게 된다.어떻게 M:N 관계를 테이블로 표현할 것인가?결국 M:N 관계를 테이블로 표현하려면 또 하나의 테이블이 필요하다.학생과 과목의 M:N 관계를 표현하기 위해 수강 테이블이 등장학생과 과목은 각자 스스로 존재하는 엔티티(개념, 테이블)이다. 그러면 수강 테이블은 스스로 존재하는 테이블일까?NO. 수강 테이블은 학생과 과목 사이에서 행위(비즈니스 로직)를 정의하기 위해 등장한 테이블이다.⭐️ M:N 관계는 비즈니스 관계이다.비즈니스 관계라는 것은 나와 상대방 간의 어떤 계약 관계를 맺어서 움직인다는 것이다.즉, 주어, 목적어, 동사가 포함되는 비즈니스 행위가 일어나서 M:N 관계를 형성하는 것이다.예를 들어, 학생(주어)이 과목(목적어)을 수강(동사)합니다. 라는 요구사항 또는 비즈니스 행위가 존재할 때 M:N 관계를 학생, 과목, 수강 테이블로 표현 가능해지는 것이다.  M:N 관계에서는 테이블을 읽는 방법을 배우는 것이 매우 중요하다.학생과 과목은 관계가 없다.선천적으로 스스로 존재하는 학생과 과목에는 사실 어떠한 관계도 없다. 하지만 이 객체들이 관계를 맺는 순간이 온다.언제???비즈니스 로직(학생이 과목을 수강합니다.)이 들어오는 순간 학생과 과목 사이의 비즈니스 행위로 인해 관계가 맺어지는 것이다.  ⭐️ M:N 관계라는 것은 어떤 행위가 있을 적에, 즉 비즈니스 로직이 나타날 때 스스로 존재하던 객체 간의 관계가 맺어지는 것이다.기존 객체끼리 M:N 관계를 억지스럽게 테이블로 표현하려다 보니 PK 중복 문제가 발생했었다. 이것은 두 객체간의 비즈니스 행위를 정의하는 어떤 테이블이 생략됐기 때문에 발생한 문제였던 것이다.Master Table명사로서 스스로 존재할 수 있는 객체(테이블)들을 Master Table이라고 한다.  ex: 학생, 과목, 고객, 상품Relation Table동사로서 비즈니스 행위를 정의하여 M:N 관계를 표현하는 테이블을 Relation Table이라고 한다. 참고로 관계(Relation) 테이블이 명사로서 마스터 테이블로 해석될 때도 존재한다.(관계 테이블의 PK가 또 다른 관계 테이블의 FK로 참조되는 경우이다.)  ex: 수강, 주문  💡 REG_DATE  참고로 비즈니스 행위를 정의하는 관계(Relation) 테이블은 항상 날짜 컬럼(REG_DATE)이 필수 요소이다. 누가 무엇을 언제 ~를 했다. 를 표현할 때 언제를 날짜 컬럼이 맡게 된다.M:N은 결국 쌍방 1:M 관계로 해석된다.  학생 - 수강 테이블 간의 관계는 1:M 관계이다.  과목 - 수강 테이블 간의 관계는 1:M 관계이다.결국 두 개의 관계를 합쳐서 M:N 관계로 해석이 되는 것이다.M:N 관계로 해석하기 전에 살펴봐야 하는 것  각 Master 테이블 간 쌍방으로 1:M 관계가 없는지 살펴봐야 한다.  1:M 관계가 없다면 두 Master 테이블은 M:N 관계인 것이다.M:N 관계 테이블의 기본키 체계M:N 관계 테이블의 PK는 어떻게 잡아야 할까?결과부터 말하자면, 비즈니스 특징에 따라 다르다.독립형 PK  FK 의 중복을 허용해야 하는 비즈니스 상황에서는 독립형 PK를 사용한다.고객은 동일한 상품을 여러 번 구매할 수 있다.중복을 허용하는 상황에서는 PK를 독립적으로 설정해주자.상속형 PK  FK의 중복을 허용하지 않는 비즈니스 상황에서는 상속형 PK를 사용한다.학생은 동일한 과목을 중복으로 수강할 수 없다.중복을 허용하지 않는 상황에서는 FK를 묶어서 복합키를 PK로 설정해주자.  💡 설계 시 주의 사항  하나의 테이블은 하나의 객체나 사물을 모델링 하는 것이다. 두 개 이상의 객체를 하나의 테이블로 모델링 하는 순간 잘못된 설계를 하는 것이다.만약 관계 테이블에 참여하고 있는 마스터 테이블이 많다면???하나의 레코드를 식별하기 위해 WHERE 절에 엄청 많은 FK 를 조건으로 적용해야지 식별할 수 있을 것이다.SELECT *FROM 수강WHERE 학생ID='1' and 과목ID='c' and f3='2' and f4 ='k'and ...;실무에서는 관계 테이블을 형성하고 있는 마스터 테이블이 단순히 2개인 경우만 있는 것이 아니라 무수히 많을 수도 있다.이런 경우 WHERE절에서 간단히 조회하기 위해 보조키(대체키, alternate key)를 선언해줘서 하나의 레코드를 식별하는 데 편함을 얻을 수 있다. 단, 상속형 PK 를 기본키로 사용하여 FK의 중복을 허용하지 않는다는 것을 전제로 해야 한다.seq 라는 보조키를 선언해줬다.SELECT *FROM 수강WHERE seq=5;보조키(대체키)는 PK의 특징을 모두 만족하지만 PK는 아니다.UNIQUE 하면서 NOT NULL을 모두 만족하지만 PK로 등록이 안된 키이다.보조키는 관계 테이블의 조건절을 간단하게 만들어 조회를 편리하게 한다.]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
          <tag> Relation </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[RDB 1:M 관계 (3)]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/05/11/1-M-relation(3)_recursive/</url>
      <content type="text"><![CDATA[  1:M 관계 중 특이한 경우인 재귀적 관계에 대해 알아보자.1:M 재귀적 관계1:M 재귀적 관계란 무엇일까?회사와 부서가 존재할 때, 부서 아래 하위 부서가 존재하는 경우 1:M 재귀적 관계로 볼 수 있다. 상위 부서와 하위 부서도 1:M 관계로 볼 수 있다.1:M 재귀적 관계의 또 다른 예로 디렉토리가 있다. 1:M 관계가 반복적으로 이어지는 관계이다.아래 이미지는 회사와 상위 부서, 하위 부서의 관계도를 나타낸 이미지이다.회사, 상위 부서, 하위 부서1:M 재귀적 관계를 테이블로 어떻게 구현할까?재귀적 관계에 놓인 테이블을(부서) 중심으로 상위, 하위 관계를 살펴보자.우선, 재귀적 관계 테이블을 아래와 같이 설계했다고 가정해보자.초기 재귀적 관계 테이블                   열 이름      데이터 타입      NULL 허용                  PK      부서 id      int      x                     부서명      varchar(50)      o              FK      하위 부서 ID      int      o              FK      상위 부서 ID      int      o      이때, 위 테이블의 상위 부서 ID와 하위 부서 ID가 테이블에 포함되어도 문제가 없을지 고민해보자.하위 부서의 경우  하위 부서의 경우 M의 위치에 있기 때문에 재귀적 관계 테이블에 포함시킬 수 없다. 그 이유는 하위 부서 ID(FK)를 포함시키면 부서 ID(자기 자신, PK)의 중복이 발생하기 때문이다.  또한 하위 ID 여러 개를 하나의 행으로 저장할 수 없다. 각 속성은 원자값을 가져야 한다는 DB 설계 대원칙을 지켜야 하기 때문이다.상위 부서의 경우  최상위 부서의 경우 상위 부서 ID는 1(자기 자신)의 위치에 있다. 따라서 문제 없이 테이블에 포함시킬 수 있다. 즉, 자기 자신을 참조하는 것이다.(self join)  1:M 재귀적 관계에 놓인 중간 위치의 테이블이라 하더라도 상위 부서(부모)는 단 하나이기 때문에, 재귀적 관계 테이블에 포함시켜도 아무 이상 없다.최종 재귀적 관계 테이블재귀적 관계 테이블의 상하 관계를 따져서 문제가 없도로 수정한 테이블은 다음과 같다.                   열 이름      데이터 타입      NULL 허용                  PK      부서 id      int      x                     부서명      varchar(50)      o              FK      상위 부서 ID      int      o        ⭐️ SELF JOIN  1:M 재귀적 관계처럼 트리 구조를 갖는 테이블 스키마는 반드시 SELF JOIN 처럼 PK와 FK 모두 본인이 갖는, 즉 자기 자신을 참조하는 스키마로 설계해야 한다.상위 부서 ID에는 NULL이 들어올 수 있을까?가능하다.예를 들어, FK(상위 부서 ID)가 없는 경우에는 재귀적 관계에서 최상위 위치로 볼 수 있다.즉, 최상위 부서의 경우에는 상위 부서 ID가 NULL 이 된다.INSERT INTO 부서1 VALUES (1, '총무부', NULL);INSERT INTO 부서1 VALUES (2, '총무1과', 1);INSERT INTO 부서1 VALUES (3, '총무2과', 1);SELECT * FROM 부서1;최상위 부서인 총무부 아래에 총무1과와 총무2과가 존재한다고 했을 때 아래와 같은 재귀적 관계 테이블이 생성된다.                   부서ID      부서명      상위부서ID                  1      1      총무부      NULL              2      2      총무1과      1              3      3      총무2과      1      1:M 재귀적 관계(트리 구조)를 SQL문의 셀프 조인으로 보면 아래와 같다.SELECT * FROM 부서1 aJOIN 부서1 b on a.부서ID = b.상위부서ID                   부서ID      부서명      상위부서ID      부서ID      부서명      상위부서ID                  1      1      총무부      NULL      2      총무1과      1              2      1      총무부      NULL      3      총무2과      1      여기서 총무1과에 또 재귀적으로 총무1팀, 총무2팀이 있다고 가정해보자.INSERT INTO 부서1 VALUES (4, '총무1팀', 2);INSERT INTO 부서1 VALUES (5, '총무2팀', 2);마찬가지로 셀프조인하여 트리 구조로 살펴보면 아래와 같다.                   부서ID      부서명      상위부서ID      부서ID      부서명      상위부서ID                  1      1      총무부      NULL      2      총무1과      1              2      1      총무부      NULL      3      총무2과      1              3      2      총무1과      1      4      총무1팀      2              4      2      총무1과      1      5      총무2팀      2              5      3      총무2과      1      NULL      NULL      NULL              6      4      총무1팀      2      NULL      NULL      NULL              7      5      총무2팀      2      NULL      NULL      NULL      위 셀프 조인한 테이블을 보고 다음과 같은 사실을 알 수 있다.왼쪽 테이블의 상위 부서 ID가 NULL 이면 최상위 부서이다. 반대로 오른쪽 테이블의 부서 ID가 NULL인 경우 최하위 부서이다.  최상위 부서: 총무부  최하위 부서: 총무2과, 총무1팀, 총무2팀  💡 자기 자신이 부모 테이블이자, 자식 테이블인 것을 1:M 재귀적 관계로 본다.만약 총무1과의 관계도(트리 구조)를 보고 싶다면?셀프 조인에서 총무1과의 부서 ID를 WHERE절에 적용하여 조회하면 된다.SELECT * FROM 부서1 aJOIN 부서1 b on a.부서ID = b.상위부서IDWHERE a.부서ID = 2;                   부서ID      부서명      상위부서ID      부서ID      부서명      상위부서ID                  1      2      총무1과      1      4      총무1팀      2              2      2      총무1과      1      5      총무2팀      2      회사와의 관계도 추가하여 마무리.회사 테이블의 회사 ID(PK)를 부서 재귀적 테이블의 FK로 추가하면 된다.최종적으로 재귀적 관계(상위 부서 - 하위 부서)를 포함한 회사 - 부서 간의 1:M 관계 완성이다.]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
          <tag> Relation </tag>
        
          <tag> Recursion </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[RDB 1:M 관계 (2)]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/05/11/1-M-relation(2)/</url>
      <content type="text"><![CDATA[  회사와 부서 간의 관계를 고려하여 1:M 관계 모델링 연습을 해보자.테이블 설계 시 숙지 사항예제에 들어가기에 앞서, 테이블 설계 시 고려하면 좋을 것들을 숙지하고 가자.      테이블 스키마를 설계할 때는 key들을(기본키, 외래키) 먼저 고려하고 key를 중심으로 어떻게 설계해나갈지 계획을 세운다.        초기 설계 시에는 속성들은 간단히 잡아가는 것이 좋다. 처음부터 모든 속성들을 작성하려면 정신 사납다. 테이블 설계의 본질은 key들의(기본키, 외래키) 관계를 잘 설계 해나가는 것이다.        고객으로부터 얻은 데이터들은 key로 사용하지 않는 것이 좋다. 고객으로부터 얻은 데이터들은 부재 또는 변화의 가능성이 존재하기 때문에 PK에 포함시켜 의미를 부여하는 것은 위험할 수 있다.        숫자 타입으로 인조 식별자를 사용하는 경우 자동 증분(AUTO INCREMENT)을 사용하는 것이 편리하고 안전하다. 단, 데이터 발생 규모를 파악하여 데이터 타입을 정하자.  1:M 모델링 예제 (회사 &amp; 부서)회사와 부서 간의 테이블을 설계하기 전에 1:M 설계 과정을 살펴보자.일단, 각 테이블의 스키마를 작성하고 PK를 정해보자.회사 테이블company_id를 PK로 잡았다. 초기에 속성은 간단하게 company_name만 작성했다.                   열 이름      데이터 형식      NULL 허용      AUTO INCREMENT                  PK      company_id      int      x      o                     company_name      varchar(50)      x      x      부서 테이블department_id를 PK로 잡았다. 마찬가지로 속성을 department_name만 간단하게 잡았다.                   열 이름      데이터 형식      NULL 허용      AUTO INCREMENT                  PK      department_id      smallint      x      o                     department_name      varchar(50)      x      x      PK 결정이 끝났다면 그 이후 두 테이블 간의 관계를 보는 것이다.  ⭐️ 관계를 볼 때는 각 테이블의 입장에서 관계를 고려해야 한다.  회사 입장에서 부서와의 관계, 부서 입장에서 회사와의 관계를 모두 살펴본 다음 1:1 인지 1:M 인지, M:N 인지 결정하는 것이다. 따라서 양쪽에서 살펴봤을 때 반드시 동일한 관계 양상이 나와야 한다.회사와 부서의 관계는?양쪽 입장에서 살펴봤을 때 회사와 부서의 관계는 1(회사):M(부서) 관계이다.즉, 1(회사)의 PK가 M(부서)의 FK로 나타나야 한다.                   열 이름      데이터 형식      NULL 허용      AUTO INCREMENT                  PK, FK      company_id      int      x      x              PK      department_id      smallint      x      x                     department_name      varchar(50)      x      x      이렇게 PK, FK 등을 설정하는 과정이 끝나면 테이블 스키마의 주된 골격을 설계했다고 볼 수 있다.이 다음 과정이 회사, 부서의 나머지 컬럼(속성)들을 조사 및 분석하여 채워나가는 것이다.회사 ID와 부서 ID를 묶어서 복합키를 PK로 잡으면 어떤 차이가 있을까?결론부터 말하자면, 부서 ID(department_id) 생성 체계가 달라진다. 복합키를 PK로 잡는다면, 각 회사마다 부서 ID를 1번으로 시작할 수 있다. 복합키가 아닌 부서 ID만을 PK로 잡는다면, 자동 증분을 이용해서 PK 값이 계속 증가하는 수 밖에 없다. 따라서 회사별로 동일한 부서에 대해서 동일한 부서 ID를 부여하고 싶으면 묶어서 PK를 만들면 된다. 즉, 복합키를 PK(기본키)로 잡는 것은 회사-부서 관계에 있어서 의미부여 효과가 생긴다.1:M 관계에서 어떤 테이블의 데이터가 먼저 들어가야할까?항상 반드시 부모가 먼저 삽입되어야 한다.insert into 회사 values('삼성전자');insert into 부서 values(1, 1, '총무부');]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
          <tag> Relation </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[RDB 1:M 관계 (1)]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/05/11/1-M-relation(1)/</url>
      <content type="text"><![CDATA[  관계형 데이터베이스에서 1:M 관계에 대해 알아보자.1:M 관계는 부자지간 관계이다.1:M 관계를 부자지간으로 볼 수 있다. 가장 중요하고 근본적인 관계이다. 가장 흔하게 나타나는 일반적인 관계이기도 하다.  파일(트리) 구조도 1:M 관계이다.부모와 자식간 1:M 관계를 맺고 있다.❓만약 자식과 손자라는 테이블이 또 다시 1:M 관계를 맺는다고 했을 때, 부모와 손자간의 관계가 존재할까???답은, NO 이다.  ✅ 관계는 3개 테이블 간의 관계를 표현하지 않는다. 반드시 2개 테이블 간의 관계에만 관심이 있다.PK &amp; FK  부모 테이블의 PK가 자식 테이블에서 표현되는 것을 FK(foreign key)라고 한다.  자식 테이블의 FK는 중복될 수 있지만 부모 테이블의 PK는 반드시 하나이다.  🚨 권고사항  PK 명칭과 FK 명칭은 일치하도록 사용하는 것이 좋다.⭐️ 관계형 데이터베이스(RDB)는 부모 없는 자식을 막아준다.부모 없는 자식을 막아준다는 것이 무슨 뜻일까?RDBMS는 부모 없는 자식의 관계를 허용하지 않는다. 따라서 다음과 같은 제약이 존재한다.  어떤 부모의 PK가 자식 테이블의 FK로 사용될 때, 해당 부모의 데이터를 DELETE 할 수 없다. 해당 부모를 삭제하면 해당 부모의 PK를 FK로 들고 있는 자식이 고아가 되기 때문이다.  부모 없는 자식은 INSERT 할 수 없다. 즉, FK가 걸려있는 테이블에 데이터를 삽입할 때는 FK를 반드시 설정해줘야지만 INSERT 할 수 있다.❓만약 부모 없는 자식 데이터가 생기면 무슨 일이 벌어질까.부모 없는 자식 데이터는 가비지 데이터(Garbage Data)가 된다. outer join으로 조회할 때 가비지 데이터가 드러나게 된다. 가비지 데이터가 생기면 COUNT 함수와 같은 집계 함수 결과에 치명적인 오차를 발생시킨다.  ✅ 프로그래밍으로 가비지 데이터를 커버치는 행위는 쓰레기 코드를 만드는 것이다. 따라서, DB 스키마 설계 시 관계를 잘 설정해서 가비지 데이터가 생기지 않도록 하는 것이 매우 중요하다.1:M 관계 PK, FK 설정 예제학년과 반이라는 테이블들을 설계해보자. 학년과 반은 1:M 관계를 지닌다.학년 테이블학년 번호를 PK로 설정했다.                   열이름      데이터 타입      NULL 허용                  PK      학년번호      tinyint      x                     학년이름      varchar(50)      o      insert into 학년 values(1, '1학년');insert into 학년 values(2, '2학년');insert into 학년 values(3, '3학년');select * from 학년;                   학년번호      학년이름                  1      1      1학년              2      2      2학년              3      3      3학년      반 테이블현재 PK만 설정되어 있다.  💡 학년번호와 반번호를 함께 복합키로 설정한 이유는 학년별로 반 번호를 1반 부터 시작하도록 하기 위함이다. 만약, 반 테이블의 PK로 반 번호만을 설정하면 반 번호의 중복이 불가능하기 때문에 학년 별로 1반부터 시작하지 못한다. 이처럼 복합키를 이용하면 두 번째 필드(반 번호)의 중복을 허용할 수 있게 된다.                   열이름      데이터 타입      NULL 허용                  PK      학년번호      tinyint      x              PK      반번호      tinyint      x                     반이름      varchar(50)      o      insert into 반 values(1, 1, '1반');insert into 반 values(4, 1, '1반');select * from 반;두 번째 행(row) 데이터는 부모가  없다.                   학년번호      반번호      반이름                  1      1      1      1반              2      4      1      1반      외래 키 설정이 없기 때문에 부모 없는 자식 데이터(가비지 데이터)가 삽입되어 버렸다.가비지 데이터를 우선 삭제해주자.delete from 반 where 학년번호 = 4;학년 테이블의 학년번호를 반 테이블의 학년번호의 외래키로 설정해준다.                   열이름      데이터 타입      NULL 허용                  PK, FK      학년번호      tinyint      x              PK      반번호      tinyint      x                     반이름      varchar(50)      o      다시 INSERT 구문을 실행해보면insert into 반 values(1, 1, '1반');insert into 반 values(4, 1, '1반');insert into 반 values(3, 1, '3반');select * from 반;  ✅ FOREIGN KEY 제약 조건에 위배되는 데이터는 INSERT 되지 않는다. 즉, FK가 설정된 테이블은 해당 FK 값이 부모 테이블의 PK 값으로 존재하는지 먼저 탐색한다. 그러고나서 부모(PK)가 정상적으로 있는 경우에만 INSERT가 수행된다.                   학년번호      반번호      반이름                  1      1      1      1반              2      3      1      1반      학년과 반 테이블을 조인하여 한 번에 조회해보자.select * from 학년 ajoin 반 b on a.학년번호 = b.학년번호;                   학년번호      학년이름      학년번호      반번호      반이름                  1      1      1학년      1      1      1반              2      3      3학년      3      1      1반      요약 및 정리  1:M 관계는 부자지간 관계이다.  PK &amp; FK 설정이 제대로 되어 있다면 RDB는 부모 없는 자식 데이터 발생을 막아준다.  PK 설정 전략에 따라 특정 필드의 중복을 허용할 수도 있고 방지할 수도 있다.]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
          <tag> Relation </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[PK 설계 시 고려 사항]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/05/09/PK-design/</url>
      <content type="text"><![CDATA[  PK를 어떻게 선정하고 설계하는지 알아보자.1. UNIQUE &amp; NOT NULL 인 컬럼들을 찾는다.2. 후보 식별자가 없는 경우 임의의 식별자(인조 식별자)를 만들어 부여한다.  인조 식별자를 만들어서 사용하는 경우가 은근 많다.  인조 식별자를 PK로 사용하는 경우, year + month + sequence number 처럼 날짜를 식별자에 부여하여 데이터를 파악하기 쉽게 할 수 있다. 시퀀스 넘버는 데이터 발생 횟수(순서)로 보면 된다.  year, month, day 등 의미를 부여하는 정도는 비즈니스 상황에 따라 결정된다.3. PK의 데이터 타입 결정  레코드의 발생 가능한 최대 수를 예측한다.          ex) 데이터가 1년에 몇 개 정도 발생하는가?      ex) 데이터가 한 달에 몇 개 정도 발생하는가?        발생 가능한 최대 레코드 수를 커버할 수 있는 데이터 타입을 선정한다.          ex) 만일 레코드가 수십 만개 정도라면 999,999로 커버 가능하다. int 또는 varchar(6)를 사용하여 처리 가능하다.      4. PK 데이터 타입 후보를 대상으로 다음을 고려한다.  int 등 숫자 타입을 PK로 채택할 경우 자동 증분(AUTO INCREMENT) 속성을 사용하면 편리하다.  String 을 PK로 채택할 경우 숫자가 아닌 문자를 섞어서 PK 값에 의미를 부여할 수 있다.  특별한 의미를 부여할 필요가 없을 경우 숫자 타입을 사용하는 것이 바람직하다.  PK는 고유 식별자 기능으로 충분하다.          PK 값에 어떤 의미를 부여하는 것은 부담이 될 수 있다.      ]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
          <tag> Primary Key </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[주식별자와 후보식별자]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/05/06/pk-candidate/</url>
      <content type="text"><![CDATA[주식별자 vs 후보식별자주식별자와 후보식별자 모두 유일성 과 최소성 특징을 만족한다. 둘 중 NOT NULL을 만족시키면서 적절한 키를 주식별자로 정하게 된다. 주식별자 외 식별자들은 후보식별자가 된다. 후보식별자들은 NULL 을 허용할 수 있다는 것이 주식별자와의 차이점이다.  참고  후보키의 인덱스 생성 시 물리적으로 인스턴스의 유일성을 보장해주기 위해 UNIQUE 인덱스를 생성하는 것이 바람직하다. 후보키들의 인덱스 테이블을 별도로 만들어 두면 특정 레코드에 빨리 접근하는 데 도움이 될 수 있다.어떤 것을 주식별자로 정할 것인지?중요하면서도 어려운 것이 주식별자를 선정하는 일이다. 주식별자는 테이블 설계 시 명시적으로 DB에 선언해줘야 한다. 주식별자를 선언해주면 대응하는 인덱스 테이블이 자동으로 생성된다.주식별자를 어떻게 선정하는지 아래 테이블 스키마 예제를 통해 살펴보자.            주민 번호      고객명      폰번호      이메일      집주소      고객번호                  000000-xxxxxxx      홍길동      010-xxxx-oooo      email@gmail.com      서울시 강남구 111-1      12345      1. 위 필드 중 UNIQUE 특징을 만족하는 필드는???  주민 번호  폰 번호  이메일  고객 번호(=회사에서 지정한 고객 식별 번호)  UNIQUE 속성을 만족하지 않으면 주식별자의 자격이 없다.고객명은 동명이인이 존재할 수 있고, 집 주소는 한 집에 여럿이 사는 경우 중복이 발생할 수 있다. 따라서 고객명과 집 주소는 UNIQUE 속성을 만족하지 않는다.2. 1번 을 만족하면서, NOT NULL 을 만족하는 필드는?  주민 번호  고객 번호  NOT NULL 속성을 만족하지 않으면 주식별자의 자격이 없다.핸드폰이 없는 사람이 존재할 수도 있고, 이메일이 없는 사람도 존재할 수 있기 때문에 NOT NULL 을 만족하지 않을 있다.결국 주식별자로 채택될 만한 필드는 주민 번호, 고객 번호가 있다.그럼 둘 중에 어떤 것이 주식별자로 채택하기 적합할까???주민 번호 와 고객 번호 의 데이터가 어떤 측면에서 제공되는지 살펴봐야 한다.주민 번호는 고객 쪽에서 제공하는 데이터이고, 고객 번호는 DB를 관리하는 주체(회사)에서 제공하는 데이터다.  일반적으로 고객들이 제공하는 데이터들은 주식별자로 채택 하지 않는다.  고객쪽에서 제공하는 데이터는 100% 확률로 안전하다고 볼 수 없다. 반면에 회사측에서 제공 또는 부여하는 데이터는 100% 확률로 보장할 수 있기 때문에 고객쪽 데이터는 주식별자로 채택하지 않는 것이 좋다.주민번호 경우에도 100% 확률로 확인할 수 있다는 보장이 없다. 따라서, 고객 번호를 주식별자로 정하는 것이 적합하다.결정자 vs 종속자방금 살펴폰 테이블 스키마를 가지고 결정자와 종속자를 구분해보자.  결정자(Determinant attribute): 이 값을 알면 나머지 속성 값도 알 수 있는(정해지는) 속성이다.          고객 번호        종속자(Dependent attribute): 결정자에 의해 정해지는 속성이다.          주민 번호      고객명      폰 번호      이메일      집 주소      Functional Dependency(함수 종속성)두 개의 애트리뷰트 집합 간의 제약의 일종이다. 애트리뷰트 X의 값 각각에 대해 시간에 관계없이 항상 애트리뷰트 Y의 값이 오직 하나만 연관되어 있을 때 Y는 X에 함수 종속이라 하고, X -&gt; Y라고 표기한다.  고객 번호 -&gt; 고객명  주민 번호 -&gt; 고객명  폰 번호 -&gt; 고객명  이메일 -&gt; 고객명]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
          <tag> Primary Key </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[데이터 타입]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/05/06/dataType/</url>
      <content type="text"><![CDATA[  실무에서 자주 사용되는 데이터 타입에 대해 알아보자!정수  Tinyint: 소형 테이블의 PK 타입으로 자주 사용된다.  Bigint: 대략 920경 정도이다.실수  🚨 위 부동소수점 타입들은 소수점 표현 시 근사치를 나타내게 된다. 표현 가능한 소숫점 자릿수가 제한되기 때문이다.돈 계산과 같이 정확한 소수점 표현이 필요하다면 decimal 타입을 사용하자.decimal 타입은 4~16byte로 비교적 메모리를 많이 사용하긴 하지만, 소수점을 정확히 표현하기 때문에, 돈 계산과 같이 정확한 수치를 다뤄야 하는 경우에는 실수 타입으로 deciaml 사용을 권고하고 있다. numeric 이라는 타입도 존재하는데, deciaml 하고 똑같다고 보면된다. 그러니깐 decimal 을 그냥 사용하자.문자  char: 고정길이 타입이다. char(10) = 'ab'와 같이 2글자만 할당해도 10byte 모두 사용한다.  varchar: 가변길이 타입이다. varchar(50) = 'ab' 와 같이 2글자만 할당하면 2byte만 사용하여 메모리를 효율적으로 사용한다. 따라서 varchar를 보통 많이 사용한다. 보통 50 정도로 할당해준다.  Text: 가변길이 타입이다. varchar(max)와 같다고 보면된다. 엄청 큰 내용들의 문자들을 집어넣을 때 사용한다.  🚨 주의  varchar(max)와 Text 타입은 크기가 매우 클 수 있기 때문에 PK로 정하거나, 인덱스 생성하거나 정렬 등을 할 수 없게 되므로 주의하자. 매우 큰 문자열을 가지고 인덱싱, 정렬 등을 하는 것은 성능 상 매우 안 좋기 때문이다.유니코드(unicode)문자 타입에 접두사로 n만 붙여주면 유니코드 문자데이터로 인코딩된다.날짜, 시간, 화폐  날짜와 시간을 표현할 때는 표현하려는 시간 범위와 메모리량을 고려하여 datetime 과 smalldatetime 둘 중에 하나를 알맞게 사용하자.  타임존을 고려한 글로벌한 서비스의 경우 timestamp 타입을 사용하는 것이 권장되고 있다.  화폐 단위는 사용하지 말자. 대신 decimal을 사용하자.unique identifierNEWID() 함수는 GUID를 생성해준다. 이것을 사용할 바에 차라리 범위가 매우 큰 bigint를 사용하자.DECLARE @myid uniqueidentifierSET @myid = NEWID()PRINT 'Value of @myid is: ' + CONVERT(varchar(255), @myid)INSERT INTO TEST(UNIQUEID) VALUES (NEWID())SELECT * FROM TEST  GUID ???  SQL Server에서 NEWID() 함수는 새로운 GUID(전역 고유 식별자)를 생성해준다. GUID는 전 세계적으로 고유한 값을 나타내는 128비트 숫자이다.]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
          <tag> Data Type </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[관계형 데이터베이스 소개]]></title>
      <url>/database/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%20%EC%A4%91%EA%B8%89(modeling)/2024/05/04/IntroRDBMS/</url>
      <content type="text"><![CDATA[  이교준 강사님의 데이터베이스 강의를 듣고 관계형 데이터베이스에 관해 간략히 정리하고 가자.데이터베이스란?  구조화된 데이터들의 집합이다.관계형 데이터베이스  데이터들을 2차원 배열과 같은 테이블에 저장하고 관리하는 데이터베이스.관계 정의  1:M 관계 (부자지간 관계)  N:M 관계 (비즈니스 관계)  1:1 관계 (부부 관계)앞으로 위 세 개의 관계에 대해서도 정리할 예정이다. 관계는 테이블 설계시 매우 중요하다.주식별자(Primary Key, 기본키)  하나의 레코드를 고유하게 구분할 수 있는 것으로 하나의 컬럼 또는 여러 개의 컬럼이 모여 Primary Key를 구성한다.주식별자 특징  NOT NULL  UNIQUE (유일성)  최소한의 속성 조합으로 이뤄진다(최소성)  다양한 종류의 무결성을 설정하고 강화하는 것을 도와준다.  테이블 관계를 설정하도록 해준다.  관계에 반드시 PK 가 포함된다.테이블 설계 시 고려해야 할 문제 상황들예를 들어, 학생 수강신청 관리 테이블 설계를 만든다고 가정해보자.아래 이미지처럼 테이블 초안을 작성했다고 해보자.    학생 수강신청 관리 테이블 예시이때, 주의해야 할 점들이 무엇이 있을까?테이블 변경 시  만일 같은 과목을 담당하는 선생님들이 두 명 이상으로 늘어날 경우 대처 방법데이터 오류 시  과목명을 오기했을 경우 대처 방법  학생명을 오기했을 경우 대처 방법  한 학생이 동일한 과목을 두 번 이상 수강신청 한 경우 대처 방법  한 과목도 신청하지 않은 학생을 찾아내는 방법  동명 이인이 존재하는 경우 해결 방법  한 과목이 폐강되었을 때 대처 방법  위 문제 상황들을 고려하지 않고 테이블을 설계한다면, 문제 상황이 발생했을 때 테이블의 한계에 부딪힐 것이다. 결국 테이블 구조 수정을 할 수 밖에 없게된다. 테이블의 구조체가 변경된다면 해당 테이블을 이용하고 있는 프로그램도 다시 코딩해야하는 불상사가 발생하게 된다.]]></content>
      <categories>
        
          <category> Database </category>
        
          <category> 데이터베이스 중급(Modeling) </category>
        
      </categories>
      <tags>
        
          <tag> Database </tag>
        
          <tag> RDBMS </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[김영한 자바 입문 메서드]]></title>
      <url>/java/%EA%B9%80%EC%98%81%ED%95%9C%20%EC%9E%90%EB%B0%94%20%EC%9E%85%EB%AC%B8/2024/05/01/Java_method/</url>
      <content type="text"><![CDATA[  자바 메서드에 관해 알아보자.메서드 정의public static int add(int a, int b) {  // 메서드 본문, 실행 코드}제어자 반환타입 메서드이름(매개변수 목록) {  메서드 본문}자바에서는 함수를 메서드(Method)라 한다. 메서드도 함수의 한 종류라고 생각하면 된다. 지금은 둘을 구분하지 않고, 이정도만 알아두자.  메서드는 크게 메서드 선언과 메서드 본문으로 나눌 수 있다.메서드 선언(Method Declaration)public static int add(int a, int b) 부분에서 반환타입, 메서드 이름, 매개변수 목록들을 메서드 선언부분이라 한다. 메서드 선언 정보를 통해 다른 곳에서 해당 메서드를 호출할 수 있다.메서드 본문(Method Body){  int sum = a + b;  return sum;}  메서드가 수행해야 하는 코드블록이다.  메서드 본문은 블랙박스이다. 메서드를 호출하는 곳에서는 메서드 선언은 알지만 본문은 모른다.  메서드 실행 결과를 반환하려면 return문을 사용해야 한다.메서드 호출과 용어 정리메서드를 호출할 때는 메서드에 넘기는 매개변수(파라미터)의 타입과 순서, 개수도 일치해야 한다.호출: call("hello", 20);메서드 정의: int call(String str, int age)인수(argument)여기서 "hello", 20처럼 넘기는 값을 영어로 Argument라고 한다. 실무에서는 아규먼트, 인수, 인자라는 용어를 모두 사용한다.매개변수(parameter)메서드를 정의할 때 선언한 변수인 String str, int age를 매개변수 또는 파라미터라 한다. 메서드를 호출할 때 인수를 넘기면, 그 인수가 매개변수에 대입된다.  IntelliJ 단축키 tip  메서드명에 커서를 올려놓고 cmd + B를 입력하면 해당 메서드 선언 부분으로 이동한다.메서드 호출과 값 전달  ⭐️ 자바는 원시형과 참조형 관계없이 항상 변수의 값을 복사해서 대입한다.이 대원칙은 반드시 숙지해야 복잡한 상황에도 코드를 단순하게 이해할 수 있다.public class MethodValue1 {    public static void main(String[] args) {        int number = 5;        System.out.println("1. changeNumber 호출 전, number: " + number);        changeNumber(number);        System.out.println("4. changeNumber 호출 후, number: " + number);    }    public static void changeNumber(int number) {        System.out.println("2. changeNumber 변경 전, number: " + number);        number = number * 2;        System.out.println("3. changeNumber 변경 후, number: " + number);    }}1. changeNumber 호출 전, number: 52. changeNumber 변경 전, number: 53. changeNumber 변경 후, number: 104. changeNumber 호출 후, number: 5main() 내부의 number변수의 값을 읽고 복사해서 changeNumber 메서드의 매개변수에 전달해준다. 복사해서 전달해주었기 때문에, changeNumber 내부의 number변화는 main() 내부의 number 에 영향을 주지 않는다. 즉, 각 메서드 안에서 사용하는 number변수는 서로 완전히 분리된 다른 변수이다. 이름이 같아도 완전히 다른 변수다.메서드 형변환메서드를 호출할 때 인수와 매개변수 사이에서 명시적 형변환 또는 자동 형변환이 일어난다.명시적 형변환큰 범위에서 작은 범위로 인수를 전달하여 메서드를 호출할 때는 명시적 형변환이 필요하다. 그렇지 않으면 컴파일 오류가 발생한다.public class MethodCasting1 {    public static void main(String[] args) {        double number = 1.5;        //printNumber(number); //double을 int에 대입하므로 컴파일 오류        printNumber((int) number); //명시적 형변환을 사용해 double을 int로 변환    }    public static void printNumber(int n) {        System.out.println("숫자: " + n); // 숫자: 1    }}자동 형변환작은 범위에서 큰 범위로 인수를 전달하여 메서드를 호출할 때는 자동 형변환이 일어난다. 단, 타입이 달라도 자동 형변환이 가능한 경우에 호출할 수 있다.public class MethodCasting2 {    public static void main(String[] args) {        int number = 100;        printNumber(number);    }    public static void printNumber(double n) {        System.out.println("숫자: " + n); // 숫자: 100.0    }}메서드 오버로딩  메서드 오버로딩이란?  같은 이름의 메서드를 여러 개 정의하되 매개변수의 유형, 개수 또는 순서가 다른 경우를 가리킨다.public class Calculator {    public int add(int a, int b) {        return a + b;    }    public double add(double a, double b) {        return a + b;    }    public int add(int a, int b, int c) {        return a + b + c;    }}  참고로 반환 타입은 오버로딩 규칙으로 인정하지 않는다.오버로딩 실패int add(int a, int b)double add(int a, int b)메서드 시그니처(method signature)메서드 시그니처 = 메서드 이름 + 매개변수 타입(순서)메서든 시그니처는 자바에서 메서드를 구분할 수 있는 고유한 식별자나 서명을 뜻한다. 메서드 시그니처는 메서드의 이름과 매개변수 타입(순서 포함)으로 구성되어 있다.자바 입장에서 각각의 메서드를 고유하게 식별할 수 있어야 어떤 메서드를 호출할 지 결정할 수 있다.  먼저 전달된 인수 타입에 최대한 맞는 메서드를 찾아서 실행하고, 그래도 없으면 형 변환 가능한 타입의 메서드를 찾아서 실행한다.]]></content>
      <categories>
        
          <category> Java </category>
        
          <category> 김영한 자바 입문 </category>
        
      </categories>
      <tags>
        
          <tag> Java </tag>
        
          <tag> Method </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[김영한 자바 입문 배열 - 2차원]]></title>
      <url>/java/%EA%B9%80%EC%98%81%ED%95%9C%20%EC%9E%90%EB%B0%94%20%EC%9E%85%EB%AC%B8/2024/04/28/Java_array_2D/</url>
      <content type="text"><![CDATA[2차원 배열 선언, 생성 그리고 사용2차원 배열은 int[][] arr = new int[2][3]와 같이 선언하고 생성한다. 그리고 arr[1][2]와 같이 사용하는데, 먼저 행(row) 번호를 찾고, 그 다음 열(column) 변호를 찾으면 된다.  arr[행][열], arr[row][column]2차원 배열을 선언, 생성, 사용하는 예제 코드를 살펴보자.public class ArrayDi0 {    public static void main(String[] args) {        // 2x3 2차원 배열을 만든다.        int[][] arr = new int[2][3]; //행2, 열3        arr[0][0] = 1; //0행, 0열        arr[0][1] = 2; //0행, 1열        arr[0][2] = 3; //0행, 2열        arr[1][0] = 4; //1행, 0열        arr[1][1] = 5; //1행, 1열        arr[1][2] = 6; //0행, 2열        //0행 출력        System.out.print(arr[0][0] + " "); //0열 출력        System.out.print(arr[0][1] + " "); //1열 출력        System.out.print(arr[0][2] + " "); //2열 출력        System.out.println();//한 행이 끝나면 라인을 변경한다.        //1행 출력        System.out.print(arr[1][0] + " "); //0열 출력        System.out.print(arr[1][1] + " "); //1열 출력        System.out.print(arr[1][2] + " "); //2열 출력        System.out.println();//한 행이 끝나면 라인을 변경한다.    }}다음은 중첩된 for문을 사용하여 첫 번째 for문은 row를 탐색하고, 두 번째 for문은 열을 탐색하여 작업을 수행하도록 리팩토링 해보자.public class ArrayDi2 {    public static void main(String[] args) {        // 2x3 2차원 배열을 만든다.        int[][] arr = new int[2][3]; //행2, 열3        arr[0][0] = 1; //0행, 0열        arr[0][1] = 2; //0행, 1열        arr[0][2] = 3; //0행, 2열        arr[1][0] = 4; //1행, 0열        arr[1][1] = 5; //1행, 1열        arr[1][2] = 6; //0행, 2열        for (int row = 0; row &lt; 2; row++) {            for (int column = 0; column &lt; 3; column++) {                System.out.print(arr[row][column] + " ");            }            System.out.println();//한 행이 끝나면 라인을 변경한다.        }    }}2차원 배열 간단한 생성과 초기화1차원 배열과 마찬가지로 {}를 사용하여 간단히 생성과 초기화를 할 수 있다.public class ArrayDi3 {    public static void main(String[] args) {        // 2x3 2차원 배열을 만든다.        int[][] arr = new int[][]{            {1,2,3},            {4,5,6}        }; //행2, 열3        for (int row = 0; row &lt; arr.length; row++) {            for (int column = 0; column &lt; arr[row].length; column++) {                System.out.print(arr[row][column] + " ");            }            System.out.println();//한 행이 끝나면 라인을 변경한다.        }    }}new int[][]를 생략하여 한 줄로 선언과 초기화를 할 수 있다.public class ArrayDi3 {    public static void main(String[] args) {        // 2x3 2차원 배열을 만든다.        int[][] arr = { // new int[][] 생략 가능            {1,2,3},            {4,5,6}        }; //행2, 열3        for (int row = 0; row &lt; arr.length; row++) {            for (int column = 0; column &lt; arr[row].length; column++) {                System.out.print(arr[row][column] + " ");            }            System.out.println();//한 행이 끝나면 라인을 변경한다.        }    }}  라인을 적절하게 넘겨주어 행과 열이 명확해지도록 간단히 초기화하는 것이 코드의 가독성을 높인다.  {  {1, 2, 3},  {4, 5, 6}}  2차원 배열 길이  arr.length는 행의 길이를 뜻한다.  arr[row].length는 열의 길이를 뜻한다.]]></content>
      <categories>
        
          <category> Java </category>
        
          <category> 김영한 자바 입문 </category>
        
      </categories>
      <tags>
        
          <tag> Java </tag>
        
          <tag> Array </tag>
        
          <tag> Data Structure </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[김영한 자바 입문 배열]]></title>
      <url>/java/%EA%B9%80%EC%98%81%ED%95%9C%20%EC%9E%90%EB%B0%94%20%EC%9E%85%EB%AC%B8/2024/04/28/Java_array_1D/</url>
      <content type="text"><![CDATA[배열이 필요한 이유같은 타입의 변수를 반복해서 선언하고 반복해서 사용하는 문제를 해결하기 위해 필요한 것이 배열이다.public class Array1 {   public static void main(String[] args) {       int student1 = 90;       int student2 = 80;       int student3 = 70;       int student4 = 60;       int student5 = 50;			 System.out.println("학생1 점수: " + student1); 			 System.out.println("학생2 점수: " + student2); 			 System.out.println("학생3 점수: " + student3); 			 System.out.println("학생4 점수: " + student4); 			 System.out.println("학생5 점수: " + student5);		}}배열의 선언과 생성  배열은 같은 타입의 변수를 사용하기 편하게 하나로 묶어둔 것이다.public class Array1Ref1 {    public static void main(String[] args) {        int[] students; //배열 변수 선언        students = new int[5]; //배열 생성        //변수 값 대입        students[0] = 90;        students[1] = 80;        students[2] = 70;        students[3] = 60;        students[4] = 50;        //변수 값 사용        System.out.println("학생1 점수: " + students[0]);        System.out.println("학생2 점수: " + students[1]);        System.out.println("학생3 점수: " + students[2]);        System.out.println("학생4 점수: " + students[3]);        System.out.println("학생5 점수: " + students[4]);    }}아주 간단해보이는 다음 두 줄은 아주 자세히 살펴봐야 한다.int[] students; //1. 배열 변수 선언 students = new int[5]; //2. 배열 생성배열 변수 선언  배열을 사용하려면 int[] students와 같이 배열 변수를 선언해야 한다.  일반적인 변수와 차이점은 int[] 처럼 타입 다음에 대괄호[]가 들어간다는 점이다.  배열 변수를 선언한다고 해서 사용할 수 있는 배열이 만들어진 것은 아니다!          int[] students와 같은 배열 변수에는 배열을 담을 수 있다.      배열 생성  배열을 사용하려면 배열을 생성해야 한다.  new int[5]라고 입력하면 총 5개의 int를 저장할 수 있는 배열 변수가 생성된다.배열과 초기화  new int[5]라고 하면 총 5개의 int형 변수가 만들어진다. 자바는 배열을 생성할 때 그 내부값을 자동을 초기화 한다.  숫자는 0, boolean은 false, String은 null로 초기화 된다.배열 참조값 보관  new int[5]로 배열을 생성하면 배열의 크기만큼 메모리를 확보한다.          int형 5개: 4byte * 5 =&gt; 20byte        배열을 생성하고 나면 자바는 메모리 상에서 해당 배열에 접근할 수 있는 참조값(주소)을 반환한다.  앞서 선언한 배열 변수인 int[] students에 배열의 참조값을 보관한다. 참조값을 통해 메모리에 있는 실제 배열에 접근하고 사용할 수 있다.int[] students = new int[5]; // 1. 배열 생성int[] students = x001; // 2. new int[5]의 결과로 x001 참조값 반환students = x001; // 3. 배열 별수에 참조값 보관배열 사용배열을 사용하려면 변수에 []를 붙이고 대괄호 안에 배열의 위치를 나타내는 숫자인 인덱스(index)를 넣어주면 된다.//변수 값 대입students[0] = 90; students[1] = 80;//변수 값 사용System.out.println("학생1 점수: " + students[0]);System.out.println("학생2 점수: " + students[1]);배열에 값 대입students[0] = 90; //1. 배열에 값을 대입x001[0] = 90; //2. 변수에 있는 참조값을 통해 실제 배열에 접근. 인덱스를 사용해서 해당 위치의 요소에접근, 값 대입배열 값 읽기//1. 변수 값 읽기System.out.println("학생1 점수: " + students[0]);//2. 변수에 있는 참조값을 통해 실제 배열에 접근. 인덱스를 사용해서 해당 위치의 요소에 접근System.out.println("학생1 점수: " + x001[0]);//3. 배열의 값을 읽어옴System.out.println("학생1 점수: " + 90);  배열을 사용하면 이렇게 참조값을 통해서 실제 배열에 접근하고 인덱스를 통해서 원하는 요소를 찾는다.참고  배열은 인덱스가 0부터 시작한다. 따라서 사용 가능한 인덱스 범위는 0 ~ (n-1)이 된다.  만약 인덱스 허용 범위를 넘어선다면 에러가 발생한다.Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: Index 5 outof bounds for length 5 at array.Array1Ref1.main(Array1Ref1.java:14)기본형 vs 참조형자바의 변수 데이터 타입은 크게 기본형과 참조형으로 분류할 수 있다.  기본형(Primitive Type): int, long, double, boolean처럼 변수에 사용할 값을 직접 넣을 수 있는 데이터 타입을 기본형 또는 원시형이라 한다.  참조형(Reference Type): int[] students와 같이 데이터에 접근하기 위한 참조(주소)를 저장하는 데이터 타입을 참조형이라 한다.참고기본형과 참조형은 각각 언제 사용할까?기본형은 모두 사이즈가 명확하게 정해져있다.int i; // 4bytelong l; // 8bytedouble d; // 8byte배열은 동적으로 사이즈를 변경할 수 있다.new int[size]; // size 값에 따라 배열의 크기가 정해진다.  기본형은 선언과 동시에 크기가 정해진다. 따라서 크기를 동적으로 바꿀 수 없다. 반면에 배열과 같은 참조형은 크기를 동적으로 할당할 수 있다.  기본형은 사용할 값을 직접 저장한다. 반면에 참조형은 메모리에 저장된 배열이나 객체의 참조를 저장한다. 이로 인해 참조형은 더 복잡한 데이터 구조를 만들고 관리할 수 있다. 반면 기본형은 더 빠르고 메모리를 효율적으로 처리한다.배열 리팩토링맨 처음 예제 코드를 다음과 같이 리팩토링 할 수 있다. 배열 생성 코드 new int[] 옆에 {}을 가지고 초기화를 해준다. 이때 배열의 크기는 작성하지 않는다.public class Array1Ref3 {    public static void main(String[] args) {        int[] students; //배열 변수 선언        students = new int[]{90, 80, 70, 60, 50}; //배열 생성과 초기화        //변수 값 사용        for (int i = 0; i &lt; students.length; i++) {            System.out.println("학생" + (i + 1) + " 점수: " + students[i]);        }    }}간단한 배열 생성  배열은 {}만 사용해서 생성과 동시에 초기화 하는 기능을 제공한다. 단, 이때는 배열 변수의 선언과 한 줄에 함께 사용할 때만 가능하다.int[] students = {90, 80, 70, 60, 50};오류int[] students;students = {90, 80, 70, 60, 50}; // 이렇게 사용하지 말자.간단한 배열 생성을 통해 리팩토링 해보자.public class Array1Ref4 {    public static void main(String[] args) {        int[] students = {90, 80, 70, 60, 50};        //변수 값 사용        for (int i = 0; i &lt; students.length; i++) {            System.out.println("학생" + (i + 1) + " 점수: " + students[i]);        }    }}배열을 사용한 덕분에 예제 코드를 전체적으로 잘 구조화 할 수 있었다.]]></content>
      <categories>
        
          <category> Java </category>
        
          <category> 김영한 자바 입문 </category>
        
      </categories>
      <tags>
        
          <tag> Java </tag>
        
          <tag> Array </tag>
        
          <tag> Data Structure </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[김영한 자바 입문 향상된 for문]]></title>
      <url>/java/%EA%B9%80%EC%98%81%ED%95%9C%20%EC%9E%90%EB%B0%94%20%EC%9E%85%EB%AC%B8/2024/04/28/Java_Enhanced_for/</url>
      <content type="text"><![CDATA[  앞서 반복문에서 설명하지 않은 내용인 향상된 for문(Enhanced For Loop)에 대해 알아보자. 향상된 for문을 살펴보기 전에 배열에 대한 이해가 우선되어야 한다.향상된 for문 또는 for-each문은 다음과 같이 정의하여 사용한다.// 향상된 for문 정의for (변수 : 배열 또는 컬렉션) {	// 배열 또는 컬렉션의 요소를 순회하면서 수행할 작업}다음 향상된 for문 사용 예제를 살펴보자.public class EnhancedFor1 {    public static void main(String[] args) {        int[] numbers = {1, 2, 3, 4, 5};        //일반 for문        for (int i = 0; i &lt; numbers.length; i++) {            int number = numbers[i];            System.out.println(number);        }        //향상된 for문, for-each문        for (int number : numbers) {            System.out.println(number);        }        //for-each문을 사용할 수 없는 경우, 증가하는 index 값 필요        for (int i = 0; i &lt; numbers.length; i++) {            System.out.println("number" + i + "번의 결과는: " + numbers[i]);        }    }}향상된 for문 장점실무에서 배열은 처음부터 끝까지 순서대로 읽어서 사용하는 경우가 많다. 그런데 일반 for문 사용 할 경우 배열의 값을 읽기 위해 int i 와 같은 인덱스 변수를 선언해야 한다. 그리고 i &lt; numbers.length 와 같이 배열의 끝 조건을 지정해주어야 한다. 개발자 입장에서는 그냥 배열을 순서대로 처음부터 끝까지 탐색하고 싶은데 너무 번잡한 일을 해주어야 한다. 그래서 향상된 for문이 등장했다.  향상된 for문은 배열의 인덱스를 사용하지 않고도 배열의 요소를 순회할 수 있기 때문에 코드가 간결하고 가독성이 좋다.향상된 for문을 사용하지 못하는 경우향상된 for문은 인덱스 값이 감추어져 있기 때문에 사용하지 못하는 경우가 있다. 즉, int i와 같은 인덱스 값을 직접 사용해야 하는 경우에는 향상된 for문을 사용할 수 없다.//for-each문을 사용할 수 없는 경우, 증가하는 index 값 필요for (int i = 0; i &lt; numbers.length; i++) {    System.out.println("number" + i + "번의 결과는: " + numbers[i]);}단축키 tip  iter: 직전에 선언된 배열 또는 컬렉션을 가지고 향상된 for문을 자동완성 시켜준다.]]></content>
      <categories>
        
          <category> Java </category>
        
          <category> 김영한 자바 입문 </category>
        
      </categories>
      <tags>
        
          <tag> Java </tag>
        
          <tag> Loop Statement </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[김영한 자바 입문 형변환]]></title>
      <url>/java/%EA%B9%80%EC%98%81%ED%95%9C%20%EC%9E%90%EB%B0%94%20%EC%9E%85%EB%AC%B8/2024/04/26/Java_casting/</url>
      <content type="text"><![CDATA[  Java 형변환이 언제 어떻게 발생하는지 알아보자.대입과 형변환  작은 범위에서 큰 범위로는 당연히 값을 넣을 수 있다.          int -&gt; long -&gt; double        큰 범위에서 작은 범위로는 다음과 같은 문제가 발생한다.          오버플로우      소수점 버림        Java는 작은 범위에서 큰 범위로 대입할 때 자동 형변환이 일어난다.자동 형변환작은 범위 숫자 타입에서 큰 범위 숫자 타입으로 대입할 때 개발자가 직접 형변환을 하지 않아도 자동으로 형변환이 일어난다. 이것을 자동 형변환 또는 묵시적 형변환이라 한다.int intValue = 10;double doubleValue = intValue;// double doubleValue = (double) intValue // 자동 형변환System.out.println(doubleValue); // 10.0명시적 형변환큰 범위에서 작은 범위로 대입할 때는 반드시 명시적 형변환이 필요하다.  🚨 주의  큰 범위에서 작은 범위로 대입할 경우 명시적 형변환을 하지 않으면 컴파일 오류가 발생한다. 컴파일 오류는 문제를 가장 빨리 발견할 수 있는 좋은 오류이다.java: incompatible types: possible lossy conversion from double to int 명시적 형변환은 변경하고 싶은 데이터 타입을 (int) 와 같이 괄호를 함께 사용하여 변수앞에 입력하면 된다. 영어로는 캐스팅(casting)이라고 한다.public class Casting {   public static void main(String[] args) {      double doubleValue = 1.5;      int intValue = 0;			//intValue = doubleValue; // 컴파일 오류 발생 			intValue = (int) doubleValue; // 명시적 형변환 			System.out.println(intValue); // 출력:1	 } }형변환과 오버플로우형변환을 할 때 만약 작은 숫자가 표현할 수 있는 범위를 넘어서면 어떻게 될까?package casting;public class Casting {	public static void main(String[] args) {		long maxIntValue = 2147483647; //int 최고값		long maxIntOver = 2147483648L; //int 최고값 + 1(초과) 		int intValue = 0;				intValue = (int) maxIntValue; //형변환		System.out.println("maxIntValue casting=" + intValue); //출력:2147483647				intValue = (int) maxIntOver; //형변환		System.out.println("maxIntOver casting=" + intValue); //출력:-2147483648 	}}출력결과 maxIntValue casting=2147483647 maxIntOver  casting=-2147483648  long maxIntValue = 2147483647 정수 범위의 최대값을 가진 long 타입을 int로 캐스팅할 때는 아무런 문제가 없다.  long maxIntOver = 2147483648L 정수 범위 최대값을 넘긴 리터럴은 L을 붙여서 long 타입으로 받고 있다. 이 경우 int로 표현할 수 있는 범위를 초과했기 때문에 long -&gt; int 캐스팅시 오버플로우 문제가 발생한다.  💡 중요한 것은 오버플로우 자체가 발생하지 않도록 막아야 한다는 것이다. 오버플로우가 발생한 결과를 가지고 어떻게 할 생각을 하지 말자. 변수 타입을 int -&gt; long으로 사이즈를 늘리면 오버플로우 문제를 간단히 해결할 수 있다.계산과 형변환대입뿐만 아니라 계산식에서도 형변환이 일어난다.자바에서 계산에서의 형변환은 다음 2가지를 꼭 기억하자.  같은 타입끼리의 계산은 같은 타입의 결과를 낸다.          int + int -&gt; int        서로 다른 타입의 계산은 큰 범위로 자동 형변환이 일어난다.          int + long -&gt; long + long      int + double -&gt; double + double      String + number literal → String + String 문자열과 숫자 리터럴을 더할 경우 숫자 리터럴이 문자열로 캐스팅된다.      ]]></content>
      <categories>
        
          <category> Java </category>
        
          <category> 김영한 자바 입문 </category>
        
      </categories>
      <tags>
        
          <tag> Java </tag>
        
          <tag> Casting </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[김영한 자바 입문 스코프]]></title>
      <url>/java/%EA%B9%80%EC%98%81%ED%95%9C%20%EC%9E%90%EB%B0%94%20%EC%9E%85%EB%AC%B8/2024/04/22/Java_scope/</url>
      <content type="text"><![CDATA[  변수는 선언한 위치에 따라 지역 변수, 멤버 변수(클래스 변수, 인스턴스 변수)와 같이 분류된다. 이번에는 지역 변수(Local Variable, 로컬 변수)와 스코프에 관해서 정리해보자.지역 변수지역 변수의 지역은 바로 변수가 선언된 코드 블록 {}이다. 지역 변수는 자신이 선언된 코드 블록 {} 안에서만 생존할 수 있고, 자신이 선언된 코드 블록을 벗어나면 제거된다.Scope변수의 접근 가능한 범위를 스코프(scope)라고 한다. 즉, 변수가 선언된 코드 블록의 범위를 가리킨다. {}EXpublic class Scope1 {    public static void main(String[] args) {        int m = 10; // 변수 m 생성        if (true) {            int x = 20; // 변수 x 생성            System.out.println("x = " + x);            System.out.println("m = " + m);        } // x 제거        // x 접근 불가능        System.out.println("m = " + m);    } // m 제거}⭐️ 스코프 존재 이유  변수를 선언한 시점부터 변수를 계속 사용할 수 있게 해도 되지 않을까? 왜 복잡하게 접근 범위(스코프)라는 개념을 만들었을까?아래 예제 코드를 보고 이유를 생각해보자.Bad Examplepackage scope;public class Scope2 {     public static void main(String[] args) {         int m = 10;         int temp = 0;         if (m &gt; 0) {						 temp = m * 2;             System.out.println("temp = " + temp);         }         System.out.println("m = " + m);     }}위 코드는 좋은 코드라고 보기 어렵다. 왜냐하면 임시 변수 temp 는 if 조건절에서만 임시로 잠깐 사용하는 변수이다. 그런데 temp 변수는 main() 코드 블록에 선언되어 있다. 이렇게 되면 다음과 같은 문제가 발생한다.  비효율적인 메모리 사용: main() 코드 블록이 종료될 때 까지 temp 변수는 메모리에 유지되는 비효율적인 메모리 사용이 발생한다.  코드 복잡성 증가: if절이 끝나더라도 여전히 temp에 접근 가능하다. 예제 코드는 단순하지만, 실무 코드는 매우 복잡하기 때문에 접근 가능한 temp 변수를 계속 신경을 써야해서 복잡성이 증가한다.Good Example package scope; public class Scope3 {     public static void main(String[] args) {         int m = 10;         if (m &gt; 0) {             int temp = m * 2;             System.out.println("temp = " + temp);         }         System.out.println("m = " + m);     }}temp 를 if 코드 블록 안에서 선언했다. 이제 temp 는 if 코드 블록 안으로 스코프가 줄어든다. 덕분에 temp 변수를 메모리에서 빨리 제거하여 메모리를 효율적으로 사용하고, temp 변수를 생각해야 하는 범위를 줄여서 더 유지보수 하기 좋은 코드로 만들었다.while문 vs for문 with 스코프 관점스코프 관점에서 while문과 for문을 비교해보자.while package loop; public class While {     public static void main(String[] args) {         int sum = 0;         int endNum = 3;         int i = 1;         while (i &lt;= endNum) {             sum = sum + i;             System.out.println("i=" + i + " sum=" + sum);             i++;         }         // ... 아래 더 많은 코드들이 있다고 가정     }}forpackage loop; public class For {     public static void main(String[] args) {         int sum = 0;         int endNum = 3;         for (int i = 1; i &lt;= endNum; i++) {             sum = sum + i;             System.out.println("i=" + i + " sum=" + sum);         }				//... 아래에 더 많은 코드들이 있다고 가정 		}}스코프 관점에서 카운터 변수 i를 비교해보자.  while문의 경우 변수 i의 스코프가 main() 메서드 전체가 된다. 반면에 for문의 경우 변수 i의 스코프가 for문 안으로 한정된다.  따라서 for문의 i와 같이 반복문 안에서만 사용되는 카운터 변수가 있다면 while문 보다 for문을 사용해서 스코프의 범위를 제한하는 것이 메모리 사용과 유지보수 관점에서 더 좋다.정리  변수는 꼭 필요한 범위로 한정해서 사용하는 것이 좋다. 변수의 스코프는 꼭 필요한 곳으로 한정해서 사용하자. 메모리를 효율적으로 사용하고 더 유지보수하기 좋은 코드를 만들 수 있다.  김영한님 말씀  좋은 프로그램은 무한한 자유가 있는 프로그램이 아니라 적절한 제약이 있는 프로그램이다.]]></content>
      <categories>
        
          <category> Java </category>
        
          <category> 김영한 자바 입문 </category>
        
      </categories>
      <tags>
        
          <tag> Java </tag>
        
          <tag> Scope </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[김영한 자바 입문 반복문]]></title>
      <url>/java/%EA%B9%80%EC%98%81%ED%95%9C%20%EC%9E%90%EB%B0%94%20%EC%9E%85%EB%AC%B8/2024/04/21/Java_loop/</url>
      <content type="text"><![CDATA[  자바는 다음 3가지 반복문을 제공한다.  while, do-while, forwhilewhile (조건식) {  // 코드}장점  루프의 조건이 루프 내부에서 변경되는 경우, while 루프는 이를 관리하기 쉽다.  for 루프보다 더 복잡한 조건과 시나리오에 적합하다.  조건이 충족되는 동안 계속해서 루프를 실행하며, 종료 시점을 명확하게 알 수 없는 경우에 유용하다.단점  초기화, 조건 체크, 반복 후의 작업이 분산되어 있어 코드를 이해하거나 작성하기 어려울 수 있다.  루프 변수가 while 블록 바깥에서도 접근 가능하므로, 이 변수를 실수로 변경하는 상황이 발생할 수 있다.do - whiledo {  // 코드} while (조건식);do-while 문은 while 문과 비슷하지만, 조건에 상관없이 무조건 한 번은 코드를 실행한다.forfor (1.초기식; 2.조건식; 4.증감식) { 	// 3.코드}장점  초기화, 조건 체크, 반복 후의 작업을 한 줄에서 처리할 수 있어 편리하다.  정해진 횟수만큼의 반복을 수행하는 경우에 사용하기 적합하다.  루프 변수의 범위가 for 루프 블록에 제한되므로, 다른 곳에서 이 변수를 실수로 변경할 가능성이 적다.단점  루프의 조건이 루프 내부에서 변경되는 경우, for 루프는 관리하기 어렵다.  복잡한 조건을 가진 반복문을 작성하기에는 while문이 더 적합할 수 있다.break, continuebreak 와 continue 는 반복문에서 사용할 수 있는 키워드다. break 는 반복문을 즉시 종료하고 나간다. continue 는 반복문의 나머지 부분을 건너뛰고 다음 반복으로 진행하는데 사용된다. 모든 반복문에서 사용할 수 있다.for문 vs while문for문은 반복 횟수에 직접적인 영향을 주는 변수를 선언부터, 값 변화, 조건식에 활용까지 가능하다. 즉, for문은 while문을 조금 더 편하게 다룰 수 있도록 구조화한 것이다. for문을 사용하면 개발자는 반복 횟수와 관련된 코드를 나머지 코드와 명확하게 구분할 수 있다. 반면에 while문을 사용하는 경우 반복 횟수 변수인 i를 선언하는 부분과 ++i 변화하는 부분이 분산되어 있다.  💡 요약  정해진 횟수만큼 반복을 수행해야 하면 for문을 사용하고 그렇지 않으면 while문을 사용하면 된다. 물론 이것이 항상 정답은 아니다.]]></content>
      <categories>
        
          <category> Java </category>
        
          <category> 김영한 자바 입문 </category>
        
      </categories>
      <tags>
        
          <tag> Java </tag>
        
          <tag> Loop Statement </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[김영한 자바 입문 조건문]]></title>
      <url>/java/%EA%B9%80%EC%98%81%ED%95%9C%20%EC%9E%90%EB%B0%94%20%EC%9E%85%EB%AC%B8/2024/04/20/Java_condition/</url>
      <content type="text"><![CDATA[if문과 else if문if문에 else if를 함께 사용하는 것은 서로 연관된 조건일 때 사용한다. 그런데 서로 관련이 없는 독립 조건이면 else if를 사용하지 않고 if문을 각각 따로 사용해야 한다.// 예시1. if-else 사용: 서로 연관된 조건이어서, 하나로 묶을 때 if (condition1) {	// 작업1 수행} else if (condition2) {	// 작업2 수행 }	// 예시2. if 각각 사용: 독립 조건일 때 if (condition1) {	// 작업1 수행 }if (condition2) { 	// 작업2 수행}else if 핵심은 순서대로 맞는 조건 을 찾아보고, 맞는 조건이 있으면 딱 1개만 실행이 되는 것이다. 또한 불필요한 조건 검사를 피하고 코드의 효율성을 향상시킬 수 있다.  💡 참고  if, else if, else 문 다음에 실행할 명령어가 하나만 있을 경우에는 {} 중괄호를생략할수있다.if (true) System.out.println("중괄호 생략 가능");일반적으로 if문의 명령이 한 개만 있을 경우에도 다음과 같은 이유로 중괄호를 사용하는 것이 좋다.  가독성: 조건문의 범위가 명확하게 표시된다.  유지보수성: if문 안에 코드를 추가 시 오류 발생 가능성이 낮다.switch문switch문은 if문을 좀 더 편리하게 사용할 수 있는 기능이다.단, if문은 비교 연산자를 사용할 수 있지만 switch문은 단순히 값이 같은지만 비교 가능하다.switch (조건식) { 	case value1:		 // 조건식의 결과 값이 value1일 때 실행되는 코드     break;	case value2:		 // 조건식의 결과 값이 value2일 때 실행되는 코드     break;  default:		 // 조건식의 결과 값이 위의 어떤 값에도 해당하지 않을 때 실행되는 코드 }  🚨 break 문이 없다면?  break를 만나기 전까지 아래 case 문들을 연속해서 계속 실행한다.자바 14 switch사실 if문이 더 큰 범주의 조건을 다룰 수 있기도 하고 switch문이 엄청 간결하다고 보기 어렵다.그래서 자바 14버전부터 새로운 switch문이 정식으로 도입되었다.int coupon = switch (grade) {   case 1 -&gt; 1000;   case 2 -&gt; 2000;   case 3 -&gt; 3000;   default -&gt; 500; }기존 switch 문과 차이점  -&gt; 를 사용한다.  break 문 없이 선택된 데이터를 반환한다.새로운 switch 문은 더 많은 내용을 다루고 있지만, 자세한 내용은 별도로 다룬다.삼항 연산자참과 거짓에 따라서 특정 값을 구하는 경우 삼항 연산자 또는 조건 연산자라고 불리는 ?, : 연산자를 사용할 수 있다. 이 연산자를 사용하면 if 문과 비교해서 코드를 단순화 할 수 있다.(조건식) ? 참인 경우 리턴 값 : 거짓인 경우 리턴 값]]></content>
      <categories>
        
          <category> Java </category>
        
          <category> 김영한 자바 입문 </category>
        
      </categories>
      <tags>
        
          <tag> Java </tag>
        
          <tag> Condition Statement </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[김영한 자바 입문 연산자]]></title>
      <url>/java/%EA%B9%80%EC%98%81%ED%95%9C%20%EC%9E%90%EB%B0%94%20%EC%9E%85%EB%AC%B8/2024/04/16/Java_Operator/</url>
      <content type="text"><![CDATA[  김영한 자바 입문편 “연산자” 강의에서 중요하거나 새롭게 깨달은 것들만 간단하게 정리하자.연산자자주 사용하는 연산자  산술 연산자: +, -, *, /, %  증감 연산자: ++, --  비교 연산자: ==, !=, &gt;, &lt;, &gt;=, &lt;=  논리 연산자: &amp;&amp;, ||, !  대입 연산자: =, -=, +=, /=, %=  삼항 연산자: ? :  객체 타입 확인 연산자: instanceof  그 외 : new, [] (배열 인덱스), . (객체 멤버 접근), () (메소드 호출)실무에서 거의 사용할 일이 없는 연산자  비트 연산자: &amp;, |, ^, ~, &lt;&lt;, &gt;&gt;, &gt;&gt;&gt;연산자 우선순위연산자가 많은 수식에는 괄호를 명시적으로 사용하는 것이 더 명확하고 이해하기 쉽다.2 * 2 + 3 * 3(2 * 2) + (3 * 3)위 두 개의 간단한 수식을 비교해볼 때 확실히 괄호가 포함된 것이 더 이해하기 쉽다.  코드를 몇자 줄여서 모호하거나 복잡해 지는 것 보다는 코드가 더 많더라도 명확하고 단순한 것이 더 유지보수 하기 좋다.대부분의 개발자들은 연산자 우선순위를 외우지 않을 것이다.(나 또한.. 외우지 않는다.)연산자 우선순위를 외우지 말고 아래 두 가지만 생각하자.  상식선에서 우선순위를 결정하자.  애매하면 괄호() 를 사용하자.  개발에서 가장 중요한 것은 단숨함과 명확함이다. 애매하거나 복잡하면 안된다.문자열 비교문자열이 같은지 비교할 때는 == 이 아니라 .equals() 메서드를 사용해야 한다. == 를 사용하면 성공할 때도 있지만 실패할 때도 있다.String str1 = "hello";String str2 = "hello";System.out.println(str1 == str2) // trueSystem.out.println(str1.equals(str2)) // trueString str3 = new String("hello");String str4 = new String("hello");System.out.println(str3 == str4) // falseSystem.out.println(str3.equals(str4)) // true문자열 리터럴 값 비교시에는 == 으로도 괜찮지만, 문자열 객체를 비교할 때는 예상치 못 한 결과가 나오게 된다. 각각 다른 객체를 가리키고 있기 때문이다.  ✅ equals()를 사용하면 문자열의 실제 내용을 비교하여 일치 여부를 확인할 수 있으므로, 예상치 못한 결과나 버그를 방지하고 정확한 비교를 할 수 있다.]]></content>
      <categories>
        
          <category> Java </category>
        
          <category> 김영한 자바 입문 </category>
        
      </categories>
      <tags>
        
          <tag> Java </tag>
        
          <tag> Operator </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[김영한 자바 입문 변수]]></title>
      <url>/java/%EA%B9%80%EC%98%81%ED%95%9C%20%EC%9E%90%EB%B0%94%20%EC%9E%85%EB%AC%B8/2024/04/15/Java_Variable/</url>
      <content type="text"><![CDATA[  김영한 자바 입문편 “변수” 강의에서 중요하거나, 새롭게 깨달은 것들만 간단하게 정리하자.💣 실무에서 거의 사용하지 않는 변수 타입  byte : 표현 길이가 너무 작다. 파일 전송, 복사 등 파일을 바이트 단위로 다룰 때 주로 사용한다.  short : 표현 길이가 너무 작다.  float : 표현 길이와 정밀도가 낮다.  char : 문자 하나를 표현할 일은 거의 없다. 문자 하나라도 문자열 String을 사용하자.⭐️ 실무에서 자주 사용하는 타입  정수 int, long  실수 double  불린형 boolean  문자열 String  💡 자바는 정수 기본 리터럴로 int 를 사용한다. 자바는 4byte(int)를 효율적으로 계산하도록 설계되어 있다.  💡 메모리 용량은 매우 저렴해졌다. 따라서 메모리 용량을 약간 절약하기 보다는 개발 속도나 효율에 초점을 맞추는 것이 더 효과적이다.새롭게 깨달은 사실  정수 리터럴은 기본으로 int 이다. int의 범위를 벗어나는 정수 리터럴(long)의 경우 대문자 L을 붙여줘야 한다.  비슷하게, 실수 리터럴은 double 이다. float 를 사용하고 싶다면 소문자 f를 붙여줘야 한다.  자바 프로젝트에서 src가 package 선언 시작 경로이다.  자바 파일이 위치하는 패키지와 package [path] 선언 위치가 같아야 한다.]]></content>
      <categories>
        
          <category> Java </category>
        
          <category> 김영한 자바 입문 </category>
        
      </categories>
      <tags>
        
          <tag> Java </tag>
        
          <tag> Variable </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Why is kafka fast?]]></title>
      <url>/kafka/2024/01/01/Why_is_kafka_fast/</url>
      <content type="text"><![CDATA[  Kafka를 빠르게 하는 Zero Copy에 대해 알아보자.Kafka는 왜 빠른 것일까?아래와 같은 이유 등으로 Kafka의 성능이 좋다. 오늘은 Zero Copy 에 대해서 자세히 알아보자.  파티션 파일은 OS 페이지캐시 사용  Zero Copy  브로커가 하는 일이 비교적 단순  묶어서 보내기, 묶어서 받기 (batch) 가능Zero Copy?  Zero Copy란?  Zero Copy는 CPU가 한 메모리 영역에서 다른 메모리 영역으로 데이터를 복사하는 작업을 수행하지 않거나 불필요한 데이터 복사를 피하는 컴퓨터 작업을 말한다.Read without zero copy VS Read with zero copywhat zero-copy means / Ref: ByteByteGo위 이미지에서 Zero Copy를 이용하는 경우 불필요한 데이터 copy가 발생하지 않고 바로 NIC Buffer를 통해 message를 consume 하는 것을 확인할 수 있다. 이 접근 방식을 사용하면 시간이 약 65% 단축된다고 한다.Reference  System Design Interview  kafka 조금 아는 척하기 1(개발자용), 최범균]]></content>
      <categories>
        
          <category> Kafka </category>
        
      </categories>
      <tags>
        
          <tag> Kafka </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
